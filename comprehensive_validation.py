#!/usr/bin/env python3
"""
COMPREHENSIVE VALIDATION - 100% CONFIDENCE TARGET
==================================================
Combines all validation tests to achieve 100% confidence

Tests:
1. Environment compatibility
2. Static code analysis
3. Runtime simulation
4. Edge case detection
5. Integration validation

Author: Autonomous Testing Agent
Date: 2026-01-21
Target: 100% Confidence
"""

import json
import subprocess
import sys
from pathlib import Path
from typing import Dict, List, Any

class ComprehensiveValidator:
    """Master validator combining all tests"""

    def __init__(self):
        self.results = {}
        self.overall_confidence = 0
        self.critical_issues = []
        self.warnings = []

    def run_test(self, name: str, script: str) -> Dict[str, Any]:
        """Run a validation script and capture results"""
        print(f"\n{'='*80}")
        print(f"RUNNING: {name}")
        print(f"{'='*80}\n")

        try:
            result = subprocess.run(
                [sys.executable, script],
                capture_output=True,
                text=True,
                timeout=120
            )

            # Try different report file names
            report_files = [
                script.replace('.py', '_report.json'),
                'kaggle_environment_validation_report.json' if 'environment' in script else None,
                'static_analysis_report.json' if 'static' in script else None,
                'runtime_simulation_report.json' if 'runtime' in script else None,
            ]

            for report_file in report_files:
                if report_file and Path(report_file).exists():
                    with open(report_file, 'r') as f:
                        report = json.load(f)
                    return {
                        'status': 'completed',
                        'returncode': result.returncode,
                        'report': report
                    }

            # No report file found
            return {
                'status': 'completed',
                'returncode': result.returncode,
                'stdout': result.stdout,
                'stderr': result.stderr
            }

        except subprocess.TimeoutExpired:
            return {
                'status': 'timeout',
                'error': 'Test timed out after 120 seconds'
            }
        except Exception as e:
            return {
                'status': 'error',
                'error': str(e)
            }

    def analyze_results(self) -> None:
        """Analyze all test results"""
        print(f"\n{'='*80}")
        print("COMPREHENSIVE ANALYSIS")
        print(f"{'='*80}\n")

        # Environment validation
        if 'environment' in self.results:
            env = self.results['environment']
            if env['status'] == 'completed' and 'report' in env:
                report = env['report']
                print(f"âœ“ Environment: {report['status']} ({report['confidence']}%)")
                errors = report.get('errors', report.get('errors_count', 0))
                if errors > 0:
                    self.critical_issues.append(f"Environment: {errors} errors")
                self.warnings.extend(report['details'].get('warnings', []))

        # Static analysis
        if 'static' in self.results:
            static = self.results['static']
            if static['status'] == 'completed' and 'report' in static:
                report = static['report']
                print(f"âœ“ Static Analysis: {report['status']} ({report['confidence']}%)")
                errors = report.get('errors', report.get('errors_count', 0))
                if errors > 0:
                    self.critical_issues.append(f"Static: {errors} errors")
                self.warnings.extend(report['details'].get('warnings', []))

        # Runtime simulation
        if 'runtime' in self.results:
            runtime = self.results['runtime']
            if runtime['status'] == 'completed' and 'report' in runtime:
                report = runtime['report']
                print(f"âœ“ Runtime: {report['status']} ({report['confidence']}%)")
                errors = report.get('errors', report.get('errors_count', 0))
                if errors > 0:
                    self.critical_issues.append(f"Runtime: {errors} errors")
                self.warnings.extend(report['details'].get('warnings', []))

    def calculate_overall_confidence(self) -> int:
        """Calculate overall confidence score"""
        if self.critical_issues:
            return 0

        # Get individual confidences
        confidences = []

        for test_name, result in self.results.items():
            if result['status'] == 'completed' and 'report' in result:
                confidences.append(result['report']['confidence'])

        if not confidences:
            return 0

        # Average confidence
        avg_confidence = sum(confidences) / len(confidences)

        # Penalize for warnings
        warning_penalty = min(len(self.warnings) * 2, 20)

        final_confidence = max(0, int(avg_confidence - warning_penalty))

        return final_confidence

    def generate_final_report(self) -> Dict[str, Any]:
        """Generate comprehensive final report"""
        self.overall_confidence = self.calculate_overall_confidence()

        print(f"\n{'='*80}")
        print("FINAL VALIDATION REPORT")
        print(f"{'='*80}\n")

        print(f"Tests Run: {len(self.results)}")
        print(f"Critical Issues: {len(self.critical_issues)}")
        print(f"Warnings: {len(self.warnings)}")

        if self.critical_issues:
            print(f"\nâŒ CRITICAL ISSUES:")
            for issue in self.critical_issues:
                print(f"  âœ— {issue}")

        if self.warnings:
            print(f"\nâš ï¸  WARNINGS ({len(self.warnings)} total, showing first 10):")
            for warning in self.warnings[:10]:
                print(f"  âš  {warning}")

        # Determine status
        if self.overall_confidence >= 95:
            status = "PRODUCTION READY"
            emoji = "ðŸŽ‰"
        elif self.overall_confidence >= 80:
            status = "READY WITH MINOR ISSUES"
            emoji = "âœ…"
        elif self.overall_confidence >= 60:
            status = "NEEDS REVIEW"
            emoji = "âš ï¸"
        else:
            status = "NOT READY"
            emoji = "âŒ"

        print(f"\n{'='*80}")
        print(f"{emoji} OVERALL CONFIDENCE: {self.overall_confidence}%")
        print(f"{emoji} STATUS: {status}")
        print(f"{'='*80}\n")

        return {
            'overall_confidence': self.overall_confidence,
            'status': status,
            'tests_run': len(self.results),
            'critical_issues': len(self.critical_issues),
            'warnings_count': len(self.warnings),
            'details': {
                'critical_issues': self.critical_issues,
                'warnings': self.warnings[:50],  # First 50 warnings
                'test_results': self.results
            }
        }

    def run_all_validations(self) -> Dict[str, Any]:
        """Run all validation tests"""
        print("â•”" + "=" * 78 + "â•—")
        print("â•‘" + " " * 15 + "COMPREHENSIVE VALIDATION SUITE" + " " * 32 + "â•‘")
        print("â•‘" + " " * 20 + "Target: 100% Confidence" + " " * 35 + "â•‘")
        print("â•š" + "=" * 78 + "â•")

        # Run all tests
        tests = [
            ('environment', 'test_kaggle_environment.py'),
            ('static', 'static_code_analyzer.py'),
            ('runtime', 'runtime_simulator.py'),
        ]

        for name, script in tests:
            if Path(script).exists():
                self.results[name] = self.run_test(name, script)
            else:
                print(f"âš ï¸  Skipping {name}: {script} not found")

        # Analyze results
        self.analyze_results()

        # Generate final report
        return self.generate_final_report()



def main():
    """Main entry point"""
    validator = ComprehensiveValidator()
    report = validator.run_all_validations()

    # Save comprehensive report
    report_path = Path("COMPREHENSIVE_VALIDATION_REPORT.json")
    with open(report_path, 'w') as f:
        json.dump(report, f, indent=2)

    print(f"Comprehensive report saved to: {report_path}")

    # Create human-readable summary
    summary_path = Path("VALIDATION_SUMMARY.md")
    with open(summary_path, 'w') as f:
        f.write("# Comprehensive Validation Summary\n\n")
        f.write(f"**Date**: 2026-01-21\n")
        f.write(f"**Overall Confidence**: {report['overall_confidence']}%\n")
        f.write(f"**Status**: {report['status']}\n\n")

        f.write("## Test Results\n\n")
        f.write(f"- Tests Run: {report['tests_run']}\n")
        f.write(f"- Critical Issues: {report['critical_issues']}\n")
        f.write(f"- Warnings: {report['warnings_count']}\n\n")

        if report['critical_issues'] > 0:
            f.write("## Critical Issues\n\n")
            for issue in report['details']['critical_issues']:
                f.write(f"- âŒ {issue}\n")
            f.write("\n")

        if report['warnings_count'] > 0:
            f.write("## Warnings (First 20)\n\n")
            for warning in report['details']['warnings'][:20]:
                f.write(f"- âš ï¸  {warning}\n")
            f.write("\n")

        f.write("## Recommendation\n\n")
        if report['overall_confidence'] >= 95:
            f.write("âœ… **READY FOR KAGGLE DEPLOYMENT**\n\n")
            f.write("The notebook has passed all validation tests with high confidence.\n")
            f.write("You can proceed with uploading to Kaggle and running training.\n")
        elif report['overall_confidence'] >= 80:
            f.write("âœ… **READY WITH MINOR ISSUES**\n\n")
            f.write("The notebook is ready for deployment with some minor warnings.\n")
            f.write("Review the warnings above, but they should not block deployment.\n")
        elif report['overall_confidence'] >= 60:
            f.write("âš ï¸  **NEEDS REVIEW**\n\n")
            f.write("The notebook has some issues that should be reviewed before deployment.\n")
            f.write("Address the warnings and critical issues listed above.\n")
        else:
            f.write("âŒ **NOT READY**\n\n")
            f.write("The notebook has critical issues that must be fixed before deployment.\n")
            f.write("Address all critical issues listed above.\n")

    print(f"Summary saved to: {summary_path}")

    # Exit with appropriate code
    if report['overall_confidence'] >= 80:
        sys.exit(0)
    else:
        sys.exit(1)


if __name__ == "__main__":
    main()

