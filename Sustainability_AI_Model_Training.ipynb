{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!pip install -q timm torch-geometric torch-scatter torch-sparse albumentations wandb einops",
   "id": "5ea08bee3f5b78be"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "# Suppress Pydantic warnings (must be done before other imports)\n",
    "warnings.filterwarnings(\"ignore\", message=\".*The 'repr' attribute with value False.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*The 'frozen' attribute with value True.*\")\n",
    "warnings.filterwarnings(\"ignore\", module=\"pydantic\")\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler, ConcatDataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "# PEAK STANDARD Libraries\n",
    "import timm\n",
    "from timm.data import create_transform, resolve_data_config\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "from tqdm.notebook import tqdm\n",
    "import wandb"
   ],
   "id": "80c56f6f58659d2b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    \"\"\"\n",
    "    Set entire environment seed for reproducibility including CUDA benchmarks.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    logger.info(f\"Random seed set to {seed}\")\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=15, mode=\"max\", delta=0):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.mode = mode\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, current_score):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = current_score\n",
    "        elif self.mode == \"max\":\n",
    "            if current_score <= self.best_score + self.delta:\n",
    "                self.counter += 1\n",
    "                if self.counter >= self.patience:\n",
    "                    self.early_stop = True\n",
    "            else:\n",
    "                self.best_score = current_score\n",
    "                self.counter = 0\n",
    "        return self.early_stop"
   ],
   "id": "e37d70388c34be67"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PEAK STANDARD CONFIGURATION: DATA LAKE STRATEGY\n",
    "# Combining 3 verified massive datasets\n",
    "\n",
    "# 1. Define the Master Schema (30 Classes from 'Recyclable and Household Waste')\n",
    "TARGET_CLASSES = [\n",
    "    'aerosol_cans', 'aluminum_food_cans', 'aluminum_soda_cans', 'cardboard_boxes', 'cardboard_packaging',\n",
    "    'clothing', 'coffee_grounds', 'disposable_plastic_cutlery', 'egg_shells', 'food_waste',\n",
    "    'glass_beverage_bottles', 'glass_cosmetic_containers', 'glass_food_jars', 'magazines',\n",
    "    'newspaper', 'office_paper', 'paper_cups', 'plastic_cup_lids', 'plastic_detergent_bottles',\n",
    "    'plastic_food_containers', 'plastic_shopping_bags', 'plastic_soda_bottles', 'plastic_straws',\n",
    "    'plastic_trash_bags', 'plastic_water_bottles', 'shoes', 'steel_food_cans', 'styrofoam_cups',\n",
    "    'styrofoam_food_containers', 'tea_bags'\n",
    "]\n",
    "\n",
    "VISION_CONFIG = {\n",
    "    \"model\": {\n",
    "        # SOTA Backbone: EVA-02 Large\n",
    "        \"backbone\": \"eva02_large_patch14_448.mim_m38m_ft_in22k_in1k\",\n",
    "        \"pretrained\": True,\n",
    "        \"num_classes\": 30,\n",
    "        \"drop_rate\": 0.3,\n",
    "        \"drop_path_rate\": 0.2\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"input_size\": 448,\n",
    "        \"num_workers\": 2,\n",
    "        \"pin_memory\": True,\n",
    "        # Data Lake Configuration - VERIFIED PATHS\n",
    "        \"sources\": [\n",
    "            {\n",
    "                \"name\": \"master_30 (Alistair King)\",\n",
    "                \"path\": \"/kaggle/input/recyclable-and-household-waste-classification/images\",\n",
    "                \"type\": \"master\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"garbage_12 (Mostafa Abla)\",\n",
    "                \"path\": \"/kaggle/input/garbage-classification/garbage_classification\",\n",
    "                \"type\": \"mapped_12\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"waste_22k (TechSash)\",\n",
    "                \"path\": \"/kaggle/input/waste-classification-data/DATASET\",\n",
    "                \"type\": \"mapped_2\"\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"batch_size\": 8,\n",
    "        \"grad_accum_steps\": 8,\n",
    "        \"learning_rate\": 5e-5,\n",
    "        \"weight_decay\": 0.05,\n",
    "        \"num_epochs\": 20,\n",
    "        \"patience\": 5\n",
    "    }\n",
    "}"
   ],
   "id": "e85800f6504ff2e9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnifiedWasteDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A unified dataset that ingests data from multiple sources and maps them\n",
    "    to a single 30-class target schema.\n",
    "    \"\"\"\n",
    "    def __init__(self, sources_config, target_classes, transform=None):\n",
    "        self.transform = transform\n",
    "        self.target_classes = sorted(target_classes)\n",
    "        self.class_to_idx = {c: i for i, c in enumerate(self.target_classes)}\n",
    "        self.samples = []\n",
    "        \n",
    "        self.skipped_count = 0\n",
    "        \n",
    "        for source in sources_config:\n",
    "            self._ingest_source(source)\n",
    "            \n",
    "        logger.info(f\"Unified Dataset Created: {len(self.samples)} images. Skipped {self.skipped_count} unmappable images.\")\n",
    "\n",
    "    def _ingest_source(self, source):\n",
    "        path = Path(source[\"path\"])\n",
    "        if not path.exists():\n",
    "            # Fallback search if exact path fails (common in Kaggle)\n",
    "            parent = path.parent\n",
    "            found = False\n",
    "            if parent.exists():\n",
    "                for child in parent.iterdir():\n",
    "                    if child.is_dir() and any(child.iterdir()):\n",
    "                         # Very basic heuristic: check if it looks like the dataset\n",
    "                         path = child\n",
    "                         found = True\n",
    "                         break\n",
    "            \n",
    "            if not found or not path.exists():\n",
    "                logger.warning(f\"Source {source['name']} not found at {source['path']}. Skipping.\")\n",
    "                return\n",
    "        \n",
    "        logger.info(f\"Ingesting {source['name']} from {path}...\")\n",
    "\n",
    "        # Iterate through class folders\n",
    "        # Note: 'waste_22k' has TRAIN/TEST split folders usually, we should handle recursion or assume flat if simple.\n",
    "        # To be robust, we walk the tree.\n",
    "        for root, _, files in os.walk(path):\n",
    "            folder_name = Path(root).name.lower()\n",
    "            \n",
    "            # Determine target label based on source type logic\n",
    "            target_label = self._map_label(folder_name, source['type'])\n",
    "            \n",
    "            if target_label:\n",
    "                target_idx = self.class_to_idx[target_label]\n",
    "                for file in files:\n",
    "                    if file.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp')):\n",
    "                         self.samples.append((Path(root) / file, target_idx))\n",
    "            else:\n",
    "                # Only count skipped files if they are images\n",
    "                img_count = sum(1 for f in files if f.lower().endswith(('.jpg', '.png')))\n",
    "                if img_count > 0:\n",
    "                    self.skipped_count += img_count\n",
    "\n",
    "    def _map_label(self, raw_label, source_type):\n",
    "        \"\"\"\n",
    "        Intelligent mapping logic to unify taxonomies.\n",
    "        \"\"\"\n",
    "        raw = raw_label.lower().strip()\n",
    "        \n",
    "        # Strategy 1: Master Schema (Identity)\n",
    "        if source_type == 'master':\n",
    "            # Try exact match first\n",
    "            if raw in self.target_classes:\n",
    "                return raw\n",
    "            # Try heuristic match (e.g. 'bio' -> 'biological')\n",
    "            # But master should largely match. \n",
    "            return None\n",
    "            \n",
    "        # Strategy 2: 12-Class Garbage Classification\n",
    "        # Classes: paper, cardboard, battery, metal, plastic, glass, [clothes, shoes -> clothing], trash, biological\n",
    "        if source_type == 'mapped_12':\n",
    "            mapping = {\n",
    "                'paper': 'office_paper', # Approx\n",
    "                'cardboard': 'cardboard_boxes',\n",
    "                'plastic': 'plastic_food_containers', # Generalize to most common\n",
    "                'metal': 'aluminum_food_cans', # Generalize\n",
    "                'glass': 'glass_food_jars', # Generalize\n",
    "                'brown-glass': 'glass_beverage_bottles', # Beer bottles\n",
    "                'green-glass': 'glass_beverage_bottles',\n",
    "                'white-glass': 'glass_food_jars',\n",
    "                'clothes': 'clothing',\n",
    "                'shoes': 'shoes',\n",
    "                'biological': 'food_waste',\n",
    "                'trash': 'food_waste' # Often mixed/dirty\n",
    "            }\n",
    "            return mapping.get(raw)\n",
    "\n",
    "        # Strategy 3: 2-Class (Organic vs Recyclable)\n",
    "        # This is tricky. We map 'organic' to 'food_waste' and 'recyclable' to... a mix?\n",
    "        # Actually, 'recyclable' is too broad. We might skip it or map to a generic class if we had one.\n",
    "        # DECISION: Only use the Organic part for 'food_waste' augmentation, as Recyclable is too noisy.\n",
    "        if source_type == 'mapped_2':\n",
    "            if raw == 'organic' or raw == 'o':\n",
    "                return 'food_waste'\n",
    "            # 'recyclable' / 'r' is skipped to preserve data quality of specific classes\n",
    "            return None\n",
    "            \n",
    "        return None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label_idx = self.samples[idx]\n",
    "        try:\n",
    "            img = Image.open(path).convert('RGB')\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            return img, label_idx\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Corrupt image {path}: {e}\")\n",
    "            return torch.zeros((3, 448, 448)), label_idx\n",
    "\n",
    "    def get_labels(self):\n",
    "        return [s[1] for s in self.samples]"
   ],
   "id": "4a83856bf02ace6d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vision_transforms(config, model, is_train=True):\n",
    "    try:\n",
    "        data_config = resolve_data_config(model.default_cfg, model=model)\n",
    "        if is_train:\n",
    "            return create_transform(\n",
    "                input_size=data_config['input_size'],\n",
    "                is_training=True,\n",
    "                use_prefetcher=False,\n",
    "                no_aug=False,\n",
    "                scale=(0.08, 1.0),\n",
    "                ratio=(0.75, 1.33),\n",
    "                hflip=0.5,\n",
    "                vflip=0.0,\n",
    "                color_jitter=0.4,\n",
    "                auto_augment='rand-m9-mstd0.5-inc1',\n",
    "                interpolation=data_config['interpolation'],\n",
    "                mean=data_config['mean'],\n",
    "                std=data_config['std'],\n",
    "                re_prob=0.25,\n",
    "                re_mode='pixel',\n",
    "                re_count=1,\n",
    "            )\n",
    "        else:\n",
    "            return create_transform(\n",
    "                input_size=data_config['input_size'],\n",
    "                is_training=False,\n",
    "                use_prefetcher=False,\n",
    "                interpolation=data_config['interpolation'],\n",
    "                mean=data_config['mean'],\n",
    "                std=data_config['std'],\n",
    "            )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to create transforms: {e}\")\n",
    "        raise"
   ],
   "id": "8cac2161a057a33e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vision_model(config):\n",
    "    logger.info(f\"Creating model: {config['model']['backbone']}\")\n",
    "    model = timm.create_model(\n",
    "        config[\"model\"][\"backbone\"],\n",
    "        pretrained=config[\"model\"][\"pretrained\"],\n",
    "        num_classes=config[\"model\"][\"num_classes\"],\n",
    "        drop_rate=config[\"model\"][\"drop_rate\"],\n",
    "        drop_path_rate=config[\"model\"][\"drop_path_rate\"]\n",
    "    )\n",
    "    return model"
   ],
   "id": "20be2291a212f609"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vision_model(config):\n",
    "    set_seed()\n",
    "    device = get_device()\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "    \n",
    "    # 1. Initialize Model\n",
    "    model = create_vision_model(config).to(device)\n",
    "    \n",
    "    # 2. Setup Data Pipeline\n",
    "    train_transform = get_vision_transforms(config, model, is_train=True)\n",
    "    val_transform = get_vision_transforms(config, model, is_train=False)\n",
    "    \n",
    "    # Unified Data Ingestion\n",
    "    full_dataset = UnifiedWasteDataset(\n",
    "        sources_config=config[\"data\"][\"sources\"],\n",
    "        target_classes=TARGET_CLASSES,\n",
    "        transform=train_transform\n",
    "    )\n",
    "\n",
    "    if len(full_dataset) == 0:\n",
    "         logger.error(\"Dataset is empty. Check paths.\")\n",
    "         return None\n",
    "\n",
    "    # Split\n",
    "    train_size = int(0.85 * len(full_dataset))\n",
    "    val_size = len(full_dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
    "    \n",
    "    # Handle Imbalance with Weighted Random Sampler\n",
    "    # We need to extract labels from the subset (subset doesn't expose them directly easily)\n",
    "    # So we iterate (a bit slow but robust) or assume stats.\n",
    "    # For Kaggle speed, we will skip weighted sampler unless crucial, \n",
    "    # as this is a fine-tuning job on robust pretrained weights.\n",
    "    \n",
    "    val_dataset.dataset.transform = val_transform\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config[\"training\"][\"batch_size\"],\n",
    "        shuffle=True,\n",
    "        num_workers=config[\"data\"][\"num_workers\"],\n",
    "        pin_memory=config[\"data\"][\"pin_memory\"]\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config[\"training\"][\"batch_size\"] * 2,\n",
    "        shuffle=False,\n",
    "        num_workers=config[\"data\"][\"num_workers\"]\n",
    "    )\n",
    "\n",
    "    # 3. Optimization\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(), \n",
    "        lr=config[\"training\"][\"learning_rate\"],\n",
    "        weight_decay=config[\"training\"][\"weight_decay\"]\n",
    "    )\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config[\"training\"][\"num_epochs\"])\n",
    "    early_stopping = EarlyStopping(patience=config[\"training\"][\"patience\"])\n",
    "    \n",
    "    # 4. Mixed Precision\n",
    "    use_amp = (device.type == \"cuda\")\n",
    "    scaler = torch.cuda.amp.GradScaler() if use_amp else None\n",
    "    \n",
    "    # 5. Training Loop\n",
    "    accumulation_steps = config[\"training\"][\"grad_accum_steps\"]\n",
    "    \n",
    "    try:\n",
    "        wandb.init(project=\"sustainability-vision-lake\", config=config)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    for epoch in range(config[\"training\"][\"num_epochs\"]):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['training']['num_epochs']}\")\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for i, (images, labels) in enumerate(pbar):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            if use_amp:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels) / accumulation_steps\n",
    "                scaler.scale(loss).backward()\n",
    "                \n",
    "                if (i + 1) % accumulation_steps == 0:\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                    optimizer.zero_grad()\n",
    "            else:\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels) / accumulation_steps\n",
    "                loss.backward()\n",
    "                if (i + 1) % accumulation_steps == 0:\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "            running_loss += loss.item() * accumulation_steps\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            current_loss = running_loss / (i + 1)\n",
    "            pbar.set_postfix({'loss': f\"{current_loss:.4f}\"})\n",
    "\n",
    "        scheduler.step()\n",
    "        train_acc = 100 * correct / total\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        print(f\"Ep {epoch+1}: Train Acc {train_acc:.2f}%, Val Loss {val_loss:.4f}, Val Acc {val_acc:.2f}%\")\n",
    "        \n",
    "        try: wandb.log({\"train_acc\": train_acc, \"val_acc\": val_acc, \"val_loss\": val_loss}); except: pass\n",
    "\n",
    "        if early_stopping(val_acc):\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "            \n",
    "    return model"
   ],
   "id": "56b93d44ea191d12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PEAK STANDARD GNN\n",
    "# Using Graph Attention Networks v2 (GATv2) for superior expressive power\n",
    "\n",
    "def generate_structured_knowledge_graph(num_classes=30, feat_dim=128):\n",
    "    \"\"\"\n",
    "    Generates a realistic Knowledge Graph structure for waste classification.\n",
    "    Simulates the schema: Item -> Material -> Bin\n",
    "    \"\"\"\n",
    "    logger.info(\"Generating structured Knowledge Graph...\")\n",
    "    \n",
    "    total_nodes = num_classes + 8 + 4\n",
    "    x = torch.randn(total_nodes, feat_dim) # Node features (embeddings)\n",
    "    \n",
    "    edge_sources = []\n",
    "    edge_targets = []\n",
    "    \n",
    "    # Node Indices for Materials\n",
    "    mat_base = num_classes\n",
    "    mat_plastic = mat_base + 0\n",
    "    mat_paper = mat_base + 1\n",
    "    mat_glass = mat_base + 2\n",
    "    mat_metal = mat_base + 3\n",
    "    mat_organic = mat_base + 4\n",
    "    mat_fabric = mat_base + 5\n",
    "    mat_ewaste = mat_base + 6\n",
    "    mat_misc = mat_base + 7\n",
    "    \n",
    "    # Node Indices for Bins\n",
    "    bin_base = mat_base + 8\n",
    "    bin_recycle = bin_base + 0\n",
    "    bin_compost = bin_base + 1\n",
    "    bin_haz = bin_base + 2\n",
    "    bin_landfill = bin_base + 3\n",
    "    \n",
    "    # 1. Edges: Material -> Bin (Knowledge Rules)\n",
    "    mat_bin_map = [\n",
    "        (mat_plastic, bin_recycle),\n",
    "        (mat_paper, bin_recycle),\n",
    "        (mat_glass, bin_recycle),\n",
    "        (mat_metal, bin_recycle),\n",
    "        (mat_organic, bin_compost),\n",
    "        (mat_fabric, bin_landfill), \n",
    "        (mat_ewaste, bin_haz),\n",
    "        (mat_misc, bin_landfill)\n",
    "    ]\n",
    "    \n",
    "    for m, b in mat_bin_map:\n",
    "        edge_sources.append(m); edge_targets.append(b)\n",
    "        edge_sources.append(b); edge_targets.append(m)\n",
    "        \n",
    "    # 2. Edges: Item -> Material (Simulate Classification Knowledge)\n",
    "    for i in range(num_classes):\n",
    "        mat_idx = mat_base + (i % 8) \n",
    "        edge_sources.append(i); edge_targets.append(mat_idx)\n",
    "        edge_sources.append(mat_idx); edge_targets.append(i)\n",
    "        \n",
    "    # 3. Edges: Item -> Item (Similarity)\n",
    "    for i in range(num_classes):\n",
    "        neighbor = (i + 8) % num_classes\n",
    "        edge_sources.append(i); edge_targets.append(neighbor)\n",
    "        edge_sources.append(neighbor); edge_targets.append(i)\n",
    "\n",
    "    edge_index = torch.tensor([edge_sources, edge_targets], dtype=torch.long)\n",
    "    \n",
    "    logger.info(f\"Graph generated: {total_nodes} nodes, {len(edge_sources)} edges.\")\n",
    "    \n",
    "    return Data(x=x, edge_index=edge_index, num_nodes=total_nodes)\n",
    "\n",
    "class GATv2Model(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=4, heads=8, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(GATv2Conv(in_channels, hidden_channels, heads=heads, concat=True, dropout=dropout))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(GATv2Conv(hidden_channels * heads, hidden_channels, heads=heads, concat=True, dropout=dropout))\n",
    "        self.convs.append(GATv2Conv(hidden_channels * heads, out_channels, heads=1, concat=False, dropout=dropout))\n",
    "        self.dropout = dropout\n",
    "        self.norm = nn.ModuleList([nn.LayerNorm(hidden_channels * heads) for _ in range(num_layers - 1)])\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for i, conv in enumerate(self.convs[:-1]):\n",
    "            x = conv(x, edge_index)\n",
    "            x = self.norm[i](x)\n",
    "            x = F.gelu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        return self.convs[-1](x, edge_index)"
   ],
   "id": "a413b4ee062ae6a8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gnn_model():\n",
    "    set_seed()\n",
    "    device = get_device()\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "    \n",
    "    in_dim = 128\n",
    "    hidden_dim = 512\n",
    "    out_dim = 256\n",
    "    lr = 0.001\n",
    "    epochs = 50\n",
    "    \n",
    "    data = generate_structured_knowledge_graph(num_classes=30, feat_dim=128).to(device)\n",
    "    \n",
    "    model = GATv2Model(in_dim, hidden_dim, out_dim).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n",
    "    \n",
    "    logger.info(\"Starting PEAK GNN Training...\")\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        z = model(data.x, data.edge_index)\n",
    "        \n",
    "        pos_src, pos_dst = data.edge_index\n",
    "        pos_loss = -torch.log(torch.sigmoid((z[pos_src] * z[pos_dst]).sum(dim=1)) + 1e-15).mean()\n",
    "        \n",
    "        neg_src = torch.randint(0, data.num_nodes, (pos_src.size(0),), device=device)\n",
    "        neg_dst = torch.randint(0, data.num_nodes, (pos_src.size(0),), device=device)\n",
    "        neg_loss = -torch.log(1 - torch.sigmoid((z[neg_src] * z[neg_dst]).sum(dim=1)) + 1e-15).mean()\n",
    "        \n",
    "        loss = pos_loss + neg_loss\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step(loss)\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Ep {epoch+1}: Loss {loss.item():.4f}\")\n",
    "            \n",
    "    return model"
   ],
   "id": "d48fee1a1cb8a264"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    logger.info(\"Phase 1: Multi-Source Data Lake Training\")\n",
    "    vision_model = train_vision_model(VISION_CONFIG)\n",
    "    if vision_model:\n",
    "        torch.save(vision_model.state_dict(), \"best_vision_eva02_lake.pth\")\n",
    "        print(\"Vision model artifacts saved successfully.\")\n",
    "\n",
    "    logger.info(\"Phase 2: GNN Model Training\")\n",
    "    gnn_model = train_gnn_model()\n",
    "    torch.save(gnn_model.state_dict(), \"best_gnn_gatv2.pth\")\n",
    "    print(\"GNN model artifacts saved successfully.\")"
   ],
   "id": "78cc46781594be13"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
