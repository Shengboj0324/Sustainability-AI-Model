{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q timm torch-geometric albumentations pycocotools wandb einops"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "import json\n",
                "import random\n",
                "import logging\n",
                "import time\n",
                "from pathlib import Path\n",
                "from typing import Dict, List, Tuple, Optional, Any\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
                "from torchvision.datasets import ImageFolder\n",
                "import torchvision.transforms as transforms\n",
                "from PIL import Image\n",
                "\n",
                "# PEAK STANDARD Libraries\n",
                "import timm\n",
                "from timm.data import create_transform, resolve_data_config\n",
                "from torch_geometric.data import Data\n",
                "from torch_geometric.nn import GATv2Conv\n",
                "from tqdm.notebook import tqdm\n",
                "import wandb"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
                "logger = logging.getLogger(__name__)\n",
                "\n",
                "def set_seed(seed: int = 42):\n",
                "    random.seed(seed)\n",
                "    np.random.seed(seed)\n",
                "    torch.manual_seed(seed)\n",
                "    if torch.cuda.is_available():\n",
                "        torch.cuda.manual_seed_all(seed)\n",
                "        torch.backends.cudnn.deterministic = True\n",
                "        torch.backends.cudnn.benchmark = False\n",
                "\n",
                "def get_device():\n",
                "    if torch.cuda.is_available():\n",
                "        return torch.device(\"cuda\")\n",
                "    elif torch.backends.mps.is_available():\n",
                "        return torch.device(\"mps\")\n",
                "    return torch.device(\"cpu\")\n",
                "\n",
                "class EarlyStopping:\n",
                "    def __init__(self, patience=15, mode=\"max\", delta=0):\n",
                "        self.patience = patience\n",
                "        self.counter = 0\n",
                "        self.best_score = None\n",
                "        self.early_stop = False\n",
                "        self.mode = mode\n",
                "        self.delta = delta\n",
                "\n",
                "    def __call__(self, current_score):\n",
                "        if self.best_score is None:\n",
                "            self.best_score = current_score\n",
                "        elif self.mode == \"max\":\n",
                "            if current_score <= self.best_score + self.delta:\n",
                "                self.counter += 1\n",
                "                if self.counter >= self.patience:\n",
                "                    self.early_stop = True\n",
                "            else:\n",
                "                self.best_score = current_score\n",
                "                self.counter = 0\n",
                "        return self.early_stop"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# PEAK STANDARD CONFIGURATION\n",
                "VISION_CONFIG = {\n",
                "    \"model\": {\n",
                "        # SOTA Backbone: EVA-02 Large\n",
                "        \"backbone\": \"eva02_large_patch14_448.mim_m38m_ft_in22k_in1k\",\n",
                "        \"pretrained\": True,\n",
                "        \"num_classes\": 12,\n",
                "        \"drop_rate\": 0.2,\n",
                "        \"drop_path_rate\": 0.2\n",
                "    },\n",
                "    \"data\": {\n",
                "        \"train_dir\": \"/kaggle/input/garbage-classification/garbage_classification/\",\n",
                "        \"input_size\": 448,  # High Resolution\n",
                "        \"num_workers\": 4,\n",
                "        \"pin_memory\": True\n",
                "    },\n",
                "    \"training\": {\n",
                "        \"batch_size\": 16,  # Smaller batch for 448px\n",
                "        \"grad_accum_steps\": 4, \n",
                "        \"learning_rate\": 1e-4,\n",
                "        \"weight_decay\": 0.05,\n",
                "        \"num_epochs\": 30,\n",
                "        \"patience\": 10\n",
                "    }\n",
                "}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class VisionDataset(Dataset):\n",
                "    def __init__(self, root_dir, transform=None):\n",
                "        self.root_dir = Path(root_dir)\n",
                "        self.transform = transform\n",
                "        self.classes = sorted([d.name for d in self.root_dir.iterdir() if d.is_dir()])\n",
                "        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
                "        self.images = self._load_images()\n",
                "\n",
                "    def _load_images(self):\n",
                "        images = []\n",
                "        for class_name in self.classes:\n",
                "            class_dir = self.root_dir / class_name\n",
                "            for img_path in class_dir.glob(\"*\"):\n",
                "                if img_path.suffix.lower() in [\".jpg\", \".jpeg\", \".png\", \".bmp\"]:\n",
                "                    images.append((img_path, self.class_to_idx[class_name]))\n",
                "        return images\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.images)\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        img_path, label = self.images[idx]\n",
                "        try:\n",
                "            image = Image.open(img_path).convert(\"RGB\")\n",
                "            if self.transform:\n",
                "                image = self.transform(image)\n",
                "            return image, label\n",
                "        except Exception as e:\n",
                "            print(f\"Error loading {img_path}: {e}\")\n",
                "            return torch.zeros((3, 448, 448)), label"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_vision_transforms(config, model, is_train=True):\n",
                "    # Use timm's data config to get optimal normalization and resolution\n",
                "    data_config = resolve_data_config(model.default_cfg, model=model)\n",
                "    \n",
                "    if is_train:\n",
                "        # State-of-the-Art Training Transforms (AutoAugment, RandAugment, etc.)\n",
                "        return create_transform(\n",
                "            input_size=data_config['input_size'],\n",
                "            is_training=True,\n",
                "            use_prefetcher=False,\n",
                "            no_aug=False,\n",
                "            scale=(0.08, 1.0),\n",
                "            ratio=(0.75, 1.33),\n",
                "            hflip=0.5,\n",
                "            vflip=0.0,\n",
                "            color_jitter=0.4,\n",
                "            auto_augment='rand-m9-mstd0.5-inc1',  # Strongest augmentations\n",
                "            interpolation=data_config['interpolation'],\n",
                "            mean=data_config['mean'],\n",
                "            std=data_config['std'],\n",
                "        )\n",
                "    else:\n",
                "        # Validation Transforms (Center Crop / Resize)\n",
                "        return create_transform(\n",
                "            input_size=data_config['input_size'],\n",
                "            is_training=False,\n",
                "            use_prefetcher=False,\n",
                "            interpolation=data_config['interpolation'],\n",
                "            mean=data_config['mean'],\n",
                "            std=data_config['std'],\n",
                "        )\n",
                "\n",
                "def create_vision_model(config):\n",
                "    model = timm.create_model(\n",
                "        config[\"model\"][\"backbone\"],\n",
                "        pretrained=config[\"model\"][\"pretrained\"],\n",
                "        num_classes=config[\"model\"][\"num_classes\"],\n",
                "        drop_rate=config[\"model\"][\"drop_rate\"],\n",
                "        drop_path_rate=config[\"model\"][\"drop_path_rate\"]\n",
                "    )\n",
                "    return model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_vision_model(config):\n",
                "    set_seed()\n",
                "    device = get_device()\n",
                "    logger.info(f\"Using device: {device}\")\n",
                "    \n",
                "    model = create_vision_model(config).to(device)\n",
                "    \n",
                "    # Get transforms specifically tuned for this model\n",
                "    train_transform = get_vision_transforms(config, model, is_train=True)\n",
                "    val_transform = get_vision_transforms(config, model, is_train=False)\n",
                "\n",
                "    full_dataset = VisionDataset(\n",
                "        config[\"data\"][\"train_dir\"],\n",
                "        transform=train_transform\n",
                "    )\n",
                "\n",
                "    train_size = int(0.85 * len(full_dataset))\n",
                "    val_size = len(full_dataset) - train_size\n",
                "    if train_size == 0:\n",
                "        print(\"No data found.\")\n",
                "        return\n",
                "\n",
                "    train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
                "    val_dataset.dataset.transform = val_transform\n",
                "\n",
                "    train_loader = DataLoader(\n",
                "        train_dataset,\n",
                "        batch_size=config[\"training\"][\"batch_size\"],\n",
                "        shuffle=True,\n",
                "        num_workers=4,\n",
                "        pin_memory=True\n",
                "    )\n",
                "    val_loader = DataLoader(\n",
                "        val_dataset,\n",
                "        batch_size=config[\"training\"][\"batch_size\"] * 2,\n",
                "        shuffle=False,\n",
                "        num_workers=4\n",
                "    )\n",
                "\n",
                "    optimizer = optim.AdamW(\n",
                "        model.parameters(), \n",
                "        lr=config[\"training\"][\"learning_rate\"],\n",
                "        weight_decay=config[\"training\"][\"weight_decay\"]\n",
                "    )\n",
                "    criterion = nn.CrossEntropyLoss()\n",
                "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config[\"training\"][\"num_epochs\"])\n",
                "    early_stopping = EarlyStopping(patience=config[\"training\"][\"patience\"])\n",
                "    \n",
                "    # Modern AMP\n",
                "    scaler = torch.cuda.amp.GradScaler() if device.type == \"cuda\" else None\n",
                "\n",
                "    logger.info(f\"Starting PEAK Vision Training with {config['model']['backbone']}...\")\n",
                "    accumulation_steps = config[\"training\"][\"grad_accum_steps\"]\n",
                "\n",
                "    for epoch in range(config[\"training\"][\"num_epochs\"]):\n",
                "        model.train()\n",
                "        running_loss = 0.0\n",
                "        correct = 0\n",
                "        total = 0\n",
                "\n",
                "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
                "        optimizer.zero_grad()\n",
                "\n",
                "        for i, (images, labels) in enumerate(pbar):\n",
                "            images, labels = images.to(device), labels.to(device)\n",
                "            \n",
                "            # Mixed Precision Training\n",
                "            if scaler:\n",
                "                with torch.cuda.amp.autocast():\n",
                "                    outputs = model(images)\n",
                "                    loss = criterion(outputs, labels) / accumulation_steps\n",
                "                scaler.scale(loss).backward()\n",
                "                \n",
                "                if (i + 1) % accumulation_steps == 0:\n",
                "                    scaler.step(optimizer)\n",
                "                    scaler.update()\n",
                "                    optimizer.zero_grad()\n",
                "            else:\n",
                "                outputs = model(images)\n",
                "                loss = criterion(outputs, labels) / accumulation_steps\n",
                "                loss.backward()\n",
                "                if (i + 1) % accumulation_steps == 0:\n",
                "                    optimizer.step()\n",
                "                    optimizer.zero_grad()\n",
                "\n",
                "            running_loss += loss.item() * accumulation_steps\n",
                "            _, predicted = outputs.max(1)\n",
                "            total += labels.size(0)\n",
                "            correct += predicted.eq(labels).sum().item()\n",
                "            \n",
                "            pbar.set_postfix({'loss': running_loss / (i+1)})\n",
                "\n",
                "        scheduler.step()\n",
                "        train_acc = 100 * correct / total\n",
                "        \n",
                "        # Validation\n",
                "        model.eval()\n",
                "        val_loss = 0.0\n",
                "        val_correct = 0\n",
                "        val_total = 0\n",
                "        with torch.no_grad():\n",
                "            for images, labels in val_loader:\n",
                "                images, labels = images.to(device), labels.to(device)\n",
                "                outputs = model(images)\n",
                "                loss = criterion(outputs, labels)\n",
                "                val_loss += loss.item()\n",
                "                _, predicted = outputs.max(1)\n",
                "                val_total += labels.size(0)\n",
                "                val_correct += predicted.eq(labels).sum().item()\n",
                "\n",
                "        val_acc = 100 * val_correct / val_total\n",
                "        val_loss /= len(val_loader)\n",
                "\n",
                "        print(f\"Ep {epoch+1}: Train Acc {train_acc:.2f}%, Val Loss {val_loss:.4f}, Val Acc {val_acc:.2f}%\")\n",
                "\n",
                "        if early_stopping(val_acc):\n",
                "            print(\"Early stopping triggered\")\n",
                "            break\n",
                "            \n",
                "    return model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_synthetic_graph_data(num_nodes=1000, feat_dim=128):\n",
                "    x = torch.randn(num_nodes, feat_dim)\n",
                "    \n",
                "    # Generate random edges\n",
                "    src = torch.randint(0, num_nodes, (num_nodes * 10,))\n",
                "    dst = torch.randint(0, num_nodes, (num_nodes * 10,))\n",
                "    edge_index = torch.stack([src, dst], dim=0)\n",
                "    \n",
                "    # Generate dummy labels (0: not upcyclable, 1: upcyclable)\n",
                "    # In Link Prediction, negative sampling is used, so valid edges are positives\n",
                "    return Data(x=x, edge_index=edge_index)\n",
                "\n",
                "# PEAK STANDARD GNN: GATv2\n",
                "class GATv2Model(nn.Module):\n",
                "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=4, heads=8, dropout=0.3):\n",
                "        super().__init__()\n",
                "        self.convs = nn.ModuleList()\n",
                "        \n",
                "        # Multi-head attention layers\n",
                "        self.convs.append(GATv2Conv(in_channels, hidden_channels, heads=heads, concat=True, dropout=dropout))\n",
                "        for _ in range(num_layers - 2):\n",
                "            self.convs.append(GATv2Conv(hidden_channels * heads, hidden_channels, heads=heads, concat=True, dropout=dropout))\n",
                "        \n",
                "        # Output layer\n",
                "        self.convs.append(GATv2Conv(hidden_channels * heads, out_channels, heads=1, concat=False, dropout=dropout))\n",
                "        \n",
                "        self.dropout = dropout\n",
                "        self.norm = nn.ModuleList([nn.LayerNorm(hidden_channels * heads) for _ in range(num_layers - 1)])\n",
                "\n",
                "    def forward(self, x, edge_index):\n",
                "        for i, conv in enumerate(self.convs[:-1]):\n",
                "            x = conv(x, edge_index)\n",
                "            x = self.norm[i](x)\n",
                "            x = F.gelu(x)\n",
                "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
                "        return self.convs[-1](x, edge_index)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_gnn_model():\n",
                "    set_seed()\n",
                "    device = get_device()\n",
                "    logger.info(f\"Using device: {device}\")\n",
                "    \n",
                "    # Config\n",
                "    in_dim = 128\n",
                "    hidden_dim = 512    # Peak Capacity\n",
                "    out_dim = 256\n",
                "    lr = 0.0005\n",
                "    epochs = 100\n",
                "    \n",
                "    # Synthetic Data with higher complexity\n",
                "    data = generate_synthetic_graph_data(num_nodes=5000, feat_dim=128).to(device)\n",
                "    \n",
                "    model = GATv2Model(in_dim, hidden_dim, out_dim).to(device)\n",
                "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
                "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.5)\n",
                "    \n",
                "    logger.info(\"Starting PEAK GNN Training...\")\n",
                "    for epoch in range(epochs):\n",
                "        model.train()\n",
                "        optimizer.zero_grad()\n",
                "        \n",
                "        z = model(data.x, data.edge_index)\n",
                "        \n",
                "        # Link Prediction\n",
                "        pos_loss = -torch.log(torch.sigmoid((z[data.edge_index[0]] * z[data.edge_index[1]]).sum(dim=1)) + 1e-15).mean()\n",
                "        \n",
                "        # Hard negative mining (random for demo)\n",
                "        neg_src = torch.randint(0, data.num_nodes, (data.edge_index.size(1),), device=device)\n",
                "        neg_dst = torch.randint(0, data.num_nodes, (data.edge_index.size(1),), device=device)\n",
                "        neg_loss = -torch.log(1 - torch.sigmoid((z[neg_src] * z[neg_dst]).sum(dim=1)) + 1e-15).mean()\n",
                "        \n",
                "        loss = pos_loss + neg_loss\n",
                "        loss.backward()\n",
                "        \n",
                "        # Gradient clipping\n",
                "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
                "        optimizer.step()\n",
                "        scheduler.step(loss)\n",
                "        \n",
                "        if (epoch + 1) % 10 == 0:\n",
                "            print(f\"Ep {epoch+1}: Loss {loss.item():.4f}\")\n",
                "            \n",
                "    return model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if __name__ == \"__main__\":\n",
                "    # 1. Train Vision Model\n",
                "    if os.path.exists(VISION_CONFIG[\"data\"][\"train_dir\"]):\n",
                "        vision_model = train_vision_model(VISION_CONFIG)\n",
                "        torch.save(vision_model.state_dict(), \"best_vision_eva02.pth\")\n",
                "        print(\"Vision model saved.\")\n",
                "    else:\n",
                "        print(\"Dataset not found. Please attach 'Garbage Classification' dataset.\")\n",
                "\n",
                "    # 2. Train GNN Model\n",
                "    gnn_model = train_gnn_model()\n",
                "    torch.save(gnn_model.state_dict(), \"best_gnn_gatv2.pth\")\n",
                "    print(\"GNN model saved.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}