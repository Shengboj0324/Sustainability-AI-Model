{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install -q timm torch-geometric torch-scatter torch-sparse albumentations wandb einops"
   ],
   "id": "5ea08bee3f5b78be"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "# PEAK STANDARD Libraries\n",
    "import timm\n",
    "from timm.data import create_transform, resolve_data_config\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "from tqdm.notebook import tqdm\n",
    "import wandb"
   ],
   "id": "80c56f6f58659d2b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    \"\"\"\n",
    "    Set entire environment seed for reproducibility including CUDA benchmarks.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    logger.info(f\"Random seed set to {seed}\")\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "def find_dataset_path(preferred_path: str) -> Path:\n",
    "    \"\"\"\n",
    "    Robustly find the dataset path in Kaggle's input directory.\n",
    "    If the preferred path doesn't exist, it searches for a directory containing images.\n",
    "    \"\"\"\n",
    "    preferred = Path(preferred_path)\n",
    "    if preferred.exists() and any(preferred.iterdir()):\n",
    "        logger.info(f\"Dataset found at: {preferred}\")\n",
    "        return preferred\n",
    "    \n",
    "    logger.warning(f\"Preferred path {preferred} not found. Searching /kaggle/input...\")\n",
    "    base = Path(\"/kaggle/input\")\n",
    "    if not base.exists():\n",
    "        logger.warning(\"Run is not on Kaggle or input directory missing.\")\n",
    "        return preferred # Return default and let it fail later if not on Kaggle\n",
    "\n",
    "    # BFS Search for a folder that looks like a dataset (has subfolders with images)\n",
    "    for root, dirs, files in os.walk(base):\n",
    "        if len(dirs) > 1: # Candidate for class folders\n",
    "            # Check if first subfolder contains images\n",
    "            first_sub = Path(root) / dirs[0]\n",
    "            if any(f.lower().endswith(('.jpg', '.png', '.jpeg')) for f in os.listdir(first_sub)):\n",
    "                found_path = Path(root)\n",
    "                logger.info(f\"Dataset discovered at: {found_path}\")\n",
    "                return found_path\n",
    "    \n",
    "    logger.error(\"Could not automatically locate a valid dataset.\")\n",
    "    return preferred\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=15, mode=\"max\", delta=0):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.mode = mode\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, current_score):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = current_score\n",
    "        elif self.mode == \"max\":\n",
    "            if current_score <= self.best_score + self.delta:\n",
    "                self.counter += 1\n",
    "                if self.counter >= self.patience:\n",
    "                    self.early_stop = True\n",
    "            else:\n",
    "                self.best_score = current_score\n",
    "                self.counter = 0\n",
    "        return self.early_stop"
   ],
   "id": "e37d70388c34be67"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PEAK STANDARD CONFIGURATION\n",
    "VISION_CONFIG = {\n",
    "    \"model\": {\n",
    "        # SOTA Backbone: EVA-02 Large\n",
    "        # Optimized for fine-grained classification\n",
    "        \"backbone\": \"eva02_large_patch14_448.mim_m38m_ft_in22k_in1k\",\n",
    "        \"pretrained\": True,\n",
    "        \"num_classes\": 12,\n",
    "        \"drop_rate\": 0.3,      # Increased regularization for large model\n",
    "        \"drop_path_rate\": 0.2\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"train_dir\": \"/kaggle/input/garbage-classification/garbage_classification/\",\n",
    "        \"input_size\": 448,  # High Resolution\n",
    "        \"num_workers\": 2,   # Kaggle often limits shared mem, 2 is safe\n",
    "        \"pin_memory\": True\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"batch_size\": 8,     # Reduced for memory safety with Large model @ 448px\n",
    "        \"grad_accum_steps\": 8, # Effective batch size = 64\n",
    "        \"learning_rate\": 5e-5, # Lower LR for finetuning large model\n",
    "        \"weight_decay\": 0.05,\n",
    "        \"num_epochs\": 20,    # Large models converge faster\n",
    "        \"patience\": 5\n",
    "    }\n",
    "}"
   ],
   "id": "e85800f6504ff2e9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.transform = transform\n",
    "        if not self.root_dir.exists():\n",
    "            # Empty initialization if path is wrong, handled in main\n",
    "            self.classes = []\n",
    "            self.images = []\n",
    "            return\n",
    "            \n",
    "        self.classes = sorted([d.name for d in self.root_dir.iterdir() if d.is_dir()])\n",
    "        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
    "        self.images = self._load_images()\n",
    "\n",
    "    def _load_images(self):\n",
    "        images = []\n",
    "        for class_name in self.classes:\n",
    "            class_dir = self.root_dir / class_name\n",
    "            for img_path in class_dir.glob(\"*\"):\n",
    "                if img_path.suffix.lower() in [\".jpg\", \".jpeg\", \".png\", \".bmp\"]:\n",
    "                    images.append((img_path, self.class_to_idx[class_name]))\n",
    "        return images\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.images[idx]\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, label\n",
    "        except Exception as e:\n",
    "            # Return a blank image to avoid crashing training\n",
    "            logger.error(f\"Error loading {img_path}: {e}\")\n",
    "            return torch.zeros((3, 448, 448)), label"
   ],
   "id": "4a83856bf02ace6d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vision_transforms(config, model, is_train=True):\n",
    "    \"\"\"\n",
    "    Creates SOTA transforms using timm's factory methods.\n",
    "    Guarantees compatibility with the model's pretraining (normalization, etc.)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use timm's data config to get optimal normalization and resolution\n",
    "        data_config = resolve_data_config(model.default_cfg, model=model)\n",
    "        \n",
    "        if is_train:\n",
    "            # State-of-the-Art Training Transforms\n",
    "            # Includes: AutoAugment/RandAugment, RandomErasing, ColorJitter\n",
    "            return create_transform(\n",
    "                input_size=data_config['input_size'],\n",
    "                is_training=True,\n",
    "                use_prefetcher=False, # We use PyTorch DataLoader\n",
    "                no_aug=False,\n",
    "                scale=(0.08, 1.0),\n",
    "                ratio=(0.75, 1.33),\n",
    "                hflip=0.5,\n",
    "                vflip=0.0,\n",
    "                color_jitter=0.4,\n",
    "                auto_augment='rand-m9-mstd0.5-inc1',  # Robust RandAugment policy\n",
    "                interpolation=data_config['interpolation'],\n",
    "                mean=data_config['mean'],\n",
    "                std=data_config['std'],\n",
    "                re_prob=0.25, # Random Erasing Probability\n",
    "                re_mode='pixel',\n",
    "                re_count=1,\n",
    "            )\n",
    "        else:\n",
    "            # Exact validation transforms used during pretraining\n",
    "            return create_transform(\n",
    "                input_size=data_config['input_size'],\n",
    "                is_training=False,\n",
    "                use_prefetcher=False,\n",
    "                interpolation=data_config['interpolation'],\n",
    "                mean=data_config['mean'],\n",
    "                std=data_config['std'],\n",
    "            )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to create transforms: {e}\")\n",
    "        raise"
   ],
   "id": "8cac2161a057a33e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vision_model(config):\n",
    "    \"\"\"\n",
    "    Creates the vision model with specific attention to SOTA architecture support.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Creating model: {config['model']['backbone']}\")\n",
    "    model = timm.create_model(\n",
    "        config[\"model\"][\"backbone\"],\n",
    "        pretrained=config[\"model\"][\"pretrained\"],\n",
    "        num_classes=config[\"model\"][\"num_classes\"],\n",
    "        drop_rate=config[\"model\"][\"drop_rate\"],\n",
    "        drop_path_rate=config[\"model\"][\"drop_path_rate\"]\n",
    "    )\n",
    "    return model"
   ],
   "id": "20be2291a212f609"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vision_model(config):\n",
    "    set_seed()\n",
    "    device = get_device()\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "    \n",
    "    # 1. Initialize Model\n",
    "    model = create_vision_model(config).to(device)\n",
    "    \n",
    "    # 2. Setup Data Pipeline\n",
    "    train_transform = get_vision_transforms(config, model, is_train=True)\n",
    "    val_transform = get_vision_transforms(config, model, is_train=False)\n",
    "\n",
    "    # Robust Path Discovery\n",
    "    data_path = find_dataset_path(config[\"data\"][\"train_dir\"])\n",
    "    full_dataset = VisionDataset(data_path, transform=train_transform)\n",
    "\n",
    "    train_size = int(0.85 * len(full_dataset))\n",
    "    val_size = len(full_dataset) - train_size\n",
    "    if train_size == 0:\n",
    "        logger.error(\"Dataset is empty or path is incorrect. Aborting training.\")\n",
    "        return None\n",
    "\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
    "    # Important: Apply correct transforms to validation set\n",
    "    val_dataset.dataset.transform = val_transform\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config[\"training\"][\"batch_size\"],\n",
    "        shuffle=True,\n",
    "        num_workers=config[\"data\"][\"num_workers\"],\n",
    "        pin_memory=config[\"data\"][\"pin_memory\"]\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config[\"training\"][\"batch_size\"] * 2,\n",
    "        shuffle=False,\n",
    "        num_workers=config[\"data\"][\"num_workers\"]\n",
    "    )\n",
    "\n",
    "    # 3. Optimization Setup\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(), \n",
    "        lr=config[\"training\"][\"learning_rate\"],\n",
    "        weight_decay=config[\"training\"][\"weight_decay\"]\n",
    "    )\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config[\"training\"][\"num_epochs\"])\n",
    "    early_stopping = EarlyStopping(patience=config[\"training\"][\"patience\"])\n",
    "    \n",
    "    # 4. Mixed Precision Setup (Robust)\n",
    "    use_amp = (device.type == \"cuda\")\n",
    "    scaler = torch.cuda.amp.GradScaler() if use_amp else None\n",
    "    logger.info(f\"Mixed Precision Enabled: {use_amp}\")\n",
    "\n",
    "    # 5. Training Loop\n",
    "    accumulation_steps = config[\"training\"][\"grad_accum_steps\"]\n",
    "    \n",
    "    # Try to init WandB silently (fails safely if no login)\n",
    "    try: \n",
    "        wandb.init(project=\"sustainability-vision\", config=config) \n",
    "    except: \n",
    "        pass\n",
    "\n",
    "    for epoch in range(config[\"training\"][\"num_epochs\"]):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['training']['num_epochs']}\")\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for i, (images, labels) in enumerate(pbar):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # AMP Forward Pass\n",
    "            if use_amp:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels) / accumulation_steps\n",
    "                scaler.scale(loss).backward()\n",
    "                \n",
    "                if (i + 1) % accumulation_steps == 0:\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                    optimizer.zero_grad()\n",
    "            else:\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels) / accumulation_steps\n",
    "                loss.backward()\n",
    "                if (i + 1) % accumulation_steps == 0:\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "            running_loss += loss.item() * accumulation_steps\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            # Update ProgressBar\n",
    "            current_loss = running_loss / (i + 1)\n",
    "            pbar.set_postfix({'loss': f\"{current_loss:.4f}\"})\n",
    "\n",
    "        scheduler.step()\n",
    "        train_acc = 100 * correct / total\n",
    "        \n",
    "        # Validation Loop\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        print(f\"Ep {epoch+1}: Train Acc {train_acc:.2f}%, Val Loss {val_loss:.4f}, Val Acc {val_acc:.2f}%\")\n",
    "        \n",
    "        # Safe Log\n",
    "        try: wandb.log({\"train_acc\": train_acc, \"val_acc\": val_acc, \"val_loss\": val_loss}) \n",
    "        except: pass\n",
    "\n",
    "        if early_stopping(val_acc):\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "            \n",
    "    return model"
   ],
   "id": "56b93d44ea191d12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PEAK STANDARD GNN\n",
    "# Using Graph Attention Networks v2 (GATv2) for superior expressive power\n",
    "\n",
    "def generate_synthetic_graph_data(num_nodes=5000, feat_dim=128):\n",
    "    \"\"\"\n",
    "    Generates a synthetic graph to demonstrate GNN training logic.\n",
    "    In production, this would load the Knowledge Graph export.\n",
    "    \"\"\"\n",
    "    x = torch.randn(num_nodes, feat_dim)\n",
    "    \n",
    "    # Generate random edges with valid indices\n",
    "    src = torch.randint(0, num_nodes, (num_nodes * 10,), dtype=torch.long)\n",
    "    dst = torch.randint(0, num_nodes, (num_nodes * 10,), dtype=torch.long)\n",
    "    edge_index = torch.stack([src, dst], dim=0)\n",
    "    \n",
    "    return Data(x=x, edge_index=edge_index)\n",
    "\n",
    "class GATv2Model(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=4, heads=4, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        \n",
    "        # First layer\n",
    "        self.convs.append(GATv2Conv(in_channels, hidden_channels, heads=heads, concat=True, dropout=dropout))\n",
    "        \n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(GATv2Conv(hidden_channels * heads, hidden_channels, heads=heads, concat=True, dropout=dropout))\n",
    "        \n",
    "        # Output layer (Embedding size = out_channels)\n",
    "        self.convs.append(GATv2Conv(hidden_channels * heads, out_channels, heads=1, concat=False, dropout=dropout))\n",
    "        \n",
    "        self.dropout = dropout\n",
    "        self.norm = nn.ModuleList([nn.LayerNorm(hidden_channels * heads) for _ in range(num_layers - 1)])\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for i, conv in enumerate(self.convs[:-1]):\n",
    "            x = conv(x, edge_index)\n",
    "            x = self.norm[i](x)\n",
    "            x = F.gelu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        return self.convs[-1](x, edge_index)"
   ],
   "id": "a413b4ee062ae6a8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gnn_model():\n",
    "    set_seed()\n",
    "    device = get_device()\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "    \n",
    "    # Config\n",
    "    in_dim = 128\n",
    "    hidden_dim = 256    # Tuned for T4\n",
    "    out_dim = 256\n",
    "    lr = 0.001\n",
    "    epochs = 50\n",
    "    \n",
    "    data = generate_synthetic_graph_data(num_nodes=5000, feat_dim=128).to(device)\n",
    "    \n",
    "    model = GATv2Model(in_dim, hidden_dim, out_dim).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n",
    "    \n",
    "    logger.info(\"Starting PEAK GNN Training...\")\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        z = model(data.x, data.edge_index)\n",
    "        \n",
    "        # Unsupervised Link Prediction Loss\n",
    "        # Positive edges: Existing edges in graph\n",
    "        pos_src, pos_dst = data.edge_index\n",
    "        pos_loss = -torch.log(torch.sigmoid((z[pos_src] * z[pos_dst]).sum(dim=1)) + 1e-15).mean()\n",
    "        \n",
    "        # Negative edges: Random pairs\n",
    "        neg_src = torch.randint(0, data.num_nodes, (pos_src.size(0),), device=device)\n",
    "        neg_dst = torch.randint(0, data.num_nodes, (pos_src.size(0),), device=device)\n",
    "        neg_loss = -torch.log(1 - torch.sigmoid((z[neg_src] * z[neg_dst]).sum(dim=1)) + 1e-15).mean()\n",
    "        \n",
    "        loss = pos_loss + neg_loss\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient Clipping for Stability\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step(loss)\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Ep {epoch+1}: Loss {loss.item():.4f}\")\n",
    "            \n",
    "    return model"
   ],
   "id": "d48fee1a1cb8a264"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 1. Train Vision Model\n",
    "    # We search for the dataset robustly\n",
    "    vision_model = train_vision_model(VISION_CONFIG)\n",
    "    if vision_model:\n",
    "        torch.save(vision_model.state_dict(), \"best_vision_eva02.pth\")\n",
    "        print(\"Vision model artifacts saved successfully.\")\n",
    "\n",
    "    # 2. Train GNN Model\n",
    "    gnn_model = train_gnn_model()\n",
    "    torch.save(gnn_model.state_dict(), \"best_gnn_gatv2.pth\")\n",
    "    print(\"GNN model artifacts saved successfully.\")"
   ],
   "id": "78cc46781594be13"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
