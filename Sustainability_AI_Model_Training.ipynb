{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-07T05:20:24.618148Z",
     "start_time": "2026-02-07T05:20:22.446873Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "print(f\"Python: {sys.executable}\")\n",
    "\n",
    "# Uninstall NumPy 2.x\n",
    "print(\"Uninstalling NumPy 2.x...\")\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \"numpy\"], check=True)\n",
    "\n",
    "# Install NumPy 1.26.4\n",
    "print(\"Installing NumPy 1.26.4...\")\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"numpy==1.26.4\"], check=True)\n",
    "\n",
    "print(\"\\n‚úÖ DONE! Now restart kernel: Kernel ‚Üí Restart Kernel\")"
   ],
   "id": "357da545f4ba0043",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: /Applications/Xcode.app/Contents/Developer/usr/bin/python3\n",
      "Uninstalling NumPy 2.x...\n",
      "Found existing installation: numpy 2.0.2\n",
      "Uninstalling numpy-2.0.2:\n",
      "  Successfully uninstalled numpy-2.0.2\n",
      "Installing NumPy 1.26.4...\n",
      "Defaulting to user installation because normal site-packages is not writeable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[33mWARNING: Ignoring invalid distribution -cipy (/Users/jiangshengbo/Library/Python/3.9/lib/python/site-packages)\u001B[0m\u001B[33m\n",
      "\u001B[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.26.4\n",
      "  Using cached numpy-1.26.4-cp39-cp39-macosx_11_0_arm64.whl.metadata (61 kB)\n",
      "Using cached numpy-1.26.4-cp39-cp39-macosx_11_0_arm64.whl (14.0 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[33mWARNING: Ignoring invalid distribution -cipy (/Users/jiangshengbo/Library/Python/3.9/lib/python/site-packages)\u001B[0m\u001B[33m\n",
      "\u001B[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: numpy\n",
      "Successfully installed numpy-1.26.4\n",
      "\n",
      "‚úÖ DONE! Now restart kernel: Kernel ‚Üí Restart Kernel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[33m  WARNING: The script f2py is installed in '/Users/jiangshengbo/Library/Python/3.9/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "chromadb 1.1.0 requires tenacity>=8.2.3, but you have tenacity 8.2.0 which is incompatible.\n",
      "great-expectations 0.17.0 requires pydantic<2.0,>=1.9.2, but you have pydantic 2.12.3 which is incompatible.\n",
      "opencv-python-headless 4.13.0.92 requires numpy>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001B[0m\u001B[31m\n",
      "\u001B[0m\u001B[33mWARNING: Ignoring invalid distribution -cipy (/Users/jiangshengbo/Library/Python/3.9/lib/python/site-packages)\u001B[0m\u001B[33m\n",
      "\u001B[0m"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-07T05:20:31.631016Z",
     "start_time": "2026-02-07T05:20:24.625315Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# Uninstall broken packages\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \"scipy\", \"albumentations\", \"opencv-python\", \"opencv-python-headless\"])\n",
    "\n",
    "# Install in correct order\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"--no-cache-dir\", \"scipy>=1.7.0,<1.15.0\"])\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"--no-cache-dir\", \"opencv-python-headless>=4.5.0\"])\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"--no-cache-dir\", \"albumentations>=1.3.0\"])\n",
    "\n",
    "print(\"‚úÖ Done! Now restart the kernel: Kernel ‚Üí Restart Kernel\")"
   ],
   "id": "f314169c67b2a3a1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: scipy 1.13.1\n",
      "Uninstalling scipy-1.13.1:\n",
      "  Successfully uninstalled scipy-1.13.1\n",
      "Found existing installation: albumentations 2.0.8\n",
      "Uninstalling albumentations-2.0.8:\n",
      "  Successfully uninstalled albumentations-2.0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[33mWARNING: Ignoring invalid distribution -cipy (/Users/jiangshengbo/Library/Python/3.9/lib/python/site-packages)\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mWARNING: Skipping opencv-python as it is not installed.\u001B[0m\u001B[33m\n",
      "\u001B[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: opencv-python-headless 4.13.0.92\n",
      "Uninstalling opencv-python-headless-4.13.0.92:\n",
      "  Successfully uninstalled opencv-python-headless-4.13.0.92\n",
      "Defaulting to user installation because normal site-packages is not writeable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[33mWARNING: Ignoring invalid distribution -cipy (/Users/jiangshengbo/Library/Python/3.9/lib/python/site-packages)\u001B[0m\u001B[33m\n",
      "\u001B[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scipy<1.15.0,>=1.7.0\n",
      "  Downloading scipy-1.13.1-cp39-cp39-macosx_12_0_arm64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: numpy<2.3,>=1.22.4 in /Users/jiangshengbo/Library/Python/3.9/lib/python/site-packages (from scipy<1.15.0,>=1.7.0) (1.26.4)\n",
      "Downloading scipy-1.13.1-cp39-cp39-macosx_12_0_arm64.whl (30.3 MB)\n",
      "\u001B[2K   \u001B[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m \u001B[32m30.3/30.3 MB\u001B[0m \u001B[31m44.5 MB/s\u001B[0m  \u001B[33m0:00:00\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n",
      "\u001B[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[33mWARNING: Ignoring invalid distribution -cipy (/Users/jiangshengbo/Library/Python/3.9/lib/python/site-packages)\u001B[0m\u001B[33m\n",
      "\u001B[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: scipy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "great-expectations 0.17.0 requires pydantic<2.0,>=1.9.2, but you have pydantic 2.12.3 which is incompatible.\u001B[0m\u001B[31m\n",
      "\u001B[0m\u001B[33mWARNING: Ignoring invalid distribution -cipy (/Users/jiangshengbo/Library/Python/3.9/lib/python/site-packages)\u001B[0m\u001B[33m\n",
      "\u001B[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed scipy-1.13.1\n",
      "Defaulting to user installation because normal site-packages is not writeable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[33mWARNING: Ignoring invalid distribution -cipy (/Users/jiangshengbo/Library/Python/3.9/lib/python/site-packages)\u001B[0m\u001B[33m\n",
      "\u001B[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python-headless>=4.5.0\n",
      "  Downloading opencv_python_headless-4.13.0.92-cp37-abi3-macosx_13_0_arm64.whl.metadata (19 kB)\n",
      "Collecting numpy>=2 (from opencv-python-headless>=4.5.0)\n",
      "  Downloading numpy-2.0.2-cp39-cp39-macosx_14_0_arm64.whl.metadata (60 kB)\n",
      "Downloading opencv_python_headless-4.13.0.92-cp37-abi3-macosx_13_0_arm64.whl (46.2 MB)\n",
      "\u001B[2K   \u001B[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m \u001B[32m46.2/46.2 MB\u001B[0m \u001B[31m60.3 MB/s\u001B[0m  \u001B[33m0:00:00\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n",
      "\u001B[?25hDownloading numpy-2.0.2-cp39-cp39-macosx_14_0_arm64.whl (5.3 MB)\n",
      "\u001B[2K   \u001B[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m \u001B[32m5.3/5.3 MB\u001B[0m \u001B[31m73.9 MB/s\u001B[0m  \u001B[33m0:00:00\u001B[0m\n",
      "\u001B[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[33mWARNING: Ignoring invalid distribution -cipy (/Users/jiangshengbo/Library/Python/3.9/lib/python/site-packages)\u001B[0m\u001B[33m\n",
      "\u001B[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: numpy, opencv-python-headless\n",
      "\u001B[2K  Attempting uninstall: numpy\n",
      "\u001B[2K    Found existing installation: numpy 1.26.4\n",
      "\u001B[2K    Uninstalling numpy-1.26.4:\n",
      "\u001B[2K      Successfully uninstalled numpy-1.26.4\n",
      "\u001B[2K   \u001B[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m \u001B[32m0/2\u001B[0m [numpy]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[33m  WARNING: The scripts f2py and numpy-config are installed in '/Users/jiangshengbo/Library/Python/3.9/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001B[0m\u001B[33m\n",
      "\u001B[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[2K   \u001B[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m \u001B[32m2/2\u001B[0m [opencv-python-headless]v-python-headless]\n",
      "\u001B[1A\u001B[2KSuccessfully installed numpy-2.0.2 opencv-python-headless-4.13.0.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "chromadb 1.1.0 requires tenacity>=8.2.3, but you have tenacity 8.2.0 which is incompatible.\n",
      "great-expectations 0.17.0 requires pydantic<2.0,>=1.9.2, but you have pydantic 2.12.3 which is incompatible.\u001B[0m\u001B[31m\n",
      "\u001B[0m\u001B[33mWARNING: Ignoring invalid distribution -cipy (/Users/jiangshengbo/Library/Python/3.9/lib/python/site-packages)\u001B[0m\u001B[33m\n",
      "\u001B[0mTraceback (most recent call last):\n",
      "  File \"/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/Users/jiangshengbo/Library/Python/3.9/lib/python/site-packages/pip/__main__.py\", line 24, in <module>\n",
      "    sys.exit(_main())\n",
      "  File \"/Users/jiangshengbo/Library/Python/3.9/lib/python/site-packages/pip/_internal/cli/main.py\", line 47, in main\n",
      "    from pip._internal.cli.autocompletion import autocomplete\n",
      "  File \"/Users/jiangshengbo/Library/Python/3.9/lib/python/site-packages/pip/_internal/cli/autocompletion.py\", line 12, in <module>\n",
      "    from pip._internal.cli.main_parser import create_main_parser\n",
      "  File \"/Users/jiangshengbo/Library/Python/3.9/lib/python/site-packages/pip/_internal/cli/main_parser.py\", line 11, in <module>\n",
      "    from pip._internal.build_env import get_runnable_pip\n",
      "  File \"/Users/jiangshengbo/Library/Python/3.9/lib/python/site-packages/pip/_internal/build_env.py\", line 22, in <module>\n",
      "    from pip._internal.cli.spinners import open_rich_spinner, open_spinner\n",
      "  File \"/Users/jiangshengbo/Library/Python/3.9/lib/python/site-packages/pip/_internal/cli/spinners.py\", line 11, in <module>\n",
      "    from pip._vendor.rich.console import (\n",
      "  File \"/Users/jiangshengbo/Library/Python/3.9/lib/python/site-packages/pip/_vendor/rich/console.py\", line 53, in <module>\n",
      "    from .pretty import Pretty, is_expandable\n",
      "  File \"/Users/jiangshengbo/Library/Python/3.9/lib/python/site-packages/pip/_vendor/rich/pretty.py\", line 33, in <module>\n",
      "    import attr as _attr_module\n",
      "  File \"/Users/jiangshengbo/Library/Python/3.9/lib/python/site-packages/attr/__init__.py\", line 10, in <module>\n",
      "    from . import converters, exceptions, filters, setters, validators\n",
      "  File \"/Users/jiangshengbo/Library/Python/3.9/lib/python/site-packages/attr/converters.py\", line 10, in <module>\n",
      "    from ._make import NOTHING, Converter, Factory, pipe\n",
      "  File \"/Users/jiangshengbo/Library/Python/3.9/lib/python/site-packages/attr/_make.py\", line 22, in <module>\n",
      "    from . import _compat, _config, setters\n",
      "  File \"/Users/jiangshengbo/Library/Python/3.9/lib/python/site-packages/attr/setters.py\", line 8, in <module>\n",
      "    from .exceptions import FrozenAttributeError\n",
      "  File \"/Users/jiangshengbo/Library/Python/3.9/lib/python/site-packages/attr/exceptions.py\", line 23, in <module>\n",
      "    class FrozenInstanceError(FrozenError):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 10\u001B[0m\n\u001B[1;32m      8\u001B[0m subprocess\u001B[38;5;241m.\u001B[39mrun([sys\u001B[38;5;241m.\u001B[39mexecutable, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m-m\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpip\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minstall\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m--no-cache-dir\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mscipy>=1.7.0,<1.15.0\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m      9\u001B[0m subprocess\u001B[38;5;241m.\u001B[39mrun([sys\u001B[38;5;241m.\u001B[39mexecutable, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m-m\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpip\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minstall\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m--no-cache-dir\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mopencv-python-headless>=4.5.0\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m---> 10\u001B[0m \u001B[43msubprocess\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43msys\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecutable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m-m\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpip\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43minstall\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m--no-cache-dir\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43malbumentations>=1.3.0\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m‚úÖ Done! Now restart the kernel: Kernel ‚Üí Restart Kernel\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/subprocess.py:507\u001B[0m, in \u001B[0;36mrun\u001B[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001B[0m\n\u001B[1;32m    505\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Popen(\u001B[38;5;241m*\u001B[39mpopenargs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs) \u001B[38;5;28;01mas\u001B[39;00m process:\n\u001B[1;32m    506\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 507\u001B[0m         stdout, stderr \u001B[38;5;241m=\u001B[39m \u001B[43mprocess\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcommunicate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    508\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m TimeoutExpired \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[1;32m    509\u001B[0m         process\u001B[38;5;241m.\u001B[39mkill()\n",
      "File \u001B[0;32m/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/subprocess.py:1126\u001B[0m, in \u001B[0;36mPopen.communicate\u001B[0;34m(self, input, timeout)\u001B[0m\n\u001B[1;32m   1124\u001B[0m         stderr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstderr\u001B[38;5;241m.\u001B[39mread()\n\u001B[1;32m   1125\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstderr\u001B[38;5;241m.\u001B[39mclose()\n\u001B[0;32m-> 1126\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1128\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/subprocess.py:1189\u001B[0m, in \u001B[0;36mPopen.wait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m   1187\u001B[0m     endtime \u001B[38;5;241m=\u001B[39m _time() \u001B[38;5;241m+\u001B[39m timeout\n\u001B[1;32m   1188\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1189\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_wait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m:\n\u001B[1;32m   1191\u001B[0m     \u001B[38;5;66;03m# https://bugs.python.org/issue25942\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m     \u001B[38;5;66;03m# The first keyboard interrupt waits briefly for the child to\u001B[39;00m\n\u001B[1;32m   1193\u001B[0m     \u001B[38;5;66;03m# exit under the common assumption that it also received the ^C\u001B[39;00m\n\u001B[1;32m   1194\u001B[0m     \u001B[38;5;66;03m# generated SIGINT and will exit rapidly.\u001B[39;00m\n\u001B[1;32m   1195\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/subprocess.py:1917\u001B[0m, in \u001B[0;36mPopen._wait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m   1915\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturncode \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1916\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m  \u001B[38;5;66;03m# Another thread waited.\u001B[39;00m\n\u001B[0;32m-> 1917\u001B[0m (pid, sts) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_try_wait\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1918\u001B[0m \u001B[38;5;66;03m# Check the pid and loop as waitpid has been known to\u001B[39;00m\n\u001B[1;32m   1919\u001B[0m \u001B[38;5;66;03m# return 0 even without WNOHANG in odd situations.\u001B[39;00m\n\u001B[1;32m   1920\u001B[0m \u001B[38;5;66;03m# http://bugs.python.org/issue14396.\u001B[39;00m\n\u001B[1;32m   1921\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m pid \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpid:\n",
      "File \u001B[0;32m/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/subprocess.py:1875\u001B[0m, in \u001B[0;36mPopen._try_wait\u001B[0;34m(self, wait_flags)\u001B[0m\n\u001B[1;32m   1873\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001B[39;00m\n\u001B[1;32m   1874\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1875\u001B[0m     (pid, sts) \u001B[38;5;241m=\u001B[39m \u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwaitpid\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpid\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwait_flags\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1876\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mChildProcessError\u001B[39;00m:\n\u001B[1;32m   1877\u001B[0m     \u001B[38;5;66;03m# This happens if SIGCLD is set to be ignored or waiting\u001B[39;00m\n\u001B[1;32m   1878\u001B[0m     \u001B[38;5;66;03m# for child processes has otherwise been disabled for our\u001B[39;00m\n\u001B[1;32m   1879\u001B[0m     \u001B[38;5;66;03m# process.  This child is dead, we can't get the status.\u001B[39;00m\n\u001B[1;32m   1880\u001B[0m     pid \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpid\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"Installing dependencies for Sustainability AI Model (MacBook Local Training)...\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Install packages one by one with error handling\n",
    "def install_package(package_spec, description=\"\"):\n",
    "    \"\"\"Install a package with error handling.\"\"\"\n",
    "    try:\n",
    "        print(f\"Installing {description or package_spec}...\")\n",
    "        subprocess.check_call(\n",
    "            [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--upgrade\"] + package_spec.split(),\n",
    "            timeout=300  # 5 minute timeout per package\n",
    "        )\n",
    "        print(f\"  ‚úÖ {description or package_spec}\")\n",
    "        return True\n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(f\"  ‚ö†Ô∏è  Timeout installing {description or package_spec}, skipping...\")\n",
    "        return False\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"  ‚ö†Ô∏è  Failed to install {description or package_spec}: {e}\")\n",
    "        return False\n",
    "\n",
    "# Upgrade pip first\n",
    "print(\"Upgrading pip...\")\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--upgrade\", \"pip\"], timeout=60)\n",
    "\n",
    "# Install Kaggle API\n",
    "install_package(\"kaggle\", \"Kaggle API\")\n",
    "\n",
    "# Install core dependencies (Python 3.9 compatible versions)\n",
    "install_package(\"numpy>=1.19.0,<2.0\", \"NumPy\")\n",
    "install_package(\"scipy>=1.7.0,<1.15.0\", \"SciPy\")\n",
    "install_package(\"Pillow>=8.0.0\", \"Pillow\")\n",
    "install_package(\"pandas>=1.3.0\", \"Pandas\")\n",
    "install_package(\"scikit-learn>=1.0.0\", \"scikit-learn\")\n",
    "install_package(\"matplotlib>=3.4.0\", \"Matplotlib\")\n",
    "install_package(\"seaborn>=0.11.0\", \"Seaborn\")\n",
    "install_package(\"tqdm>=4.62.0\", \"tqdm\")\n",
    "\n",
    "# Install PyTorch with compatible torchvision version\n",
    "print(\"Checking PyTorch installation...\")\n",
    "try:\n",
    "    import torch\n",
    "    import torchvision\n",
    "    torch_version = torch.__version__\n",
    "    torchvision_version = torchvision.__version__\n",
    "    print(f\"  Current PyTorch: {torch_version}\")\n",
    "    print(f\"  Current torchvision: {torchvision_version}\")\n",
    "\n",
    "    # Check if versions are compatible\n",
    "    # PyTorch 2.x needs torchvision 0.15+\n",
    "    # PyTorch 1.x needs torchvision 0.x\n",
    "    torch_major = int(torch_version.split('.')[0])\n",
    "    tv_major = int(torchvision_version.split('.')[0])\n",
    "\n",
    "    if torch_major == 2 and tv_major == 0 and int(torchvision_version.split('.')[1]) < 15:\n",
    "        print(\"  ‚ö†Ô∏è  Version mismatch detected! Reinstalling compatible versions...\")\n",
    "        subprocess.run([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \"torch\", \"torchvision\"], check=False)\n",
    "        install_package(\"torch==2.0.1 torchvision==0.15.2\", \"PyTorch 2.0.1 + torchvision 0.15.2\")\n",
    "    elif torch_major != tv_major:\n",
    "        print(\"  ‚ö†Ô∏è  Major version mismatch! Reinstalling compatible versions...\")\n",
    "        subprocess.run([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \"torch\", \"torchvision\"], check=False)\n",
    "        install_package(\"torch==2.0.1 torchvision==0.15.2\", \"PyTorch 2.0.1 + torchvision 0.15.2\")\n",
    "    else:\n",
    "        print(\"  ‚úÖ PyTorch and torchvision versions are compatible\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"  Installing PyTorch and torchvision...\")\n",
    "    install_package(\"torch==2.0.1 torchvision==0.15.2\", \"PyTorch 2.0.1 + torchvision 0.15.2\")\n",
    "except AttributeError as e:\n",
    "    print(f\"  ‚ö†Ô∏è  Version compatibility issue detected: {e}\")\n",
    "    print(\"  Reinstalling compatible PyTorch and torchvision versions...\")\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \"torch\", \"torchvision\"], check=False)\n",
    "    install_package(\"torch==2.0.1 torchvision==0.15.2\", \"PyTorch 2.0.1 + torchvision 0.15.2\")\n",
    "\n",
    "# Install timm (Python 3.9 compatible)\n",
    "install_package(\"timm>=0.9.0\", \"timm\")\n",
    "\n",
    "# Install albumentations\n",
    "install_package(\"albumentations>=1.3.0\", \"Albumentations\")\n",
    "\n",
    "# Install other dependencies\n",
    "install_package(\"einops>=0.6.0\", \"einops\")\n",
    "install_package(\"wandb>=0.15.0\", \"Weights & Biases\")\n",
    "\n",
    "# Install PyTorch Geometric (simplified for Python 3.9)\n",
    "print(\"Installing PyTorch Geometric...\")\n",
    "install_package(\"torch-geometric\", \"PyTorch Geometric\")\n",
    "\n",
    "# Try to install torch-scatter and torch-sparse (optional, may fail on some systems)\n",
    "print(\"Installing optional PyG dependencies (may fail, that's OK)...\")\n",
    "try:\n",
    "    subprocess.run(\n",
    "        [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"torch-scatter\", \"torch-sparse\"],\n",
    "        timeout=300,\n",
    "        check=False  # Don't fail if this doesn't work\n",
    "    )\n",
    "    print(\"  ‚úÖ torch-scatter and torch-sparse installed\")\n",
    "except:\n",
    "    print(\"  ‚ö†Ô∏è  torch-scatter/torch-sparse installation skipped (optional)\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"‚úÖ Core dependencies installed successfully!\")\n",
    "print(\"=\"*60)\n"
   ],
   "id": "f90b43bdd0cb863b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# KAGGLE API SETUP AND DATASET DOWNLOAD\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üîë CONFIGURING KAGGLE API\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# IMPORTANT: ENTER YOUR KAGGLE CREDENTIALS HERE\n",
    "# ============================================================================\n",
    "#\n",
    "# To find your Kaggle username and API key:\n",
    "# 1. Go to https://www.kaggle.com/\n",
    "# 2. Click on your profile picture (top right)\n",
    "# 3. Click \"Settings\"\n",
    "# 4. Scroll down to \"API\" section\n",
    "# 5. Click \"Create New Token\" (downloads kaggle.json)\n",
    "# 6. Open kaggle.json and copy the username and key below\n",
    "#\n",
    "# ============================================================================\n",
    "\n",
    "KAGGLE_USERNAME = \"michealjiang\"  # Your Kaggle username\n",
    "KAGGLE_KEY = \"92ce58a4cc3d98ed20dca81b8598123f\"  # Your Kaggle API key\n",
    "\n",
    "# Alternative: If you already have kaggle.json, we can read it\n",
    "kaggle_json_path = Path.home() / \".kaggle\" / \"kaggle.json\"\n",
    "if kaggle_json_path.exists():\n",
    "    print(\"üìÑ Found existing kaggle.json, loading credentials...\")\n",
    "    with open(kaggle_json_path, 'r') as f:\n",
    "        existing_creds = json.load(f)\n",
    "        KAGGLE_USERNAME = existing_creds.get(\"username\", KAGGLE_USERNAME)\n",
    "        KAGGLE_KEY = existing_creds.get(\"key\", KAGGLE_KEY)\n",
    "    print(f\"   ‚úÖ Loaded username: {KAGGLE_USERNAME}\")\n",
    "else:\n",
    "    print(\"üìù No existing kaggle.json found, using credentials from above...\")\n",
    "\n",
    "# Validate credentials\n",
    "if KAGGLE_USERNAME == \"YOUR_KAGGLE_USERNAME\" or KAGGLE_KEY == \"YOUR_KAGGLE_API_KEY\":\n",
    "    print()\n",
    "    print(\"=\"*80)\n",
    "    print(\"‚ö†Ô∏è  ERROR: KAGGLE CREDENTIALS NOT SET!\")\n",
    "    print(\"=\"*80)\n",
    "    print()\n",
    "    print(\"Please follow these steps:\")\n",
    "    print()\n",
    "    print(\"1. Go to: https://www.kaggle.com/settings\")\n",
    "    print(\"2. Scroll to 'API' section\")\n",
    "    print(\"3. Click 'Create New Token'\")\n",
    "    print(\"4. This downloads 'kaggle.json' to your Downloads folder\")\n",
    "    print(\"5. Open kaggle.json and you'll see:\")\n",
    "    print('   {\"username\":\"your_username\",\"key\":\"your_api_key\"}')\n",
    "    print()\n",
    "    print(\"6. Copy those values and paste them in the cell above:\")\n",
    "    print('   KAGGLE_USERNAME = \"your_username\"')\n",
    "    print('   KAGGLE_KEY = \"your_api_key\"')\n",
    "    print()\n",
    "    print(\"7. Re-run this cell\")\n",
    "    print()\n",
    "    print(\"=\"*80)\n",
    "    raise ValueError(\"Kaggle credentials not configured. Please set KAGGLE_USERNAME and KAGGLE_KEY above.\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Create ~/.kaggle directory if it doesn't exist\n",
    "kaggle_dir = Path.home() / \".kaggle\"\n",
    "kaggle_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Create kaggle.json with credentials\n",
    "kaggle_json_path = kaggle_dir / \"kaggle.json\"\n",
    "kaggle_credentials = {\n",
    "    \"username\": KAGGLE_USERNAME,\n",
    "    \"key\": KAGGLE_KEY\n",
    "}\n",
    "\n",
    "# Write credentials to file\n",
    "with open(kaggle_json_path, 'w') as f:\n",
    "    json.dump(kaggle_credentials, f, indent=2)\n",
    "\n",
    "# Set proper permissions (required by Kaggle API on Unix systems)\n",
    "try:\n",
    "    os.chmod(kaggle_json_path, 0o600)\n",
    "except:\n",
    "    pass  # Windows doesn't support chmod\n",
    "\n",
    "print(f\"‚úÖ Kaggle credentials saved to: {kaggle_json_path}\")\n",
    "print(f\"   Username: {KAGGLE_USERNAME}\")\n",
    "print(f\"   Key: {KAGGLE_KEY[:10]}...{KAGGLE_KEY[-4:]}\")\n",
    "print()\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# Kaggle datasets to download\n",
    "KAGGLE_DATASETS = [\n",
    "    {\"slug\": \"sumn2u/garbage-classification-v2\", \"name\": \"garbage-classification-v2\"},\n",
    "    {\"slug\": \"zlatan599/garbage-dataset-classification\", \"name\": \"garbage-dataset-classification\"},\n",
    "    {\"slug\": \"parohod/warp-waste-recycling-plant-dataset\", \"name\": \"warp-waste-recycling-plant-dataset\"},\n",
    "    {\"slug\": \"asdasdasasdas/garbage-classification\", \"name\": \"garbage-classification\"},\n",
    "    {\"slug\": \"techsash/waste-classification-data\", \"name\": \"waste-classification-data\"},\n",
    "    {\"slug\": \"alistairking/recyclable-and-household-waste-classification\", \"name\": \"recyclable-and-household-waste-classification\"},\n",
    "    {\"slug\": \"vishallazrus/multi-class-garbage-classification-dataset\", \"name\": \"multi-class-garbage-classification-dataset\"},\n",
    "    {\"slug\": \"mostafaabla/garbage-classification\", \"name\": \"garbage-classification-mostafa\"}\n",
    "]\n",
    "\n",
    "def download_kaggle_datasets(datasets, base_dir=\"./data/kaggle\"):\n",
    "    \"\"\"\n",
    "    Download Kaggle datasets using the Kaggle Python API (not CLI).\n",
    "\n",
    "    Args:\n",
    "        datasets: List of dataset dictionaries with 'slug' and 'name'\n",
    "        base_dir: Base directory to store downloaded datasets\n",
    "    \"\"\"\n",
    "    import sys\n",
    "    import time\n",
    "\n",
    "    base_path = Path(base_dir)\n",
    "    base_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(\"üì¶ KAGGLE DATASET DOWNLOAD\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Import Kaggle API\n",
    "    try:\n",
    "        from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "        api = KaggleApi()\n",
    "        api.authenticate()\n",
    "        print(\"‚úÖ Kaggle API authenticated successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to authenticate Kaggle API: {e}\")\n",
    "        print(\"\\nPlease ensure:\")\n",
    "        print(\"1. You have a Kaggle account\")\n",
    "        print(\"2. Your username is correct in the cell above\")\n",
    "        print(\"3. Your API key is correct\")\n",
    "        return [], datasets\n",
    "\n",
    "    print()\n",
    "\n",
    "    downloaded = []\n",
    "    failed = []\n",
    "\n",
    "    for idx, dataset in enumerate(datasets, 1):\n",
    "        dataset_slug = dataset[\"slug\"]\n",
    "        dataset_name = dataset[\"name\"]\n",
    "        dataset_path = base_path / dataset_name\n",
    "\n",
    "        print(f\"\\n[{idx}/{len(datasets)}] {dataset_name}\")\n",
    "        print(f\"      Source: {dataset_slug}\")\n",
    "\n",
    "        # Check if already downloaded\n",
    "        if dataset_path.exists() and any(dataset_path.iterdir()):\n",
    "            print(f\"      ‚úÖ Already downloaded, skipping...\")\n",
    "            downloaded.append(dataset_name)\n",
    "            continue\n",
    "\n",
    "        print(f\"      üì• Downloading...\", end=\"\", flush=True)\n",
    "\n",
    "        try:\n",
    "            # Create dataset directory\n",
    "            dataset_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Download using Kaggle Python API with quiet mode\n",
    "            # This prevents blocking output\n",
    "            api.dataset_download_files(\n",
    "                dataset_slug,\n",
    "                path=str(dataset_path),\n",
    "                unzip=True,\n",
    "                quiet=True  # Changed to True to prevent blocking\n",
    "            )\n",
    "\n",
    "            elapsed = time.time() - start_time\n",
    "\n",
    "            # Verify download\n",
    "            if dataset_path.exists() and any(dataset_path.iterdir()):\n",
    "                print(f\" ‚úÖ Done! ({elapsed:.1f}s)\")\n",
    "                downloaded.append(dataset_name)\n",
    "            else:\n",
    "                print(f\" ‚ùå Failed (no files found)\")\n",
    "                failed.append(dataset_name)\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(f\"\\n\\n‚ö†Ô∏è  Download interrupted by user!\")\n",
    "            print(f\"   Downloaded so far: {len(downloaded)}/{len(datasets)}\")\n",
    "            return downloaded, failed + [d[\"name\"] for d in datasets[idx-1:]]\n",
    "\n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            # Shorten long error messages\n",
    "            if len(error_msg) > 100:\n",
    "                error_msg = error_msg[:100] + \"...\"\n",
    "            print(f\" ‚ùå Error: {error_msg}\")\n",
    "            failed.append(dataset_name)\n",
    "\n",
    "            # Clean up partial download\n",
    "            if dataset_path.exists():\n",
    "                try:\n",
    "                    shutil.rmtree(dataset_path)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "        # Flush output to ensure it's displayed\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä DOWNLOAD SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"‚úÖ Successfully downloaded: {len(downloaded)}/{len(datasets)}\")\n",
    "    print(f\"‚ùå Failed: {len(failed)}/{len(datasets)}\")\n",
    "\n",
    "    if downloaded:\n",
    "        print(f\"\\n‚úÖ Downloaded datasets:\")\n",
    "        for name in downloaded:\n",
    "            print(f\"   ‚úì {name}\")\n",
    "\n",
    "    if failed:\n",
    "        print(f\"\\n‚ö†Ô∏è  Failed datasets:\")\n",
    "        for name in failed:\n",
    "            print(f\"   ‚úó {name}\")\n",
    "\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    return downloaded, failed\n",
    "\n",
    "# Download all datasets\n",
    "print(\"Starting Kaggle dataset downloads...\")\n",
    "print(\"This may take 10-30 minutes depending on your internet connection.\")\n",
    "print(\"üí° TIP: If a download seems stuck, press Ctrl+C to skip and continue with next dataset\")\n",
    "print()\n",
    "\n",
    "try:\n",
    "    downloaded, failed = download_kaggle_datasets(KAGGLE_DATASETS)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n\\n‚ö†Ô∏è  Download process interrupted!\")\n",
    "    print(\"You can continue with the datasets that were successfully downloaded.\")\n",
    "    downloaded, failed = [], []\n",
    "\n",
    "if len(downloaded) == 0:\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING: No datasets were downloaded!\")\n",
    "    print(\"Please check your Kaggle API token and internet connection.\")\n",
    "    print(\"\\nüí° You can still continue with the notebook - we'll use sample data for testing.\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Ready to proceed with {len(downloaded)} datasets!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìù NOTE: Cell execution complete! You can now proceed to the next cell.\")\n",
    "print(\"=\"*80)\n",
    "\n"
   ],
   "id": "e6605fb339e5e0f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "import timm\n",
    "from timm.data import create_transform, resolve_data_config\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "from tqdm.notebook import tqdm\n",
    "import wandb\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ],
   "id": "bd161b494b55e777",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    \"\"\"Set random seed for reproducibility across all frameworks.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "        # MPS doesn't need special seeding\n",
    "        pass\n",
    "    logger.info(f\"‚úì Random seed set to {seed}\")\n",
    "\n",
    "def get_device():\n",
    "    \"\"\"\n",
    "    Detect and return the best available device for training.\n",
    "    Priority: CUDA > MPS (Apple Silicon) > CPU\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        logger.info(f\"üöÄ Using CUDA GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        logger.info(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "        return device\n",
    "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        logger.info(f\"üçé Using Apple Silicon MPS (Metal Performance Shaders)\")\n",
    "        logger.info(f\"   Optimized for M1/M2/M3 chips\")\n",
    "        return device\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        logger.info(f\"üíª Using CPU (no GPU acceleration available)\")\n",
    "        logger.info(f\"   ‚ö†Ô∏è  Training will be slower on CPU\")\n",
    "        return device\n",
    "\n",
    "def optimize_memory(device):\n",
    "    \"\"\"\n",
    "    Memory optimization for different hardware backends.\n",
    "    Supports CUDA, MPS (Apple Silicon), and CPU.\n",
    "    \"\"\"\n",
    "    if device.type == \"cuda\":\n",
    "        # CUDA GPU optimization\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "        # TF32 support (only in PyTorch 1.7+)\n",
    "        try:\n",
    "            torch.backends.cuda.matmul.allow_tf32 = True\n",
    "            torch.backends.cudnn.allow_tf32 = True\n",
    "        except AttributeError:\n",
    "            pass  # Older PyTorch versions don't have TF32 support\n",
    "\n",
    "        import os\n",
    "        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True,max_split_size_mb:512'\n",
    "\n",
    "        try:\n",
    "            torch.cuda.set_per_process_memory_fraction(0.95)\n",
    "        except AttributeError:\n",
    "            pass  # Older PyTorch versions don't have this method\n",
    "\n",
    "        logger.info(\"‚úì CUDA memory optimization enabled\")\n",
    "\n",
    "    elif device.type == \"mps\":\n",
    "        # MPS (Apple Silicon) optimization\n",
    "        # MPS doesn't have explicit memory management like CUDA\n",
    "        # But we can set environment variables for better performance\n",
    "        import os\n",
    "        os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'  # Disable memory caching\n",
    "\n",
    "        logger.info(\"‚úì MPS optimization enabled\")\n",
    "        logger.info(\"  - High watermark ratio: 0.0 (aggressive memory release)\")\n",
    "\n",
    "    else:\n",
    "        # CPU optimization\n",
    "        # Set number of threads for CPU training\n",
    "        import os\n",
    "        num_threads = os.cpu_count() or 4\n",
    "        torch.set_num_threads(num_threads)\n",
    "\n",
    "        logger.info(f\"‚úì CPU optimization enabled\")\n",
    "        logger.info(f\"  - Using {num_threads} threads\")\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=15, mode=\"max\", delta=0):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.mode = mode\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, current_score):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = current_score\n",
    "        elif self.mode == \"max\":\n",
    "            if current_score <= self.best_score + self.delta:\n",
    "                self.counter += 1\n",
    "                if self.counter >= self.patience:\n",
    "                    self.early_stop = True\n",
    "            else:\n",
    "                self.best_score = current_score\n",
    "                self.counter = 0\n",
    "        return self.early_stop"
   ],
   "id": "e37d70388c34be67",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "TARGET_CLASSES = [\n",
    "    'aerosol_cans', 'aluminum_food_cans', 'aluminum_soda_cans', 'cardboard_boxes', 'cardboard_packaging',\n",
    "    'clothing', 'coffee_grounds', 'disposable_plastic_cutlery', 'egg_shells', 'food_waste',\n",
    "    'glass_beverage_bottles', 'glass_cosmetic_containers', 'glass_food_jars', 'magazines',\n",
    "    'newspaper', 'office_paper', 'paper_cups', 'plastic_cup_lids', 'plastic_detergent_bottles',\n",
    "    'plastic_food_containers', 'plastic_shopping_bags', 'plastic_soda_bottles', 'plastic_straws',\n",
    "    'plastic_trash_bags', 'plastic_water_bottles', 'shoes', 'steel_food_cans', 'styrofoam_cups',\n",
    "    'styrofoam_food_containers', 'tea_bags'\n",
    "]\n",
    "\n",
    "VISION_CONFIG = {\n",
    "    \"model\": {\n",
    "        \"backbone\": \"eva02_base_patch14_224\",  # EVA02 Base (86M params) - MPS compatible, pretrained on ImageNet\n",
    "        \"pretrained\": True,\n",
    "        \"num_classes\": 30,\n",
    "        \"drop_rate\": 0.3,\n",
    "        \"drop_path_rate\": 0.2\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"input_size\": 224,  # Reduced from 448 for Mac CPU/MPS training\n",
    "        \"num_workers\": 2,  # Mac can handle 2 workers\n",
    "        \"pin_memory\": False,  # Not needed for CPU/MPS\n",
    "        \"sources\": [\n",
    "            {\n",
    "                \"name\": \"master_30\",\n",
    "                \"path\": \"./data/kaggle/recyclable-and-household-waste-classification/images\",\n",
    "                \"type\": \"master\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"garbage_12\",\n",
    "                \"path\": \"./data/kaggle/garbage-classification-mostafa/garbage_classification\",\n",
    "                \"type\": \"mapped_12\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"waste_22k\",\n",
    "                \"path\": \"./data/kaggle/waste-classification-data/DATASET\",\n",
    "                \"type\": \"mapped_2\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"garbage_v2_10\",\n",
    "                \"path\": \"./data/kaggle/garbage-classification-v2\",\n",
    "                \"type\": \"mapped_10\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"garbage_6\",\n",
    "                \"path\": \"./data/kaggle/garbage-classification\",\n",
    "                \"type\": \"mapped_6\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"garbage_balanced\",\n",
    "                \"path\": \"./data/kaggle/garbage-dataset-classification\",\n",
    "                \"type\": \"mapped_6\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"warp_industrial\",\n",
    "                \"path\": \"./data/kaggle/warp-waste-recycling-plant-dataset\",\n",
    "                \"type\": \"industrial\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"multiclass_garbage\",\n",
    "                \"path\": \"./data/kaggle/multi-class-garbage-classification-dataset\",\n",
    "                \"type\": \"multiclass\"\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"batch_size\": 2,  # Reduced from 4 for MPS stability (no gradient checkpointing)\n",
    "        \"grad_accum_steps\": 32,  # Increased to maintain effective batch size of 64 (2 √ó 32)\n",
    "        \"learning_rate\": 5e-5,\n",
    "        \"weight_decay\": 0.05,\n",
    "        \"num_epochs\": 20,\n",
    "        \"patience\": 5,\n",
    "        \"use_amp\": False,  # AMP not supported on MPS/CPU\n",
    "        \"max_grad_norm\": 1.0  # Gradient clipping for stability\n",
    "    }\n",
    "}"
   ],
   "id": "e85800f6504ff2e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class UnifiedWasteDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A unified dataset that ingests data from multiple sources and maps them\n",
    "    to a single 30-class target schema.\n",
    "    \"\"\"\n",
    "    def __init__(self, sources_config, target_classes, transform=None):\n",
    "        self.transform = transform\n",
    "        self.target_classes = sorted(target_classes)\n",
    "        self.class_to_idx = {c: i for i, c in enumerate(self.target_classes)}\n",
    "        self.samples = []\n",
    "\n",
    "        self.skipped_count = 0\n",
    "        self.skipped_labels = {}  # Track what labels are being skipped\n",
    "\n",
    "        total_added = 0\n",
    "        total_skipped = 0\n",
    "\n",
    "        for source in sources_config:\n",
    "            added, skipped = self._ingest_source(source)\n",
    "            total_added += added\n",
    "            total_skipped += skipped\n",
    "\n",
    "        logger.info(f\"=\"*60)\n",
    "        logger.info(f\"üìä Dataset Summary:\")\n",
    "        logger.info(f\"  ‚úì Total images loaded: {len(self.samples)}\")\n",
    "        logger.info(f\"  ‚úì Images added: {total_added}\")\n",
    "        logger.info(f\"  ‚ö† Images skipped: {total_skipped}\")\n",
    "        logger.info(f\"  üìà Utilization: {100*total_added/(total_added+total_skipped) if (total_added+total_skipped) > 0 else 0:.1f}%\")\n",
    "        logger.info(f\"=\"*60)\n",
    "\n",
    "        # Log skipped labels for debugging (top 10 only)\n",
    "        if self.skipped_labels:\n",
    "            logger.warning(f\"‚ö† Top 10 skipped labels:\")\n",
    "            for label, count in sorted(self.skipped_labels.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "                logger.warning(f\"  '{label}': {count} images\")\n",
    "\n",
    "        # Validate we have enough data\n",
    "        if len(self.samples) == 0:\n",
    "            raise ValueError(\n",
    "                \"‚ùå No images loaded! Please check:\\n\"\n",
    "                \"  1. Dataset paths are correct\\n\"\n",
    "                \"  2. Datasets are attached in Kaggle\\n\"\n",
    "                \"  3. Label mappings are configured correctly\"\n",
    "            )\n",
    "\n",
    "        if len(self.samples) < 100:\n",
    "            logger.warning(f\"‚ö† Very few images loaded ({len(self.samples)}). Training may not be effective.\")\n",
    "\n",
    "    def _ingest_source(self, source):\n",
    "        \"\"\"\n",
    "        Ingest images from a data source with robust error handling.\n",
    "        Returns: (images_added, images_skipped) tuple\n",
    "        \"\"\"\n",
    "        path = Path(source[\"path\"])\n",
    "        images_added = 0\n",
    "        images_skipped = 0\n",
    "\n",
    "        if not path.exists():\n",
    "            parent = path.parent\n",
    "            found = False\n",
    "            if parent.exists():\n",
    "                for child in parent.iterdir():\n",
    "                    if child.is_dir():\n",
    "                        try:\n",
    "                            if any(child.iterdir()):\n",
    "                                path = child\n",
    "                                found = True\n",
    "                                break\n",
    "                        except PermissionError:\n",
    "                            continue\n",
    "\n",
    "            if not found or not path.exists():\n",
    "                logger.warning(f\"‚ö† Source {source['name']} not found at {source['path']}. Skipping.\")\n",
    "                return images_added, images_skipped\n",
    "\n",
    "        logger.info(f\"üìÇ Ingesting {source['name']} from {path}...\")\n",
    "\n",
    "        for root, _, files in os.walk(path):\n",
    "            folder_name = Path(root).name.lower()\n",
    "\n",
    "            target_label = self._map_label(folder_name, source['type'])\n",
    "\n",
    "            if target_label:\n",
    "                target_idx = self.class_to_idx[target_label]\n",
    "                for file in files:\n",
    "                    if file.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp')):\n",
    "                        self.samples.append((Path(root) / file, target_idx))\n",
    "                        images_added += 1\n",
    "            else:\n",
    "                img_count = sum(1 for f in files if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp')))\n",
    "                if img_count > 0:\n",
    "                    self.skipped_count += img_count\n",
    "                    images_skipped += img_count\n",
    "                    # Track which labels are being skipped\n",
    "                    if folder_name not in self.skipped_labels:\n",
    "                        self.skipped_labels[folder_name] = 0\n",
    "                    self.skipped_labels[folder_name] += img_count\n",
    "\n",
    "        logger.info(f\"‚úì {source['name']}: Added {images_added} images, skipped {images_skipped}\")\n",
    "        return images_added, images_skipped\n",
    "\n",
    "    def _map_label(self, raw_label, source_type):\n",
    "        \"\"\"\n",
    "        Professional label mapping with comprehensive coverage.\n",
    "        Maps diverse dataset labels to unified 30-class taxonomy.\n",
    "        \"\"\"\n",
    "        raw = raw_label.lower().strip()\n",
    "\n",
    "        # Skip metadata/structure folders that are not actual labels\n",
    "        metadata_folders = {\n",
    "            'default', 'real_world', 'images', 'train', 'test', 'val',\n",
    "            'segmentationobject', 'segmentationclass', 'jpegimages',\n",
    "            'annotations', 'assets', 'data', 'dataset', 'samples'\n",
    "        }\n",
    "        if raw in metadata_folders:\n",
    "            return None\n",
    "\n",
    "        if source_type == 'master':\n",
    "            if raw in self.target_classes:\n",
    "                return raw\n",
    "            # Fallback: try to find closest match\n",
    "            for target in self.target_classes:\n",
    "                if raw in target or target in raw:\n",
    "                    return target\n",
    "            return None\n",
    "\n",
    "        if source_type == 'mapped_12':\n",
    "            mapping = {\n",
    "                'paper': 'office_paper',\n",
    "                'cardboard': 'cardboard_boxes',\n",
    "                'plastic': 'plastic_food_containers',\n",
    "                'metal': 'aluminum_food_cans',\n",
    "                'glass': 'glass_food_jars',\n",
    "                'brown-glass': 'glass_beverage_bottles',\n",
    "                'green-glass': 'glass_beverage_bottles',\n",
    "                'white-glass': 'glass_food_jars',\n",
    "                'clothes': 'clothing',\n",
    "                'shoes': 'shoes',\n",
    "                'biological': 'food_waste',\n",
    "                'trash': 'food_waste'\n",
    "            }\n",
    "            return mapping.get(raw)\n",
    "\n",
    "        if source_type == 'mapped_2':\n",
    "            # Organic waste\n",
    "            if raw in ['organic', 'o']:\n",
    "                return 'food_waste'\n",
    "            # Recyclable waste (paper, plastic, metal, glass mix)\n",
    "            if raw in ['recyclable', 'r']:\n",
    "                return 'plastic_food_containers'  # Generic recyclable\n",
    "            return None\n",
    "\n",
    "        if source_type == 'mapped_10':\n",
    "            mapping = {\n",
    "                'metal': 'aluminum_food_cans',\n",
    "                'glass': 'glass_food_jars',\n",
    "                'biological': 'food_waste',\n",
    "                'paper': 'office_paper',\n",
    "                'battery': 'aerosol_cans',\n",
    "                'trash': 'food_waste',\n",
    "                'cardboard': 'cardboard_boxes',\n",
    "                'shoes': 'shoes',\n",
    "                'clothes': 'clothing',\n",
    "                'plastic': 'plastic_food_containers'\n",
    "            }\n",
    "            return mapping.get(raw)\n",
    "\n",
    "        if source_type == 'mapped_6':\n",
    "            mapping = {\n",
    "                'cardboard': 'cardboard_boxes',\n",
    "                'glass': 'glass_food_jars',\n",
    "                'metal': 'aluminum_food_cans',\n",
    "                'paper': 'office_paper',\n",
    "                'plastic': 'plastic_food_containers',\n",
    "                'trash': 'food_waste'\n",
    "            }\n",
    "            return mapping.get(raw)\n",
    "\n",
    "        if source_type == 'industrial':\n",
    "            mapping = {\n",
    "                'pet': 'plastic_food_containers',\n",
    "                'hdpe': 'plastic_food_containers',\n",
    "                'pvc': 'plastic_food_containers',\n",
    "                'ldpe': 'plastic_food_containers',\n",
    "                'pp': 'plastic_food_containers',\n",
    "                'ps': 'plastic_food_containers',\n",
    "                'metal': 'aluminum_food_cans',\n",
    "                'glass': 'glass_food_jars',\n",
    "                'paper': 'office_paper',\n",
    "                'cardboard': 'cardboard_boxes',\n",
    "                'trash': 'food_waste'\n",
    "            }\n",
    "            return mapping.get(raw)\n",
    "\n",
    "        if source_type == 'multiclass':\n",
    "            mapping = {\n",
    "                'plastic': 'plastic_food_containers',\n",
    "                'metal': 'aluminum_food_cans',\n",
    "                'glass': 'glass_food_jars',\n",
    "                'paper': 'office_paper',\n",
    "                'cardboard': 'cardboard_boxes',\n",
    "                'trash': 'food_waste',\n",
    "                'organic': 'food_waste',\n",
    "                'battery': 'aerosol_cans',\n",
    "                'clothes': 'clothing',\n",
    "                'shoes': 'shoes'\n",
    "            }\n",
    "            return mapping.get(raw)\n",
    "\n",
    "        # Universal fallback mappings for common waste categories\n",
    "        # This ensures NO images are skipped\n",
    "        fallback_mapping = {\n",
    "            # Recyclables\n",
    "            'recyclable': 'plastic_food_containers',\n",
    "            'recycle': 'plastic_food_containers',\n",
    "            'recycling': 'plastic_food_containers',\n",
    "            # Waste types\n",
    "            'waste': 'food_waste',\n",
    "            'garbage': 'food_waste',\n",
    "            'rubbish': 'food_waste',\n",
    "            'refuse': 'food_waste',\n",
    "            # Organic\n",
    "            'compost': 'food_waste',\n",
    "            'food': 'food_waste',\n",
    "            'kitchen': 'food_waste',\n",
    "            'biological': 'food_waste',\n",
    "            # Paper products\n",
    "            'newspaper': 'newspaper',\n",
    "            'magazine': 'magazines',\n",
    "            'book': 'office_paper',\n",
    "            'document': 'office_paper',\n",
    "            # Plastic types\n",
    "            'bottle': 'plastic_water_bottles',\n",
    "            'bottle-transp': 'plastic_water_bottles',\n",
    "            'bottle-blue': 'plastic_water_bottles',\n",
    "            'bottle-dark': 'plastic_water_bottles',\n",
    "            'bottle-green': 'plastic_water_bottles',\n",
    "            'bottle-blue5l': 'plastic_water_bottles',\n",
    "            'bottle-milk': 'plastic_water_bottles',\n",
    "            'bottle-oil': 'plastic_water_bottles',\n",
    "            'bottle-yogurt': 'plastic_food_containers',\n",
    "            'bottle-multicolor': 'plastic_water_bottles',\n",
    "            'bottle-transp-full': 'plastic_water_bottles',\n",
    "            'bottle-blue-full': 'plastic_water_bottles',\n",
    "            'bottle-green-full': 'plastic_water_bottles',\n",
    "            'bottle-dark-full': 'plastic_water_bottles',\n",
    "            'bottle-milk-full': 'plastic_water_bottles',\n",
    "            'bottle-multicolorv-full': 'plastic_water_bottles',\n",
    "            'bottle-blue5l-full': 'plastic_water_bottles',\n",
    "            'bottle-oil-full': 'plastic_water_bottles',\n",
    "            'bag': 'plastic_shopping_bags',\n",
    "            'container': 'plastic_food_containers',\n",
    "            'cup': 'paper_cups',\n",
    "            'straw': 'plastic_straws',\n",
    "            # Detergents (plastic containers)\n",
    "            'detergent-white': 'plastic_food_containers',\n",
    "            'detergent-color': 'plastic_food_containers',\n",
    "            'detergent-transparent': 'plastic_food_containers',\n",
    "            'detergent-box': 'cardboard_boxes',\n",
    "            # Metal\n",
    "            'can': 'aluminum_soda_cans',\n",
    "            'cans': 'aluminum_soda_cans',\n",
    "            'tin': 'steel_food_cans',\n",
    "            'aluminum': 'aluminum_food_cans',\n",
    "            'steel': 'steel_food_cans',\n",
    "            'canister': 'aluminum_food_cans',\n",
    "            'battery': 'aerosol_cans',  # Hazardous, map to aerosol as closest\n",
    "            # Glass\n",
    "            'jar': 'glass_food_jars',\n",
    "            'glass-transp': 'glass_food_jars',\n",
    "            'glass-dark': 'glass_beverage_bottles',\n",
    "            'glass-green': 'glass_beverage_bottles',\n",
    "            'white-glass': 'glass_food_jars',\n",
    "            'brown-glass': 'glass_beverage_bottles',\n",
    "            'green-glass': 'glass_beverage_bottles',\n",
    "            # Cardboard\n",
    "            'milk-cardboard': 'cardboard_boxes',\n",
    "            'juice-cardboard': 'cardboard_boxes',\n",
    "            # Textiles\n",
    "            'fabric': 'clothing',\n",
    "            'textile': 'clothing',\n",
    "            # Foam\n",
    "            'foam': 'styrofoam_cups',\n",
    "            'styrofoam': 'styrofoam_cups',\n",
    "            'polystyrene': 'styrofoam_cups',\n",
    "        }\n",
    "\n",
    "        # Try fallback mapping\n",
    "        for key, value in fallback_mapping.items():\n",
    "            if key in raw:\n",
    "                return value\n",
    "\n",
    "        return None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label_idx = self.samples[idx]\n",
    "        try:\n",
    "            # Use PIL.Image.open with verification disabled to avoid hangs\n",
    "            img = Image.open(path)\n",
    "            img.load()  # Force load to catch corruption early\n",
    "            img = img.convert('RGB')\n",
    "\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            return img, label_idx\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Skipping corrupt image {path}: {e}\")\n",
    "            # CRITICAL: Return a normalized tensor directly to avoid NumPy compatibility issues\n",
    "            # This bypasses transforms entirely for corrupt images\n",
    "            # Create a black 224x224 image tensor with ImageNet normalization\n",
    "            # Shape: (3, 224, 224), normalized with mean and std\n",
    "            mean = torch.tensor([0.48145466, 0.4578275, 0.40821073]).view(3, 1, 1)\n",
    "            std = torch.tensor([0.26862954, 0.26130258, 0.27577711]).view(3, 1, 1)\n",
    "            # Create black 224x224 image (all zeros) and normalize\n",
    "            dummy_tensor = torch.zeros(3, 224, 224)\n",
    "            dummy_tensor = (dummy_tensor - mean) / std\n",
    "            return dummy_tensor, label_idx\n",
    "\n",
    "    def get_labels(self):\n",
    "        return [s[1] for s in self.samples]"
   ],
   "id": "4a83856bf02ace6d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def get_vision_transforms(config, model, is_train=True):\n",
    "    \"\"\"\n",
    "    Get vision transforms using standard PyTorch transforms.\n",
    "    Compatible with PIL Images from UnifiedWasteDataset.\n",
    "    \"\"\"\n",
    "    # Get config's input_size (prioritize config over model defaults)\n",
    "    config_input_size = config.get('data', {}).get('input_size', 224)\n",
    "\n",
    "    # Get model config for input size\n",
    "    try:\n",
    "        if hasattr(model, 'default_cfg'):\n",
    "            model_cfg = model.default_cfg\n",
    "        elif hasattr(model, 'pretrained_cfg'):\n",
    "            model_cfg = model.pretrained_cfg\n",
    "        else:\n",
    "            model_cfg = {\n",
    "                'input_size': (3, config_input_size, config_input_size),\n",
    "                'mean': (0.485, 0.456, 0.406),\n",
    "                'std': (0.229, 0.224, 0.225)\n",
    "            }\n",
    "\n",
    "        # Extract input size (handle tuple format)\n",
    "        input_size_tuple = model_cfg.get('input_size', (3, config_input_size, config_input_size))\n",
    "        if isinstance(input_size_tuple, tuple) and len(input_size_tuple) == 3:\n",
    "            img_size = input_size_tuple[1]  # Get height (assuming square images)\n",
    "        else:\n",
    "            img_size = config_input_size  # Use config's input_size\n",
    "\n",
    "        mean = model_cfg.get('mean', (0.485, 0.456, 0.406))\n",
    "        std = model_cfg.get('std', (0.229, 0.224, 0.225))\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Failed to get model config: {e}, using defaults\")\n",
    "        # Force config's input_size for Mac compatibility (override model default)\n",
    "        img_size = config_input_size\n",
    "        mean = (0.485, 0.456, 0.406)\n",
    "        std = (0.229, 0.224, 0.225)\n",
    "\n",
    "    logger.info(f\"Using transforms with input_size={img_size}, mean={mean}, std={std}\")\n",
    "\n",
    "    # Use standard PyTorch transforms (compatible with PIL Images)\n",
    "    if is_train:\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((img_size, img_size), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomRotation(degrees=15),\n",
    "            transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
    "            transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=mean, std=std)\n",
    "        ])\n",
    "    else:\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((img_size, img_size), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=mean, std=std)\n",
    "        ])"
   ],
   "id": "8cac2161a057a33e",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def create_vision_model(config):\n",
    "    logger.info(f\"Creating model: {config['model']['backbone']}\")\n",
    "    model = timm.create_model(\n",
    "        config[\"model\"][\"backbone\"],\n",
    "        pretrained=config[\"model\"][\"pretrained\"],\n",
    "        num_classes=config[\"model\"][\"num_classes\"],\n",
    "        drop_rate=config[\"model\"][\"drop_rate\"],\n",
    "        drop_path_rate=config[\"model\"][\"drop_path_rate\"]\n",
    "    )\n",
    "    return model"
   ],
   "id": "20be2291a212f609",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train_vision_model(config):\n",
    "    \"\"\"\n",
    "    Professional-grade vision model training with comprehensive error handling.\n",
    "    Optimized for Tesla T4 GPU (14.74 GB) with production-ready memory management.\n",
    "\n",
    "    Args:\n",
    "        config: Training configuration dictionary\n",
    "\n",
    "    Returns:\n",
    "        Trained model or None if training fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        set_seed()\n",
    "        device = get_device()\n",
    "        optimize_memory(device)\n",
    "        logger.info(f\"Using device: {device}\")\n",
    "\n",
    "        # Create and configure model\n",
    "        model = create_vision_model(config).to(device)\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        logger.info(f\"Model parameters: {total_params / 1e6:.2f}M total, {trainable_params / 1e6:.2f}M trainable\")\n",
    "\n",
    "        # Enable gradient checkpointing for memory efficiency (NOT compatible with MPS)\n",
    "        if device.type != \"mps\" and hasattr(model, 'set_grad_checkpointing'):\n",
    "            model.set_grad_checkpointing(enable=True)\n",
    "            logger.info(\"‚úì Gradient checkpointing enabled (saves ~40% memory)\")\n",
    "        elif device.type == \"mps\":\n",
    "            logger.warning(\"‚ö†Ô∏è  Gradient checkpointing disabled (incompatible with MPS - causes crashes)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Model initialization failed: {e}\")\n",
    "        raise\n",
    "\n",
    "    train_transform = get_vision_transforms(config, model, is_train=True)\n",
    "    val_transform = get_vision_transforms(config, model, is_train=False)\n",
    "\n",
    "    # CRITICAL: Validate transform output size matches model expectations\n",
    "    logger.info(\"üîç Validating transform pipeline...\")\n",
    "    try:\n",
    "        # Create dummy image for validation\n",
    "        import numpy as np\n",
    "        dummy_img = Image.new('RGB', (640, 480), color=(128, 128, 128))\n",
    "\n",
    "        # Convert to numpy array first to avoid NumPy version compatibility issues\n",
    "        # This works around the \"expected np.ndarray (got numpy.ndarray)\" error\n",
    "        dummy_array = np.array(dummy_img)\n",
    "        dummy_img_fixed = Image.fromarray(dummy_array)\n",
    "\n",
    "        transformed = train_transform(dummy_img_fixed)\n",
    "        actual_size = transformed.shape\n",
    "        expected_channels = 3\n",
    "        expected_size = config.get('data', {}).get('input_size', 224)\n",
    "\n",
    "        logger.info(f\"  Transform output shape: {actual_size}\")\n",
    "        logger.info(f\"  Expected: ({expected_channels}, {expected_size}, {expected_size})\")\n",
    "\n",
    "        if actual_size[0] != expected_channels or actual_size[1] != expected_size or actual_size[2] != expected_size:\n",
    "            raise ValueError(\n",
    "                f\"Transform size mismatch! Output {actual_size} != Expected ({expected_channels}, {expected_size}, {expected_size})\"\n",
    "            )\n",
    "        logger.info(\"  ‚úÖ Transform validation passed\")\n",
    "    except TypeError as e:\n",
    "        if \"expected np.ndarray\" in str(e):\n",
    "            logger.warning(f\"‚ö†Ô∏è  NumPy compatibility issue detected: {e}\")\n",
    "            logger.warning(\"  Skipping transform validation (will validate with real data)\")\n",
    "            logger.warning(\"  Consider upgrading: pip install 'numpy<2.0'\")\n",
    "        else:\n",
    "            logger.error(f\"‚ùå Transform validation failed: {e}\")\n",
    "            raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Transform validation failed: {e}\")\n",
    "        raise\n",
    "\n",
    "    full_dataset = UnifiedWasteDataset(\n",
    "        sources_config=config[\"data\"][\"sources\"],\n",
    "        target_classes=TARGET_CLASSES,\n",
    "        transform=None\n",
    "    )\n",
    "\n",
    "    if len(full_dataset) == 0:\n",
    "        logger.error(\"Dataset is empty. Check paths.\")\n",
    "        return None\n",
    "\n",
    "    train_size = int(0.85 * len(full_dataset))\n",
    "    val_size = len(full_dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "    train_dataset.dataset.transform = train_transform\n",
    "    val_dataset.dataset.transform = val_transform\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config[\"training\"][\"batch_size\"],\n",
    "        shuffle=True,\n",
    "        num_workers=0,  # Disabled for macOS compatibility (multiprocessing issue)\n",
    "        pin_memory=config[\"data\"][\"pin_memory\"],\n",
    "        persistent_workers=False\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config[\"training\"][\"batch_size\"] * 2,\n",
    "        shuffle=False,\n",
    "        num_workers=0,  # Disabled for macOS compatibility (multiprocessing issue)\n",
    "        persistent_workers=False\n",
    "    )\n",
    "\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config[\"training\"][\"learning_rate\"],\n",
    "        weight_decay=config[\"training\"][\"weight_decay\"]\n",
    "    )\n",
    "\n",
    "    # INDUSTRIAL-GRADE: Label smoothing for better generalization\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "    # Professional training configuration\n",
    "    # AMP only works on CUDA, not on MPS or CPU\n",
    "    use_amp = config[\"training\"].get(\"use_amp\", False) and (device.type == \"cuda\")\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=use_amp) if use_amp else None\n",
    "    accumulation_steps = config[\"training\"][\"grad_accum_steps\"]\n",
    "    max_grad_norm = config[\"training\"].get(\"max_grad_norm\", 1.0)\n",
    "\n",
    "    # INDUSTRIAL-GRADE: OneCycleLR - proven superior to cosine annealing\n",
    "    total_steps = len(train_loader) * config[\"training\"][\"num_epochs\"] // accumulation_steps\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=config[\"training\"][\"learning_rate\"] * 10,  # Peak LR\n",
    "        total_steps=total_steps,\n",
    "        pct_start=0.3,  # 30% warmup\n",
    "        anneal_strategy='cos',\n",
    "        div_factor=25.0,  # Initial LR = max_lr / 25\n",
    "        final_div_factor=1e4  # Final LR = max_lr / 10000\n",
    "    )\n",
    "\n",
    "    early_stopping = EarlyStopping(patience=config[\"training\"][\"patience\"])\n",
    "\n",
    "    if device.type == \"mps\":\n",
    "        logger.info(\"‚ÑπÔ∏è  MPS detected: AMP disabled (not supported on Apple Silicon)\")\n",
    "    elif device.type == \"cpu\":\n",
    "        logger.info(\"‚ÑπÔ∏è  CPU detected: AMP disabled (not supported on CPU)\")\n",
    "    else:\n",
    "        logger.info(f\"‚ÑπÔ∏è  AMP {'enabled' if use_amp else 'disabled'}\")\n",
    "\n",
    "    logger.info(f\"Training configuration:\")\n",
    "    logger.info(f\"  - Batch size: {config['training']['batch_size']}\")\n",
    "    logger.info(f\"  - Gradient accumulation: {accumulation_steps}\")\n",
    "    logger.info(f\"  - Effective batch size: {config['training']['batch_size'] * accumulation_steps}\")\n",
    "    logger.info(f\"  - Mixed precision (AMP): {use_amp}\")\n",
    "    logger.info(f\"  - Gradient clipping: {max_grad_norm}\")\n",
    "    logger.info(f\"  - Learning rate: {config['training']['learning_rate']}\")\n",
    "\n",
    "    # INDUSTRIAL-GRADE: Best model tracking\n",
    "    best_val_acc = 0.0\n",
    "    best_model_state = None\n",
    "    checkpoint_dir = Path(\"checkpoints\")\n",
    "    checkpoint_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    # INDUSTRIAL-GRADE: Metrics tracking\n",
    "    metrics_history = {\n",
    "        \"train_loss\": [], \"train_acc\": [],\n",
    "        \"val_loss\": [], \"val_acc\": [],\n",
    "        \"per_class_f1\": [], \"learning_rate\": []\n",
    "    }\n",
    "\n",
    "    # Initialize Weights & Biases with graceful fallback\n",
    "    try:\n",
    "        wandb.init(project=\"sustainability-vision-lake\", config=config, mode=\"online\")\n",
    "        logger.info(\"‚úì W&B logging enabled\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"W&B initialization failed: {e}. Continuing without logging.\")\n",
    "        wandb.init(mode=\"disabled\")\n",
    "\n",
    "    # CRITICAL: Pre-training sanity check - validate one batch\n",
    "    logger.info(\"üîç Running pre-training sanity check...\")\n",
    "    try:\n",
    "        model.eval()\n",
    "        test_batch = next(iter(train_loader))\n",
    "        test_images, test_labels = test_batch\n",
    "\n",
    "        logger.info(f\"  Batch shape: {test_images.shape}\")\n",
    "        logger.info(f\"  Expected: [batch_size, 3, {config.get('data', {}).get('input_size', 224)}, {config.get('data', {}).get('input_size', 224)}]\")\n",
    "\n",
    "        # Validate batch dimensions\n",
    "        expected_size = config.get('data', {}).get('input_size', 224)\n",
    "        if test_images.shape[1] != 3:\n",
    "            raise ValueError(f\"Invalid channel count: {test_images.shape[1]} (expected 3)\")\n",
    "        if test_images.shape[2] != expected_size or test_images.shape[3] != expected_size:\n",
    "            raise ValueError(\n",
    "                f\"Invalid image size: {test_images.shape[2]}x{test_images.shape[3]} \"\n",
    "                f\"(expected {expected_size}x{expected_size})\"\n",
    "            )\n",
    "\n",
    "        # Test forward pass\n",
    "        use_non_blocking = (device.type == \"cuda\")\n",
    "        test_images = test_images.to(device, non_blocking=use_non_blocking)\n",
    "        test_labels = test_labels.to(device, non_blocking=use_non_blocking)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            test_outputs = model(test_images)\n",
    "            logger.info(f\"  Model output shape: {test_outputs.shape}\")\n",
    "            logger.info(f\"  Expected: [batch_size, {config['model']['num_classes']}]\")\n",
    "\n",
    "            if test_outputs.shape[1] != config['model']['num_classes']:\n",
    "                raise ValueError(\n",
    "                    f\"Invalid output classes: {test_outputs.shape[1]} \"\n",
    "                    f\"(expected {config['model']['num_classes']})\"\n",
    "                )\n",
    "\n",
    "        logger.info(\"  ‚úÖ Pre-training sanity check passed!\")\n",
    "        logger.info(f\"  ‚úÖ All images are {expected_size}x{expected_size}\")\n",
    "        logger.info(f\"  ‚úÖ Model accepts input and produces correct output shape\")\n",
    "\n",
    "        # Clean up\n",
    "        del test_batch, test_images, test_labels, test_outputs\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "        elif device.type == \"mps\":\n",
    "            try:\n",
    "                torch.mps.empty_cache()\n",
    "            except AttributeError:\n",
    "                pass\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Pre-training sanity check failed: {e}\")\n",
    "        logger.error(\"This indicates a fundamental configuration issue. Aborting training.\")\n",
    "        raise\n",
    "\n",
    "    model.train()  # Set back to training mode\n",
    "\n",
    "    # Main training loop with error handling\n",
    "    try:\n",
    "        for epoch in range(config[\"training\"][\"num_epochs\"]):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "\n",
    "            pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['training']['num_epochs']}\")\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            for i, (images, labels) in enumerate(pbar):\n",
    "                try:\n",
    "                    # CRITICAL: Validate batch shape before processing\n",
    "                    expected_size = config.get('data', {}).get('input_size', 224)\n",
    "                    if images.shape[1] != 3 or images.shape[2] != expected_size or images.shape[3] != expected_size:\n",
    "                        logger.error(\n",
    "                            f\"‚ùå Batch {i} has invalid shape: {images.shape}. \"\n",
    "                            f\"Expected: [batch_size, 3, {expected_size}, {expected_size}]. Skipping batch.\"\n",
    "                        )\n",
    "                        continue\n",
    "\n",
    "                    # non_blocking only works with CUDA + pin_memory\n",
    "                    use_non_blocking = (device.type == \"cuda\")\n",
    "                    images, labels = images.to(device, non_blocking=use_non_blocking), labels.to(device, non_blocking=use_non_blocking)\n",
    "\n",
    "                except RuntimeError as e:\n",
    "                    if \"out of memory\" in str(e) or \"MPS\" in str(e):\n",
    "                        logger.error(f\"OOM at batch {i}. Clearing cache and skipping batch.\")\n",
    "                        if device.type == \"cuda\":\n",
    "                            torch.cuda.empty_cache()\n",
    "                        elif device.type == \"mps\":\n",
    "                            try:\n",
    "                                torch.mps.empty_cache()\n",
    "                            except AttributeError:\n",
    "                                pass  # MPS empty_cache not available in older PyTorch\n",
    "                        continue\n",
    "                    raise\n",
    "                except AssertionError as e:\n",
    "                    # Catch assertion errors from model (e.g., size mismatches)\n",
    "                    logger.error(f\"‚ùå Assertion error at batch {i}: {e}\")\n",
    "                    logger.error(f\"   Batch shape: {images.shape}\")\n",
    "                    logger.error(f\"   This indicates a size mismatch. Skipping batch.\")\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    if use_amp:\n",
    "                        with torch.cuda.amp.autocast():\n",
    "                            outputs = model(images)\n",
    "                            loss = criterion(outputs, labels) / accumulation_steps\n",
    "                        scaler.scale(loss).backward()\n",
    "\n",
    "                        if (i + 1) % accumulation_steps == 0:\n",
    "                            scaler.unscale_(optimizer)\n",
    "\n",
    "                            # CRITICAL: Check gradient health\n",
    "                            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_grad_norm)\n",
    "                            if not np.isfinite(grad_norm.item()):\n",
    "                                logger.error(f\"‚ùå Non-finite gradient norm at batch {i}: {grad_norm.item()}\")\n",
    "                                logger.error(\"   Resetting optimizer state and skipping update.\")\n",
    "                                optimizer.zero_grad()\n",
    "                                continue\n",
    "\n",
    "                            scaler.step(optimizer)\n",
    "                            scaler.update()\n",
    "                            optimizer.zero_grad()\n",
    "                            scheduler.step()  # OneCycleLR steps per batch\n",
    "                    else:\n",
    "                        outputs = model(images)\n",
    "                        loss = criterion(outputs, labels) / accumulation_steps\n",
    "                        loss.backward()\n",
    "                        if (i + 1) % accumulation_steps == 0:\n",
    "                            # CRITICAL: Check gradient health\n",
    "                            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_grad_norm)\n",
    "                            if not np.isfinite(grad_norm.item()):\n",
    "                                logger.error(f\"‚ùå Non-finite gradient norm at batch {i}: {grad_norm.item()}\")\n",
    "                                logger.error(\"   Resetting optimizer state and skipping update.\")\n",
    "                                optimizer.zero_grad()\n",
    "                                continue\n",
    "\n",
    "                            optimizer.step()\n",
    "                            optimizer.zero_grad()\n",
    "                            scheduler.step()  # OneCycleLR steps per batch\n",
    "\n",
    "                except AssertionError as e:\n",
    "                    # Catch assertion errors from model forward pass\n",
    "                    logger.error(f\"‚ùå Model forward pass failed at batch {i}: {e}\")\n",
    "                    logger.error(f\"   Input shape: {images.shape}\")\n",
    "                    logger.error(f\"   Expected size: {expected_size}x{expected_size}\")\n",
    "                    logger.error(\"   Skipping batch and continuing training.\")\n",
    "                    continue\n",
    "\n",
    "                # CRITICAL: Check for NaN/Inf in loss\n",
    "                loss_value = loss.item() * accumulation_steps\n",
    "                if not np.isfinite(loss_value):\n",
    "                    logger.error(f\"‚ùå Non-finite loss detected at batch {i}: {loss_value}\")\n",
    "                    logger.error(\"   This indicates training instability. Skipping batch.\")\n",
    "                    # Reset gradients to prevent corruption\n",
    "                    optimizer.zero_grad()\n",
    "                    continue\n",
    "\n",
    "                running_loss += loss_value\n",
    "                with torch.no_grad():\n",
    "                    _, predicted = outputs.max(1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "                current_loss = running_loss / (i + 1)\n",
    "                pbar.set_postfix({'loss': f\"{current_loss:.4f}\", 'acc': f\"{100*correct/total:.2f}%\"})\n",
    "\n",
    "            train_acc = 100 * correct / total\n",
    "\n",
    "            # CRITICAL: Epoch-level health check\n",
    "            if total == 0:\n",
    "                logger.error(f\"‚ùå Epoch {epoch+1}: No samples processed! This indicates a critical data loading issue.\")\n",
    "                break\n",
    "\n",
    "            avg_train_loss = running_loss / len(train_loader) if len(train_loader) > 0 else float('inf')\n",
    "            if not np.isfinite(avg_train_loss):\n",
    "                logger.error(f\"‚ùå Epoch {epoch+1}: Non-finite training loss: {avg_train_loss}\")\n",
    "                logger.error(\"   Training has become unstable. Stopping.\")\n",
    "                break\n",
    "\n",
    "            # INDUSTRIAL-GRADE: Comprehensive validation with per-class metrics\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            all_preds = []\n",
    "            all_labels = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for val_i, (images, labels) in enumerate(tqdm(val_loader, desc=\"Validation\", leave=False)):\n",
    "                    try:\n",
    "                        # Validate batch shape\n",
    "                        expected_size = config.get('data', {}).get('input_size', 224)\n",
    "                        if images.shape[1] != 3 or images.shape[2] != expected_size or images.shape[3] != expected_size:\n",
    "                            logger.warning(\n",
    "                                f\"‚ö†Ô∏è  Validation batch {val_i} has invalid shape: {images.shape}. Skipping.\"\n",
    "                            )\n",
    "                            continue\n",
    "\n",
    "                        use_non_blocking = (device.type == \"cuda\")\n",
    "                        images, labels = images.to(device, non_blocking=use_non_blocking), labels.to(device, non_blocking=use_non_blocking)\n",
    "\n",
    "                        if use_amp:\n",
    "                            with torch.cuda.amp.autocast():\n",
    "                                outputs = model(images)\n",
    "                                loss = criterion(outputs, labels)\n",
    "                        else:\n",
    "                            outputs = model(images)\n",
    "                            loss = criterion(outputs, labels)\n",
    "\n",
    "                        loss_val = loss.item()\n",
    "                        if not np.isfinite(loss_val):\n",
    "                            logger.warning(f\"‚ö†Ô∏è  Non-finite validation loss at batch {val_i}: {loss_val}. Skipping.\")\n",
    "                            continue\n",
    "\n",
    "                        val_loss += loss_val\n",
    "                        _, predicted = outputs.max(1)\n",
    "                        val_total += labels.size(0)\n",
    "                        val_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "                        # Collect for per-class metrics\n",
    "                        all_preds.extend(predicted.cpu().numpy())\n",
    "                        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"‚ö†Ô∏è  Validation batch {val_i} failed: {e}. Skipping.\")\n",
    "                        continue\n",
    "\n",
    "            # CRITICAL: Validation health check\n",
    "            if val_total == 0:\n",
    "                logger.error(f\"‚ùå Epoch {epoch+1}: No validation samples processed!\")\n",
    "                logger.error(\"   This indicates a critical issue with validation data.\")\n",
    "                break\n",
    "\n",
    "            val_acc = 100 * val_correct / val_total\n",
    "            val_loss /= len(val_loader)\n",
    "\n",
    "            if not np.isfinite(val_loss):\n",
    "                logger.error(f\"‚ùå Epoch {epoch+1}: Non-finite validation loss: {val_loss}\")\n",
    "                logger.error(\"   Validation has become unstable. Stopping.\")\n",
    "                break\n",
    "\n",
    "            # INDUSTRIAL-GRADE: Per-class metrics\n",
    "            if len(all_preds) == 0 or len(all_labels) == 0:\n",
    "                logger.error(f\"‚ùå Epoch {epoch+1}: No predictions collected for metrics!\")\n",
    "                break\n",
    "\n",
    "            precision, recall, f1, support = precision_recall_fscore_support(\n",
    "                all_labels, all_preds, average=None, zero_division=0\n",
    "            )\n",
    "            macro_f1 = f1.mean()\n",
    "\n",
    "            # Find worst performing classes\n",
    "            worst_classes_idx = np.argsort(f1)[:5]\n",
    "            logger.info(f\"üìä Per-Class Performance:\")\n",
    "            logger.info(f\"  Macro F1: {macro_f1:.4f}\")\n",
    "            logger.info(f\"  Worst 5 classes:\")\n",
    "            for idx in worst_classes_idx:\n",
    "                if idx < len(TARGET_CLASSES):\n",
    "                    logger.info(f\"    {TARGET_CLASSES[idx]}: F1={f1[idx]:.4f}, Support={support[idx]}\")\n",
    "\n",
    "            logger.info(f\"Epoch {epoch+1}/{config['training']['num_epochs']}: Train Acc {train_acc:.2f}%, Val Loss {val_loss:.4f}, Val Acc {val_acc:.2f}%, Macro F1 {macro_f1:.4f}\")\n",
    "\n",
    "            # INDUSTRIAL-GRADE: Track metrics history\n",
    "            metrics_history[\"train_acc\"].append(train_acc)\n",
    "            metrics_history[\"val_acc\"].append(val_acc)\n",
    "            metrics_history[\"val_loss\"].append(val_loss)\n",
    "            metrics_history[\"per_class_f1\"].append(macro_f1)\n",
    "            metrics_history[\"learning_rate\"].append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "            try:\n",
    "                wandb.log({\n",
    "                    \"epoch\": epoch + 1,\n",
    "                    \"train_acc\": train_acc,\n",
    "                    \"val_acc\": val_acc,\n",
    "                    \"val_loss\": val_loss,\n",
    "                    \"macro_f1\": macro_f1,\n",
    "                    \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "                    \"worst_class_f1\": f1[worst_classes_idx[0]] if len(worst_classes_idx) > 0 else 0\n",
    "                })\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # INDUSTRIAL-GRADE: Save best model checkpoint\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                best_model_state = {\n",
    "                    'epoch': epoch + 1,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': scheduler.state_dict(),\n",
    "                    'val_acc': val_acc,\n",
    "                    'val_loss': val_loss,\n",
    "                    'macro_f1': macro_f1,\n",
    "                    'config': config,\n",
    "                    'metrics_history': metrics_history\n",
    "                }\n",
    "                checkpoint_path = checkpoint_dir / f\"best_model_epoch{epoch+1}_acc{val_acc:.2f}.pth\"\n",
    "                torch.save(best_model_state, checkpoint_path)\n",
    "                logger.info(f\"‚úì Saved best model checkpoint: {checkpoint_path}\")\n",
    "\n",
    "                # Keep only best checkpoint, delete others\n",
    "                for old_ckpt in checkpoint_dir.glob(\"best_model_*.pth\"):\n",
    "                    if old_ckpt != checkpoint_path:\n",
    "                        old_ckpt.unlink()\n",
    "\n",
    "            if early_stopping(val_acc):\n",
    "                logger.info(\"Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "            # Clear cache after each epoch to prevent memory fragmentation\n",
    "            if device.type == \"cuda\":\n",
    "                torch.cuda.empty_cache()\n",
    "            elif device.type == \"mps\":\n",
    "                try:\n",
    "                    torch.mps.empty_cache()\n",
    "                except AttributeError:\n",
    "                    pass  # MPS empty_cache not available in older PyTorch\n",
    "\n",
    "        # INDUSTRIAL-GRADE: Training completed - generate final report\n",
    "        logger.info(\"=\"*60)\n",
    "        logger.info(\"‚úì Training completed successfully\")\n",
    "        logger.info(f\"üìä Final Results:\")\n",
    "        logger.info(f\"  Best Val Accuracy: {best_val_acc:.2f}%\")\n",
    "        logger.info(f\"  Total Epochs: {epoch + 1}\")\n",
    "        logger.info(f\"  Best Checkpoint: {checkpoint_path if best_model_state else 'None'}\")\n",
    "        logger.info(\"=\"*60)\n",
    "\n",
    "        # INDUSTRIAL-GRADE: Generate confusion matrix for best model\n",
    "        if best_model_state:\n",
    "            logger.info(\"Generating confusion matrix for best model...\")\n",
    "            model.load_state_dict(best_model_state['model_state_dict'])\n",
    "            model.eval()\n",
    "\n",
    "            all_preds = []\n",
    "            all_labels = []\n",
    "            with torch.no_grad():\n",
    "                for images, labels in tqdm(val_loader, desc=\"Final Evaluation\"):\n",
    "                    images = images.to(device)\n",
    "                    outputs = model(images)\n",
    "                    _, predicted = outputs.max(1)\n",
    "                    all_preds.extend(predicted.cpu().numpy())\n",
    "                    all_labels.extend(labels.numpy())\n",
    "\n",
    "            # Save confusion matrix\n",
    "            cm = confusion_matrix(all_labels, all_preds)\n",
    "            np.save(checkpoint_dir / \"confusion_matrix.npy\", cm)\n",
    "\n",
    "            # Save classification report\n",
    "            report = classification_report(\n",
    "                all_labels, all_preds,\n",
    "                target_names=TARGET_CLASSES,\n",
    "                output_dict=True,\n",
    "                zero_division=0\n",
    "            )\n",
    "            with open(checkpoint_dir / \"classification_report.json\", \"w\") as f:\n",
    "                json.dump(report, f, indent=2)\n",
    "\n",
    "            logger.info(f\"‚úì Saved confusion matrix and classification report to {checkpoint_dir}\")\n",
    "\n",
    "        # INDUSTRIAL-GRADE: Save final metrics\n",
    "        with open(checkpoint_dir / \"metrics_history.json\", \"w\") as f:\n",
    "            json.dump(metrics_history, f, indent=2)\n",
    "\n",
    "        logger.info(\"‚úì All artifacts saved successfully\")\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" in str(e) or \"MPS\" in str(e):\n",
    "            logger.error(f\"OOM Error: {e}\")\n",
    "            logger.error(\"Suggestions:\")\n",
    "            logger.error(\"  1. Reduce batch_size further (try batch_size=1)\")\n",
    "            logger.error(\"  2. Reduce input_size (try 128 or 192)\")\n",
    "            logger.error(\"  3. Use a smaller model backbone (e.g., resnet50)\")\n",
    "            if device.type == \"cuda\":\n",
    "                torch.cuda.empty_cache()\n",
    "            elif device.type == \"mps\":\n",
    "                try:\n",
    "                    torch.mps.empty_cache()\n",
    "                except AttributeError:\n",
    "                    pass\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Training failed with error: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        # Cleanup\n",
    "        try:\n",
    "            wandb.finish()\n",
    "        except:\n",
    "            pass\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "        elif device.type == \"mps\":\n",
    "            try:\n",
    "                torch.mps.empty_cache()\n",
    "            except AttributeError:\n",
    "                pass\n",
    "\n",
    "    return model\n"
   ],
   "id": "35e0483892870ce3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# INDUSTRIAL-GRADE: Test-Time Augmentation for Inference\n",
    "def predict_with_tta(model, image, device, num_augmentations=5):\n",
    "    \"\"\"\n",
    "    Test-Time Augmentation for robust predictions.\n",
    "    Applies multiple augmentations and averages predictions.\n",
    "\n",
    "    Args:\n",
    "        model: Trained model\n",
    "        image: PIL Image or tensor\n",
    "        device: torch device\n",
    "        num_augmentations: Number of TTA iterations\n",
    "\n",
    "    Returns:\n",
    "        Averaged predictions (logits)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # TTA transforms\n",
    "    tta_transforms = [\n",
    "        transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomRotation(degrees=15),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        for _ in range(num_augmentations)\n",
    "    ]\n",
    "\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for transform in tta_transforms:\n",
    "            if isinstance(image, torch.Tensor):\n",
    "                img_tensor = image\n",
    "            else:\n",
    "                img_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "            output = model(img_tensor)\n",
    "            predictions.append(output)\n",
    "\n",
    "    # Average predictions\n",
    "    avg_prediction = torch.stack(predictions).mean(dim=0)\n",
    "    return avg_prediction\n"
   ],
   "id": "dd26b5c26bdf32d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# INDUSTRIAL-GRADE: Model Export for Production\n",
    "def export_model_for_production(model, config, checkpoint_path, export_dir=\"exports\"):\n",
    "    \"\"\"\n",
    "    Export model to multiple formats for production deployment.\n",
    "\n",
    "    Exports:\n",
    "    - PyTorch (.pth) - for PyTorch inference\n",
    "    - TorchScript (.pt) - for C++ deployment\n",
    "    - ONNX (.onnx) - for cross-platform deployment\n",
    "    \"\"\"\n",
    "    export_dir = Path(export_dir)\n",
    "    export_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # 1. Save PyTorch model\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'config': config,\n",
    "        'target_classes': TARGET_CLASSES\n",
    "    }, export_dir / \"model.pth\")\n",
    "    logger.info(f\"‚úì Exported PyTorch model to {export_dir / 'model.pth'}\")\n",
    "\n",
    "    # 2. Export to TorchScript\n",
    "    try:\n",
    "        dummy_input = torch.randn(1, 3, config['data']['input_size'], config['data']['input_size']).to(device)\n",
    "        traced_model = torch.jit.trace(model, dummy_input)\n",
    "        traced_model.save(export_dir / \"model_torchscript.pt\")\n",
    "        logger.info(f\"‚úì Exported TorchScript model to {export_dir / 'model_torchscript.pt'}\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"TorchScript export failed: {e}\")\n",
    "\n",
    "    # 3. Export to ONNX\n",
    "    try:\n",
    "        torch.onnx.export(\n",
    "            model,\n",
    "            dummy_input,\n",
    "            export_dir / \"model.onnx\",\n",
    "            export_params=True,\n",
    "            opset_version=14,\n",
    "            do_constant_folding=True,\n",
    "            input_names=['input'],\n",
    "            output_names=['output'],\n",
    "            dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
    "        )\n",
    "        logger.info(f\"‚úì Exported ONNX model to {export_dir / 'model.onnx'}\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"ONNX export failed: {e}\")\n",
    "\n",
    "    # 4. Save metadata\n",
    "    metadata = {\n",
    "        'model_name': config['model']['backbone'],\n",
    "        'num_classes': config['model']['num_classes'],\n",
    "        'input_size': config['data']['input_size'],\n",
    "        'target_classes': TARGET_CLASSES,\n",
    "        'checkpoint_path': str(checkpoint_path)\n",
    "    }\n",
    "    with open(export_dir / \"metadata.json\", \"w\") as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    logger.info(f\"‚úì Saved metadata to {export_dir / 'metadata.json'}\")\n",
    "\n",
    "    logger.info(f\"‚úÖ Model export complete! All files in {export_dir}\")\n"
   ],
   "id": "184a60cacd5ef3a2",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# PEAK STANDARD GNN\n",
    "# Using Graph Attention Networks v2 (GATv2) for superior expressive power\n",
    "\n",
    "def generate_structured_knowledge_graph(num_classes=30, feat_dim=128):\n",
    "    \"\"\"\n",
    "    Generates a realistic Knowledge Graph structure for waste classification.\n",
    "    Simulates the schema: Item -> Material -> Bin\n",
    "    \"\"\"\n",
    "    logger.info(\"Generating structured Knowledge Graph...\")\n",
    "    \n",
    "    total_nodes = num_classes + 8 + 4\n",
    "    x = torch.randn(total_nodes, feat_dim) # Node features (embeddings)\n",
    "    \n",
    "    edge_sources = []\n",
    "    edge_targets = []\n",
    "    \n",
    "    # Node Indices for Materials\n",
    "    mat_base = num_classes\n",
    "    mat_plastic = mat_base + 0\n",
    "    mat_paper = mat_base + 1\n",
    "    mat_glass = mat_base + 2\n",
    "    mat_metal = mat_base + 3\n",
    "    mat_organic = mat_base + 4\n",
    "    mat_fabric = mat_base + 5\n",
    "    mat_ewaste = mat_base + 6\n",
    "    mat_misc = mat_base + 7\n",
    "    \n",
    "    # Node Indices for Bins\n",
    "    bin_base = mat_base + 8\n",
    "    bin_recycle = bin_base + 0\n",
    "    bin_compost = bin_base + 1\n",
    "    bin_haz = bin_base + 2\n",
    "    bin_landfill = bin_base + 3\n",
    "    \n",
    "    # 1. Edges: Material -> Bin (Knowledge Rules)\n",
    "    mat_bin_map = [\n",
    "        (mat_plastic, bin_recycle),\n",
    "        (mat_paper, bin_recycle),\n",
    "        (mat_glass, bin_recycle),\n",
    "        (mat_metal, bin_recycle),\n",
    "        (mat_organic, bin_compost),\n",
    "        (mat_fabric, bin_landfill), \n",
    "        (mat_ewaste, bin_haz),\n",
    "        (mat_misc, bin_landfill)\n",
    "    ]\n",
    "    \n",
    "    for m, b in mat_bin_map:\n",
    "        edge_sources.append(m); edge_targets.append(b)\n",
    "        edge_sources.append(b); edge_targets.append(m)\n",
    "        \n",
    "    # 2. Edges: Item -> Material (Simulate Classification Knowledge)\n",
    "    for i in range(num_classes):\n",
    "        mat_idx = mat_base + (i % 8) \n",
    "        edge_sources.append(i); edge_targets.append(mat_idx)\n",
    "        edge_sources.append(mat_idx); edge_targets.append(i)\n",
    "        \n",
    "    # 3. Edges: Item -> Item (Similarity)\n",
    "    for i in range(num_classes):\n",
    "        neighbor = (i + 8) % num_classes\n",
    "        edge_sources.append(i); edge_targets.append(neighbor)\n",
    "        edge_sources.append(neighbor); edge_targets.append(i)\n",
    "\n",
    "    edge_index = torch.tensor([edge_sources, edge_targets], dtype=torch.long)\n",
    "    \n",
    "    logger.info(f\"Graph generated: {total_nodes} nodes, {len(edge_sources)} edges.\")\n",
    "    \n",
    "    return Data(x=x, edge_index=edge_index, num_nodes=total_nodes)\n",
    "\n",
    "class GATv2Model(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=4, heads=8, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(GATv2Conv(in_channels, hidden_channels, heads=heads, concat=True, dropout=dropout))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(GATv2Conv(hidden_channels * heads, hidden_channels, heads=heads, concat=True, dropout=dropout))\n",
    "        self.convs.append(GATv2Conv(hidden_channels * heads, out_channels, heads=1, concat=False, dropout=dropout))\n",
    "        self.dropout = dropout\n",
    "        self.norm = nn.ModuleList([nn.LayerNorm(hidden_channels * heads) for _ in range(num_layers - 1)])\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for i, conv in enumerate(self.convs[:-1]):\n",
    "            x = conv(x, edge_index)\n",
    "            x = self.norm[i](x)\n",
    "            x = F.gelu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        return self.convs[-1](x, edge_index)"
   ],
   "id": "a413b4ee062ae6a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def train_gnn_model():\n",
    "    set_seed()\n",
    "    device = get_device()\n",
    "    optimize_memory(device)\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "\n",
    "    in_dim = 128\n",
    "    hidden_dim = 512\n",
    "    out_dim = 256\n",
    "    lr = 0.001\n",
    "    epochs = 50\n",
    "\n",
    "    data = generate_structured_knowledge_graph(num_classes=30, feat_dim=128).to(device)\n",
    "\n",
    "    model = GATv2Model(in_dim, hidden_dim, out_dim).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)\n",
    "\n",
    "    logger.info(\"Starting GNN Training...\")\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        z = model(data.x, data.edge_index)\n",
    "\n",
    "        pos_src, pos_dst = data.edge_index\n",
    "        pos_loss = -torch.log(torch.sigmoid((z[pos_src] * z[pos_dst]).sum(dim=1)) + 1e-15).mean()\n",
    "\n",
    "        neg_src = torch.randint(0, data.num_nodes, (pos_src.size(0),), device=device)\n",
    "        neg_dst = torch.randint(0, data.num_nodes, (pos_src.size(0),), device=device)\n",
    "        neg_loss = -torch.log(1 - torch.sigmoid((z[neg_src] * z[neg_dst]).sum(dim=1)) + 1e-15).mean()\n",
    "\n",
    "        loss = pos_loss + neg_loss\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step(loss)\n",
    "\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            logger.info(f\"Epoch {epoch+1}/{epochs}: Loss {loss.item():.4f}, Best Loss {best_loss:.4f}\")\n",
    "\n",
    "    return model"
   ],
   "id": "d48fee1a1cb8a264",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        logger.info(\"=\"*80)\n",
    "        logger.info(\"Phase 1: Multi-Source Data Lake Vision Training\")\n",
    "        logger.info(\"=\"*80)\n",
    "\n",
    "        vision_model = train_vision_model(VISION_CONFIG)\n",
    "\n",
    "        if vision_model is not None:\n",
    "            save_path = \"best_vision_eva02_lake.pth\"\n",
    "            torch.save(vision_model.state_dict(), save_path)\n",
    "            logger.info(f\"Vision model saved to {save_path}\")\n",
    "\n",
    "            del vision_model\n",
    "            device = get_device()\n",
    "            if device.type == \"cuda\":\n",
    "                torch.cuda.empty_cache()\n",
    "            elif device.type == \"mps\":\n",
    "                try:\n",
    "                    torch.mps.empty_cache()\n",
    "                except AttributeError:\n",
    "                    pass\n",
    "        else:\n",
    "            logger.error(\"Vision model training failed\")\n",
    "\n",
    "        logger.info(\"=\"*80)\n",
    "        logger.info(\"Phase 2: GNN Knowledge Graph Training\")\n",
    "        logger.info(\"=\"*80)\n",
    "\n",
    "        gnn_model = train_gnn_model()\n",
    "\n",
    "        if gnn_model is not None:\n",
    "            save_path = \"best_gnn_gatv2.pth\"\n",
    "            torch.save(gnn_model.state_dict(), save_path)\n",
    "            logger.info(f\"GNN model saved to {save_path}\")\n",
    "\n",
    "            del gnn_model\n",
    "            device = get_device()\n",
    "            if device.type == \"cuda\":\n",
    "                torch.cuda.empty_cache()\n",
    "            elif device.type == \"mps\":\n",
    "                try:\n",
    "                    torch.mps.empty_cache()\n",
    "                except AttributeError:\n",
    "                    pass\n",
    "\n",
    "        logger.info(\"=\"*80)\n",
    "        logger.info(\"Training completed successfully!\")\n",
    "        logger.info(\"=\"*80)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Training failed with error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise"
   ],
   "id": "78cc46781594be13",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
