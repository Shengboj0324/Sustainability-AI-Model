# üçé Apple M4 Max Training Guide - ReleAF AI

## üéØ System Status

**M4 Max Readiness**: ‚úÖ 100%  
**PyTorch MPS**: ‚úÖ Enabled  
**Memory**: ‚úÖ 36GB Available  
**All Components**: ‚úÖ Optimized for Apple Silicon

---

## ‚ö° Quick Start

### 1. Verify M4 Max Readiness (30 seconds)
```bash
python3 scripts/m4max_preflight_check.py
```

Expected output: **üéâ 100% READY FOR M4 MAX TRAINING!**

### 2. Start LLM Training (RECOMMENDED FIRST)
```bash
python3 training/llm/train_sft.py --config configs/llm_sft_m4max.yaml
```

**Estimated Time**: 2-3 hours on M4 Max  
**Memory Usage**: ~20-25GB  
**Output**: `models/llm/adapters/sustainability-m4max-v1/`

---

## üîç M4 Max Specific Optimizations

### What Was Changed for M4 Max

#### 1. **Device Detection** ‚úÖ
All training scripts now automatically detect and use MPS:
- `training/llm/train_sft.py` - MPS support added
- `training/vision/train_multihead.py` - MPS support added
- `training/gnn/train_gnn.py` - MPS support added
- `services/shared/common.py` - MPS cache clearing added

#### 2. **Precision Settings** ‚úÖ
- **BFloat16**: ‚ùå NOT supported on MPS ‚Üí Disabled
- **Float16**: ‚úÖ Fully supported ‚Üí Enabled
- **Float32**: ‚úÖ Fallback option

#### 3. **Quantization** ‚úÖ
- **4-bit Quantization**: ‚ùå NOT supported on MPS ‚Üí Disabled
- **Full Precision**: ‚úÖ Used instead (M4 Max has enough memory)

#### 4. **Batch Sizes** ‚úÖ
Increased due to unified memory architecture:
- **LLM**: 8 (was 4) - 2x larger
- **Vision**: 128 (was 64) - 2x larger
- **Gradient Accumulation**: Adjusted for same effective batch size

#### 5. **Memory Management** ‚úÖ
- Added `torch.mps.empty_cache()` support
- Optimized data loading for unified memory
- Disabled `pin_memory` (not needed for MPS)

---

## üìã Training Commands

### LLM Training (Llama-3-8B with LoRA)

**M4 Max Optimized**:
```bash
python3 training/llm/train_sft.py --config configs/llm_sft_m4max.yaml
```

**Standard (auto-detects M4 Max)**:
```bash
python3 training/llm/train_sft.py
```

**Key Settings**:
- Batch size: 8 per device
- Gradient accumulation: 4 steps
- Effective batch size: 32
- Precision: FP16
- LoRA rank: 64
- No quantization

**Expected Performance**:
- Training time: 2-3 hours (3 epochs)
- Memory usage: 20-25GB
- GPU utilization: 80-95%

---

### Vision Training (Multi-Head Classifier)

**M4 Max Optimized**:
```bash
python3 training/vision/train_multihead.py --config configs/vision_cls_m4max.yaml
```

**Standard (auto-detects M4 Max)**:
```bash
python3 training/vision/train_multihead.py
```

**Key Settings**:
- Batch size: 128
- Validation batch size: 256
- Precision: FP16 with AMP
- Epochs: 40

**Expected Performance**:
- Training time: 1-2 hours
- Memory usage: 8-12GB
- GPU utilization: 85-95%

---

### GNN Training (GraphSAGE)

**Command**:
```bash
python3 training/gnn/train_gnn.py
```

**Key Settings**:
- Auto-detects MPS
- Batch size: 1024
- Epochs: 50

**Expected Performance**:
- Training time: 30 minutes
- Memory usage: 4-6GB
- GPU utilization: 70-85%

---

## üîß Configuration Files

### M4 Max Specific Configs

1. **`configs/llm_sft_m4max.yaml`**
   - Optimized for Apple Silicon
   - FP16 precision
   - No quantization
   - Larger batch sizes

2. **`configs/vision_cls_m4max.yaml`**
   - Optimized for unified memory
   - 2x larger batch sizes
   - FP16 with AMP

### Standard Configs (Auto-Adapt)

The standard configs will automatically adapt to M4 Max:
- `configs/llm_sft.yaml` - Auto-detects MPS, adjusts precision
- `configs/vision_cls.yaml` - Auto-detects MPS
- `configs/gnn.yaml` - Auto-detects MPS

---

## üìä Monitoring Training

### Activity Monitor
```bash
# Open Activity Monitor
open -a "Activity Monitor"
```

Watch for:
- **Memory Pressure**: Should stay green
- **GPU Usage**: Should be 80-95% during training
- **CPU Usage**: 30-50% for data loading

### Training Logs

All training scripts log to stdout:
```bash
# LLM training logs
python3 training/llm/train_sft.py --config configs/llm_sft_m4max.yaml 2>&1 | tee llm_training.log

# Vision training logs
python3 training/vision/train_multihead.py --config configs/vision_cls_m4max.yaml 2>&1 | tee vision_training.log
```

### Weights & Biases (Optional)

All scripts support W&B logging:
```bash
# Install W&B
pip install wandb

# Login
wandb login

# Training will automatically log to W&B
```

---

## ‚ö†Ô∏è Common Issues & Solutions

### Issue 1: "MPS backend not available"
**Solution**:
```bash
pip install --upgrade torch torchvision torchaudio
```

### Issue 2: "Out of memory"
**Solution**:
- Reduce batch size in config file
- Close other applications
- Use gradient checkpointing (for LLM)

### Issue 3: "BFloat16 not supported"
**Solution**:
- This is expected on MPS
- Scripts automatically use FP16 instead
- No action needed

### Issue 4: Slow training
**Solution**:
- Check Activity Monitor for memory pressure
- Ensure no other heavy apps running
- Verify GPU is being used (check logs for "Using Apple M4 Max GPU (MPS)")

---

## üéØ Training Order (Recommended)

1. **LLM Training** (2-3 hours) ‚≠ê START HERE
   - Most important for user-facing features
   - Command: `python3 training/llm/train_sft.py --config configs/llm_sft_m4max.yaml`

2. **Vision Classifier** (1-2 hours)
   - Image classification for waste sorting
   - Command: `python3 training/vision/train_multihead.py --config configs/vision_cls_m4max.yaml`

3. **GNN** (30 minutes)
   - Upcycling recommendations
   - Command: `python3 training/gnn/train_gnn.py`

**Total Time**: 4-6 hours for all models

---

## üöÄ After Training

### 1. Verify Models
```bash
ls -lh models/llm/adapters/sustainability-m4max-v1/
ls -lh models/vision/classifier_m4max/
ls -lh models/gnn/ckpts/
```

### 2. Test Inference
```bash
# Test LLM
python3 -c "
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

base_model = AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3-8B-Instruct')
model = PeftModel.from_pretrained(base_model, 'models/llm/adapters/sustainability-m4max-v1')
tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-3-8B-Instruct')

prompt = 'How do I recycle plastic bottles?'
inputs = tokenizer(prompt, return_tensors='pt')
outputs = model.generate(**inputs, max_length=100)
print(tokenizer.decode(outputs[0]))
"
```

### 3. Deploy Services
See `GETTING_STARTED.md` for deployment instructions.

---

## üìà Performance Benchmarks

### M4 Max (36GB Unified Memory)

| Model | Batch Size | Time/Epoch | Total Time | Memory |
|-------|-----------|------------|------------|--------|
| LLM (Llama-3-8B) | 8 | 40-50 min | 2-3 hours | 20-25GB |
| Vision (ViT) | 128 | 2-3 min | 1-2 hours | 8-12GB |
| GNN (GraphSAGE) | 1024 | <1 min | 30 min | 4-6GB |

---

## ‚úÖ Checklist

Before starting training:
- [ ] Run `python3 scripts/m4max_preflight_check.py`
- [ ] Verify 100% readiness
- [ ] Close unnecessary applications
- [ ] Ensure sufficient disk space (50GB+)
- [ ] Optional: Setup W&B logging

During training:
- [ ] Monitor Activity Monitor
- [ ] Check training logs for errors
- [ ] Verify GPU utilization is high

After training:
- [ ] Verify model files exist
- [ ] Test inference
- [ ] Save checkpoints to backup

---

**üéâ Your M4 Max is ready for peak performance AI training!**

