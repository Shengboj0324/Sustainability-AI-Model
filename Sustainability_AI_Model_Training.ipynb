{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-20T22:27:15.762585Z",
     "start_time": "2026-02-20T22:27:13.477280Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "print(f\"Python: {sys.executable}\")\n",
    "\n",
    "# Uninstall NumPy 2.x\n",
    "print(\"Uninstalling NumPy 2.x...\")\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \"numpy\"], check=True)\n",
    "\n",
    "# Install NumPy 1.26.4\n",
    "print(\"Installing NumPy 1.26.4...\")\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"numpy==1.26.4\"], check=True)\n",
    "\n",
    "print(\"\\n‚úÖ DONE! Now restart kernel: Kernel ‚Üí Restart Kernel\")"
   ],
   "id": "357da545f4ba0043",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: /Applications/Xcode.app/Contents/Developer/usr/bin/python3\n",
      "Uninstalling NumPy 2.x...\n",
      "Found existing installation: numpy 2.0.2\n",
      "Uninstalling numpy-2.0.2:\n",
      "  Successfully uninstalled numpy-2.0.2\n",
      "Installing NumPy 1.26.4...\n",
      "Defaulting to user installation because normal site-packages is not writeable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[33mWARNING: Ignoring invalid distribution -cipy (/Users/jiangshengbo/Library/Python/3.9/lib/python/site-packages)\u001B[0m\u001B[33m\n",
      "\u001B[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.26.4\n",
      "  Using cached numpy-1.26.4-cp39-cp39-macosx_11_0_arm64.whl.metadata (61 kB)\n",
      "Using cached numpy-1.26.4-cp39-cp39-macosx_11_0_arm64.whl (14.0 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[33mWARNING: Ignoring invalid distribution -cipy (/Users/jiangshengbo/Library/Python/3.9/lib/python/site-packages)\u001B[0m\u001B[33m\n",
      "\u001B[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: numpy\n",
      "Successfully installed numpy-1.26.4\n",
      "\n",
      "‚úÖ DONE! Now restart kernel: Kernel ‚Üí Restart Kernel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[33m  WARNING: The script f2py is installed in '/Users/jiangshengbo/Library/Python/3.9/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "chromadb 1.1.0 requires tenacity>=8.2.3, but you have tenacity 8.2.0 which is incompatible.\n",
      "great-expectations 0.17.0 requires pydantic<2.0,>=1.9.2, but you have pydantic 2.12.3 which is incompatible.\n",
      "opencv-python-headless 4.13.0.92 requires numpy>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001B[0m\u001B[31m\n",
      "\u001B[0m\u001B[33mWARNING: Ignoring invalid distribution -cipy (/Users/jiangshengbo/Library/Python/3.9/lib/python/site-packages)\u001B[0m\u001B[33m\n",
      "\u001B[0m"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2026-02-20T22:27:15.766335Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# Uninstall broken packages\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \"scipy\", \"albumentations\", \"opencv-python\", \"opencv-python-headless\"])\n",
    "\n",
    "# Install in correct order\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"--no-cache-dir\", \"scipy>=1.7.0,<1.15.0\"])\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"--no-cache-dir\", \"opencv-python-headless>=4.5.0\"])\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"--no-cache-dir\", \"albumentations>=1.3.0\"])\n",
    "\n",
    "print(\"‚úÖ Done! Now restart the kernel: Kernel ‚Üí Restart Kernel\")"
   ],
   "id": "f314169c67b2a3a1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: scipy 1.13.1\n",
      "Uninstalling scipy-1.13.1:\n",
      "  Successfully uninstalled scipy-1.13.1\n",
      "Found existing installation: albumentations 2.0.8\n",
      "Uninstalling albumentations-2.0.8:\n",
      "  Successfully uninstalled albumentations-2.0.8\n",
      "Found existing installation: opencv-python-headless 4.13.0.92\n",
      "Uninstalling opencv-python-headless-4.13.0.92:\n",
      "  Successfully uninstalled opencv-python-headless-4.13.0.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[33mWARNING: Ignoring invalid distribution -cipy (/Users/jiangshengbo/Library/Python/3.9/lib/python/site-packages)\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mWARNING: Skipping opencv-python as it is not installed.\u001B[0m\u001B[33m\n",
      "\u001B[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[33mWARNING: Ignoring invalid distribution -cipy (/Users/jiangshengbo/Library/Python/3.9/lib/python/site-packages)\u001B[0m\u001B[33m\n",
      "\u001B[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scipy<1.15.0,>=1.7.0\n",
      "  Downloading scipy-1.13.1-cp39-cp39-macosx_12_0_arm64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: numpy<2.3,>=1.22.4 in /Users/jiangshengbo/Library/Python/3.9/lib/python/site-packages (from scipy<1.15.0,>=1.7.0) (1.26.4)\n",
      "Downloading scipy-1.13.1-cp39-cp39-macosx_12_0_arm64.whl (30.3 MB)\n",
      "\u001B[2K   \u001B[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m \u001B[32m30.3/30.3 MB\u001B[0m \u001B[31m45.1 MB/s\u001B[0m  \u001B[33m0:00:00\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n",
      "\u001B[?25h"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"Installing dependencies for Sustainability AI Model (MacBook Local Training)...\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Install packages one by one with error handling\n",
    "def install_package(package_spec, description=\"\"):\n",
    "    \"\"\"Install a package with error handling.\"\"\"\n",
    "    try:\n",
    "        print(f\"Installing {description or package_spec}...\")\n",
    "        subprocess.check_call(\n",
    "            [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--upgrade\"] + package_spec.split(),\n",
    "            timeout=300  # 5 minute timeout per package\n",
    "        )\n",
    "        print(f\"  ‚úÖ {description or package_spec}\")\n",
    "        return True\n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(f\"  ‚ö†Ô∏è  Timeout installing {description or package_spec}, skipping...\")\n",
    "        return False\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"  ‚ö†Ô∏è  Failed to install {description or package_spec}: {e}\")\n",
    "        return False\n",
    "\n",
    "# Upgrade pip first\n",
    "print(\"Upgrading pip...\")\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--upgrade\", \"pip\"], timeout=60)\n",
    "\n",
    "# Install Kaggle API\n",
    "install_package(\"kaggle\", \"Kaggle API\")\n",
    "\n",
    "# Install core dependencies (Python 3.9 compatible versions)\n",
    "install_package(\"numpy>=1.19.0,<2.0\", \"NumPy\")\n",
    "install_package(\"scipy>=1.7.0,<1.15.0\", \"SciPy\")\n",
    "install_package(\"Pillow>=8.0.0\", \"Pillow\")\n",
    "install_package(\"pandas>=1.3.0\", \"Pandas\")\n",
    "install_package(\"scikit-learn>=1.0.0\", \"scikit-learn\")\n",
    "install_package(\"matplotlib>=3.4.0\", \"Matplotlib\")\n",
    "install_package(\"seaborn>=0.11.0\", \"Seaborn\")\n",
    "install_package(\"tqdm>=4.62.0\", \"tqdm\")\n",
    "\n",
    "# Install PyTorch with compatible torchvision version\n",
    "print(\"Checking PyTorch installation...\")\n",
    "try:\n",
    "    import torch\n",
    "    import torchvision\n",
    "    torch_version = torch.__version__\n",
    "    torchvision_version = torchvision.__version__\n",
    "    print(f\"  Current PyTorch: {torch_version}\")\n",
    "    print(f\"  Current torchvision: {torchvision_version}\")\n",
    "\n",
    "    # Check if versions are compatible\n",
    "    # PyTorch 2.x needs torchvision 0.15+\n",
    "    # PyTorch 1.x needs torchvision 0.x\n",
    "    torch_major = int(torch_version.split('.')[0])\n",
    "    tv_major = int(torchvision_version.split('.')[0])\n",
    "\n",
    "    if torch_major == 2 and tv_major == 0 and int(torchvision_version.split('.')[1]) < 15:\n",
    "        print(\"  ‚ö†Ô∏è  Version mismatch detected! Reinstalling compatible versions...\")\n",
    "        subprocess.run([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \"torch\", \"torchvision\"], check=False)\n",
    "        install_package(\"torch==2.0.1 torchvision==0.15.2\", \"PyTorch 2.0.1 + torchvision 0.15.2\")\n",
    "    elif torch_major != tv_major:\n",
    "        print(\"  ‚ö†Ô∏è  Major version mismatch! Reinstalling compatible versions...\")\n",
    "        subprocess.run([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \"torch\", \"torchvision\"], check=False)\n",
    "        install_package(\"torch==2.0.1 torchvision==0.15.2\", \"PyTorch 2.0.1 + torchvision 0.15.2\")\n",
    "    else:\n",
    "        print(\"  ‚úÖ PyTorch and torchvision versions are compatible\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"  Installing PyTorch and torchvision...\")\n",
    "    install_package(\"torch==2.0.1 torchvision==0.15.2\", \"PyTorch 2.0.1 + torchvision 0.15.2\")\n",
    "except AttributeError as e:\n",
    "    print(f\"  ‚ö†Ô∏è  Version compatibility issue detected: {e}\")\n",
    "    print(\"  Reinstalling compatible PyTorch and torchvision versions...\")\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \"torch\", \"torchvision\"], check=False)\n",
    "    install_package(\"torch==2.0.1 torchvision==0.15.2\", \"PyTorch 2.0.1 + torchvision 0.15.2\")\n",
    "\n",
    "# Install timm (Python 3.9 compatible)\n",
    "install_package(\"timm>=0.9.0\", \"timm\")\n",
    "\n",
    "# Install albumentations\n",
    "install_package(\"albumentations>=1.3.0\", \"Albumentations\")\n",
    "\n",
    "# Install other dependencies\n",
    "install_package(\"einops>=0.6.0\", \"einops\")\n",
    "install_package(\"wandb>=0.15.0\", \"Weights & Biases\")\n",
    "\n",
    "# Install PyTorch Geometric (simplified for Python 3.9)\n",
    "print(\"Installing PyTorch Geometric...\")\n",
    "install_package(\"torch-geometric\", \"PyTorch Geometric\")\n",
    "\n",
    "# Try to install torch-scatter and torch-sparse (optional, may fail on some systems)\n",
    "print(\"Installing optional PyG dependencies (may fail, that's OK)...\")\n",
    "try:\n",
    "    subprocess.run(\n",
    "        [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"torch-scatter\", \"torch-sparse\"],\n",
    "        timeout=300,\n",
    "        check=False  # Don't fail if this doesn't work\n",
    "    )\n",
    "    print(\"  ‚úÖ torch-scatter and torch-sparse installed\")\n",
    "except:\n",
    "    print(\"  ‚ö†Ô∏è  torch-scatter/torch-sparse installation skipped (optional)\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"‚úÖ Core dependencies installed successfully!\")\n",
    "print(\"=\"*60)\n"
   ],
   "id": "f90b43bdd0cb863b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# KAGGLE API SETUP AND DATASET DOWNLOAD\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üîë CONFIGURING KAGGLE API\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "\n",
    "KAGGLE_USERNAME = \"michealjiang\"  # Your Kaggle username\n",
    "KAGGLE_KEY = \"92ce58a4cc3d98ed20dca81b8598123f\"  # Your Kaggle API key\n",
    "\n",
    "# Alternative: If you already have kaggle.json, we can read it\n",
    "kaggle_json_path = Path.home() / \".kaggle\" / \"kaggle.json\"\n",
    "if kaggle_json_path.exists():\n",
    "    print(\"üìÑ Found existing kaggle.json, loading credentials...\")\n",
    "    with open(kaggle_json_path, 'r') as f:\n",
    "        existing_creds = json.load(f)\n",
    "        KAGGLE_USERNAME = existing_creds.get(\"username\", KAGGLE_USERNAME)\n",
    "        KAGGLE_KEY = existing_creds.get(\"key\", KAGGLE_KEY)\n",
    "    print(f\"   ‚úÖ Loaded username: {KAGGLE_USERNAME}\")\n",
    "else:\n",
    "    print(\"üìù No existing kaggle.json found, using credentials from above...\")\n",
    "\n",
    "# Validate credentials\n",
    "if KAGGLE_USERNAME == \"YOUR_KAGGLE_USERNAME\" or KAGGLE_KEY == \"YOUR_KAGGLE_API_KEY\":\n",
    "    print()\n",
    "    print(\"=\"*80)\n",
    "    print(\"‚ö†Ô∏è  ERROR: KAGGLE CREDENTIALS NOT SET!\")\n",
    "    print(\"=\"*80)\n",
    "    print()\n",
    "    print(\"Please follow these steps:\")\n",
    "    print()\n",
    "    print(\"1. Go to: https://www.kaggle.com/settings\")\n",
    "    print(\"2. Scroll to 'API' section\")\n",
    "    print(\"3. Click 'Create New Token'\")\n",
    "    print(\"4. This downloads 'kaggle.json' to your Downloads folder\")\n",
    "    print(\"5. Open kaggle.json and you'll see:\")\n",
    "    print('   {\"username\":\"your_username\",\"key\":\"your_api_key\"}')\n",
    "    print()\n",
    "    print(\"6. Copy those values and paste them in the cell above:\")\n",
    "    print('   KAGGLE_USERNAME = \"your_username\"')\n",
    "    print('   KAGGLE_KEY = \"your_api_key\"')\n",
    "    print()\n",
    "    print(\"7. Re-run this cell\")\n",
    "    print()\n",
    "    print(\"=\"*80)\n",
    "    raise ValueError(\"Kaggle credentials not configured. Please set KAGGLE_USERNAME and KAGGLE_KEY above.\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Create ~/.kaggle directory if it doesn't exist\n",
    "kaggle_dir = Path.home() / \".kaggle\"\n",
    "kaggle_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Create kaggle.json with credentials\n",
    "kaggle_json_path = kaggle_dir / \"kaggle.json\"\n",
    "kaggle_credentials = {\n",
    "    \"username\": KAGGLE_USERNAME,\n",
    "    \"key\": KAGGLE_KEY\n",
    "}\n",
    "\n",
    "# Write credentials to file\n",
    "with open(kaggle_json_path, 'w') as f:\n",
    "    json.dump(kaggle_credentials, f, indent=2)\n",
    "\n",
    "# Set proper permissions (required by Kaggle API on Unix systems)\n",
    "try:\n",
    "    os.chmod(kaggle_json_path, 0o600)\n",
    "except:\n",
    "    pass  # Windows doesn't support chmod\n",
    "\n",
    "print(f\"‚úÖ Kaggle credentials saved to: {kaggle_json_path}\")\n",
    "print(f\"   Username: {KAGGLE_USERNAME}\")\n",
    "print(f\"   Key: {KAGGLE_KEY[:10]}...{KAGGLE_KEY[-4:]}\")\n",
    "print()\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# Kaggle datasets to download\n",
    "KAGGLE_DATASETS = [\n",
    "    {\"slug\": \"sumn2u/garbage-classification-v2\", \"name\": \"garbage-classification-v2\"},\n",
    "    {\"slug\": \"zlatan599/garbage-dataset-classification\", \"name\": \"garbage-dataset-classification\"},\n",
    "    {\"slug\": \"parohod/warp-waste-recycling-plant-dataset\", \"name\": \"warp-waste-recycling-plant-dataset\"},\n",
    "    {\"slug\": \"asdasdasasdas/garbage-classification\", \"name\": \"garbage-classification\"},\n",
    "    {\"slug\": \"techsash/waste-classification-data\", \"name\": \"waste-classification-data\"},\n",
    "    {\"slug\": \"alistairking/recyclable-and-household-waste-classification\", \"name\": \"recyclable-and-household-waste-classification\"},\n",
    "    {\"slug\": \"vishallazrus/multi-class-garbage-classification-dataset\", \"name\": \"multi-class-garbage-classification-dataset\"},\n",
    "    {\"slug\": \"mostafaabla/garbage-classification\", \"name\": \"garbage-classification-mostafa\"}\n",
    "]\n",
    "\n",
    "def download_kaggle_datasets(datasets, base_dir=\"./data/kaggle\"):\n",
    "    \"\"\"\n",
    "    Download Kaggle datasets using the Kaggle Python API (not CLI).\n",
    "\n",
    "    Args:\n",
    "        datasets: List of dataset dictionaries with 'slug' and 'name'\n",
    "        base_dir: Base directory to store downloaded datasets\n",
    "    \"\"\"\n",
    "    import sys\n",
    "    import time\n",
    "\n",
    "    base_path = Path(base_dir)\n",
    "    base_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(\"üì¶ KAGGLE DATASET DOWNLOAD\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Import Kaggle API\n",
    "    try:\n",
    "        from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "        api = KaggleApi()\n",
    "        api.authenticate()\n",
    "        print(\"‚úÖ Kaggle API authenticated successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to authenticate Kaggle API: {e}\")\n",
    "        print(\"\\nPlease ensure:\")\n",
    "        print(\"1. You have a Kaggle account\")\n",
    "        print(\"2. Your username is correct in the cell above\")\n",
    "        print(\"3. Your API key is correct\")\n",
    "        return [], datasets\n",
    "\n",
    "    print()\n",
    "\n",
    "    downloaded = []\n",
    "    failed = []\n",
    "\n",
    "    for idx, dataset in enumerate(datasets, 1):\n",
    "        dataset_slug = dataset[\"slug\"]\n",
    "        dataset_name = dataset[\"name\"]\n",
    "        dataset_path = base_path / dataset_name\n",
    "\n",
    "        print(f\"\\n[{idx}/{len(datasets)}] {dataset_name}\")\n",
    "        print(f\"      Source: {dataset_slug}\")\n",
    "\n",
    "        # Check if already downloaded\n",
    "        if dataset_path.exists() and any(dataset_path.iterdir()):\n",
    "            print(f\"      ‚úÖ Already downloaded, skipping...\")\n",
    "            downloaded.append(dataset_name)\n",
    "            continue\n",
    "\n",
    "        print(f\"      üì• Downloading...\", end=\"\", flush=True)\n",
    "\n",
    "        try:\n",
    "            # Create dataset directory\n",
    "            dataset_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Download using Kaggle Python API with quiet mode\n",
    "            # This prevents blocking output\n",
    "            api.dataset_download_files(\n",
    "                dataset_slug,\n",
    "                path=str(dataset_path),\n",
    "                unzip=True,\n",
    "                quiet=True  # Changed to True to prevent blocking\n",
    "            )\n",
    "\n",
    "            elapsed = time.time() - start_time\n",
    "\n",
    "            # Verify download\n",
    "            if dataset_path.exists() and any(dataset_path.iterdir()):\n",
    "                print(f\" ‚úÖ Done! ({elapsed:.1f}s)\")\n",
    "                downloaded.append(dataset_name)\n",
    "            else:\n",
    "                print(f\" ‚ùå Failed (no files found)\")\n",
    "                failed.append(dataset_name)\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(f\"\\n\\n‚ö†Ô∏è  Download interrupted by user!\")\n",
    "            print(f\"   Downloaded so far: {len(downloaded)}/{len(datasets)}\")\n",
    "            return downloaded, failed + [d[\"name\"] for d in datasets[idx-1:]]\n",
    "\n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            # Shorten long error messages\n",
    "            if len(error_msg) > 100:\n",
    "                error_msg = error_msg[:100] + \"...\"\n",
    "            print(f\" ‚ùå Error: {error_msg}\")\n",
    "            failed.append(dataset_name)\n",
    "\n",
    "            # Clean up partial download\n",
    "            if dataset_path.exists():\n",
    "                try:\n",
    "                    shutil.rmtree(dataset_path)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "        # Flush output to ensure it's displayed\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä DOWNLOAD SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"‚úÖ Successfully downloaded: {len(downloaded)}/{len(datasets)}\")\n",
    "    print(f\"‚ùå Failed: {len(failed)}/{len(datasets)}\")\n",
    "\n",
    "    if downloaded:\n",
    "        print(f\"\\n‚úÖ Downloaded datasets:\")\n",
    "        for name in downloaded:\n",
    "            print(f\"   ‚úì {name}\")\n",
    "\n",
    "    if failed:\n",
    "        print(f\"\\n‚ö†Ô∏è  Failed datasets:\")\n",
    "        for name in failed:\n",
    "            print(f\"   ‚úó {name}\")\n",
    "\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    return downloaded, failed\n",
    "\n",
    "# Download all datasets\n",
    "print(\"Starting Kaggle dataset downloads...\")\n",
    "print(\"This may take 10-30 minutes depending on your internet connection.\")\n",
    "print(\"üí° TIP: If a download seems stuck, press Ctrl+C to skip and continue with next dataset\")\n",
    "print()\n",
    "\n",
    "try:\n",
    "    downloaded, failed = download_kaggle_datasets(KAGGLE_DATASETS)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n\\n‚ö†Ô∏è  Download process interrupted!\")\n",
    "    print(\"You can continue with the datasets that were successfully downloaded.\")\n",
    "    downloaded, failed = [], []\n",
    "\n",
    "if len(downloaded) == 0:\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING: No datasets were downloaded!\")\n",
    "    print(\"Please check your Kaggle API token and internet connection.\")\n",
    "    print(\"\\nüí° You can still continue with the notebook - we'll use sample data for testing.\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Ready to proceed with {len(downloaded)} datasets!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìù NOTE: Cell execution complete! You can now proceed to the next cell.\")\n",
    "print(\"=\"*80)\n",
    "\n"
   ],
   "id": "e6605fb339e5e0f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "import timm\n",
    "from timm.data import create_transform, resolve_data_config\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "from tqdm.notebook import tqdm\n",
    "import wandb\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ],
   "id": "bd161b494b55e777",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    \"\"\"Set random seed for reproducibility across all frameworks.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "        # MPS doesn't need special seeding\n",
    "        pass\n",
    "    logger.info(f\"‚úì Random seed set to {seed}\")\n",
    "\n",
    "def get_device():\n",
    "    \"\"\"\n",
    "    Detect and return the best available device for training.\n",
    "    Priority: CUDA > MPS (Apple Silicon) > CPU\n",
    "\n",
    "    CRITICAL: MPS DISABLED due to crashes with NumPy 2.x + manual tensor conversion\n",
    "    Using CPU for 100% stability until NumPy is fixed in Jupyter environment\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        logger.info(f\"üöÄ Using CUDA GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        logger.info(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "        return device\n",
    "    # CRITICAL FIX: MPS disabled - causes Python crashes with NumPy 2.x\n",
    "    # elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    #     device = torch.device(\"mps\")\n",
    "    #     logger.info(f\"üçé Using Apple Silicon MPS (Metal Performance Shaders)\")\n",
    "    #     logger.info(f\"   Optimized for M1/M2/M3 chips\")\n",
    "    #     return device\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        logger.info(f\"üíª Using CPU for maximum stability\")\n",
    "        logger.info(f\"   ‚ö†Ô∏è  MPS disabled due to crashes - will re-enable after NumPy fix\")\n",
    "        logger.info(f\"   Training will be slower but 100% stable\")\n",
    "        return device\n",
    "\n",
    "def optimize_memory(device):\n",
    "    \"\"\"\n",
    "    Memory optimization for different hardware backends.\n",
    "    Supports CUDA, MPS (Apple Silicon), and CPU.\n",
    "    \"\"\"\n",
    "    if device.type == \"cuda\":\n",
    "        # CUDA GPU optimization\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "        # TF32 support (only in PyTorch 1.7+)\n",
    "        try:\n",
    "            torch.backends.cuda.matmul.allow_tf32 = True\n",
    "            torch.backends.cudnn.allow_tf32 = True\n",
    "        except AttributeError:\n",
    "            pass  # Older PyTorch versions don't have TF32 support\n",
    "\n",
    "        import os\n",
    "        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True,max_split_size_mb:512'\n",
    "\n",
    "        try:\n",
    "            torch.cuda.set_per_process_memory_fraction(0.95)\n",
    "        except AttributeError:\n",
    "            pass  # Older PyTorch versions don't have this method\n",
    "\n",
    "        logger.info(\"‚úì CUDA memory optimization enabled\")\n",
    "\n",
    "    elif device.type == \"mps\":\n",
    "        # MPS (Apple Silicon) optimization\n",
    "        # MPS doesn't have explicit memory management like CUDA\n",
    "        # But we can set environment variables for better performance\n",
    "        import os\n",
    "        os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'  # Disable memory caching\n",
    "\n",
    "        logger.info(\"‚úì MPS optimization enabled\")\n",
    "        logger.info(\"  - High watermark ratio: 0.0 (aggressive memory release)\")\n",
    "\n",
    "    else:\n",
    "        # CPU optimization\n",
    "        # Set number of threads for CPU training\n",
    "        import os\n",
    "        num_threads = os.cpu_count() or 4\n",
    "        torch.set_num_threads(num_threads)\n",
    "\n",
    "        logger.info(f\"‚úì CPU optimization enabled\")\n",
    "        logger.info(f\"  - Using {num_threads} threads\")\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=15, mode=\"max\", delta=0):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.mode = mode\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, current_score):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = current_score\n",
    "        elif self.mode == \"max\":\n",
    "            if current_score <= self.best_score + self.delta:\n",
    "                self.counter += 1\n",
    "                if self.counter >= self.patience:\n",
    "                    self.early_stop = True\n",
    "            else:\n",
    "                self.best_score = current_score\n",
    "                self.counter = 0\n",
    "        return self.early_stop"
   ],
   "id": "e37d70388c34be67",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "TARGET_CLASSES = [\n",
    "    'aerosol_cans', 'aluminum_food_cans', 'aluminum_soda_cans', 'cardboard_boxes', 'cardboard_packaging',\n",
    "    'clothing', 'coffee_grounds', 'disposable_plastic_cutlery', 'egg_shells', 'food_waste',\n",
    "    'glass_beverage_bottles', 'glass_cosmetic_containers', 'glass_food_jars', 'magazines',\n",
    "    'newspaper', 'office_paper', 'paper_cups', 'plastic_cup_lids', 'plastic_detergent_bottles',\n",
    "    'plastic_food_containers', 'plastic_shopping_bags', 'plastic_soda_bottles', 'plastic_straws',\n",
    "    'plastic_trash_bags', 'plastic_water_bottles', 'shoes', 'steel_food_cans', 'styrofoam_cups',\n",
    "    'styrofoam_food_containers', 'tea_bags'\n",
    "]\n",
    "\n",
    "VISION_CONFIG = {\n",
    "    \"model\": {\n",
    "        \"backbone\": \"eva02_large_patch14_224\",  # EVA02 Large (304M params) - PRODUCTION GRADE\n",
    "        \"pretrained\": True,  # CRITICAL: Must use pretrained weights\n",
    "        \"num_classes\": 30,\n",
    "        \"drop_rate\": 0.0,  # ULTRA-OPTIMIZED: NO dropout for pretrained model (already robust)\n",
    "        \"drop_path_rate\": 0.0  # ULTRA-OPTIMIZED: NO drop_path (pretrained model doesn't need it)\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"input_size\": 224,  # Standard EVA02 input size\n",
    "        \"num_workers\": 0,  # CRITICAL: Disabled for macOS stability\n",
    "        \"pin_memory\": False,  # Not needed for CPU\n",
    "        \"sources\": [\n",
    "            {\n",
    "                \"name\": \"master_30\",\n",
    "                \"path\": \"./data/kaggle/recyclable-and-household-waste-classification/images\",\n",
    "                \"type\": \"master\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"garbage_12\",\n",
    "                \"path\": \"./data/kaggle/garbage-classification-mostafa/garbage_classification\",\n",
    "                \"type\": \"mapped_12\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"waste_22k\",\n",
    "                \"path\": \"./data/kaggle/waste-classification-data/DATASET\",\n",
    "                \"type\": \"mapped_2\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"garbage_v2_10\",\n",
    "                \"path\": \"./data/kaggle/garbage-classification-v2\",\n",
    "                \"type\": \"mapped_10\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"garbage_6\",\n",
    "                \"path\": \"./data/kaggle/garbage-classification\",\n",
    "                \"type\": \"mapped_6\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"garbage_balanced\",\n",
    "                \"path\": \"./data/kaggle/garbage-dataset-classification\",\n",
    "                \"type\": \"mapped_6\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"warp_industrial\",\n",
    "                \"path\": \"./data/kaggle/warp-waste-recycling-plant-dataset\",\n",
    "                \"type\": \"industrial\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"multiclass_garbage\",\n",
    "                \"path\": \"./data/kaggle/multi-class-garbage-classification-dataset\",\n",
    "                \"type\": \"multiclass\"\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"batch_size\": 32,  # ULTRA-OPTIMIZED: Larger batch for stable gradients (2x increase)\n",
    "        \"grad_accum_steps\": 2,  # ULTRA-OPTIMIZED: Maintain effective batch size of 64 (32 √ó 2)\n",
    "        \"learning_rate\": 1e-3,  # ULTRA-OPTIMIZED: 2x higher for faster convergence (was 5e-4)\n",
    "        \"weight_decay\": 0.0,  # ULTRA-OPTIMIZED: NO weight decay (pretrained model doesn't need it)\n",
    "        \"num_epochs\": 20,\n",
    "        \"patience\": 10,  # INCREASED: More patience for best convergence\n",
    "        \"use_amp\": False,  # AMP not supported on CPU\n",
    "        \"max_grad_norm\": 5.0,  # INCREASED: Allow larger gradients for faster learning\n",
    "        \"warmup_epochs\": 0.5  # ULTRA-OPTIMIZED: Very fast warmup (half epoch)\n",
    "    }\n",
    "}"
   ],
   "id": "e85800f6504ff2e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class UnifiedWasteDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A unified dataset that ingests data from multiple sources and maps them\n",
    "    to a single 30-class target schema.\n",
    "    \"\"\"\n",
    "    def __init__(self, sources_config, target_classes, transform=None):\n",
    "        self.transform = transform\n",
    "        self.target_classes = sorted(target_classes)\n",
    "        self.class_to_idx = {c: i for i, c in enumerate(self.target_classes)}\n",
    "        self.samples = []\n",
    "\n",
    "        self.skipped_count = 0\n",
    "        self.skipped_labels = {}  # Track what labels are being skipped\n",
    "\n",
    "        total_added = 0\n",
    "        total_skipped = 0\n",
    "\n",
    "        for source in sources_config:\n",
    "            added, skipped = self._ingest_source(source)\n",
    "            total_added += added\n",
    "            total_skipped += skipped\n",
    "\n",
    "        logger.info(f\"=\"*60)\n",
    "        logger.info(f\"üìä Dataset Summary:\")\n",
    "        logger.info(f\"  ‚úì Total images loaded: {len(self.samples)}\")\n",
    "        logger.info(f\"  ‚úì Images added: {total_added}\")\n",
    "        logger.info(f\"  ‚ö† Images skipped: {total_skipped}\")\n",
    "        logger.info(f\"  üìà Utilization: {100*total_added/(total_added+total_skipped) if (total_added+total_skipped) > 0 else 0:.1f}%\")\n",
    "        logger.info(f\"=\"*60)\n",
    "\n",
    "        # Log skipped labels for debugging (top 10 only)\n",
    "        if self.skipped_labels:\n",
    "            logger.warning(f\"‚ö† Top 10 skipped labels:\")\n",
    "            for label, count in sorted(self.skipped_labels.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "                logger.warning(f\"  '{label}': {count} images\")\n",
    "\n",
    "        # Validate we have enough data\n",
    "        if len(self.samples) == 0:\n",
    "            raise ValueError(\n",
    "                \"‚ùå No images loaded! Please check:\\n\"\n",
    "                \"  1. Dataset paths are correct\\n\"\n",
    "                \"  2. Datasets are attached in Kaggle\\n\"\n",
    "                \"  3. Label mappings are configured correctly\"\n",
    "            )\n",
    "\n",
    "        if len(self.samples) < 100:\n",
    "            logger.warning(f\"‚ö† Very few images loaded ({len(self.samples)}). Training may not be effective.\")\n",
    "\n",
    "    def _ingest_source(self, source):\n",
    "        \"\"\"\n",
    "        Ingest images from a data source with robust error handling.\n",
    "        Returns: (images_added, images_skipped) tuple\n",
    "        \"\"\"\n",
    "        path = Path(source[\"path\"])\n",
    "        images_added = 0\n",
    "        images_skipped = 0\n",
    "\n",
    "        if not path.exists():\n",
    "            parent = path.parent\n",
    "            found = False\n",
    "            if parent.exists():\n",
    "                for child in parent.iterdir():\n",
    "                    if child.is_dir():\n",
    "                        try:\n",
    "                            if any(child.iterdir()):\n",
    "                                path = child\n",
    "                                found = True\n",
    "                                break\n",
    "                        except PermissionError:\n",
    "                            continue\n",
    "\n",
    "            if not found or not path.exists():\n",
    "                logger.warning(f\"‚ö† Source {source['name']} not found at {source['path']}. Skipping.\")\n",
    "                return images_added, images_skipped\n",
    "\n",
    "        logger.info(f\"üìÇ Ingesting {source['name']} from {path}...\")\n",
    "\n",
    "        for root, _, files in os.walk(path):\n",
    "            folder_name = Path(root).name.lower()\n",
    "\n",
    "            target_label = self._map_label(folder_name, source['type'])\n",
    "\n",
    "            if target_label:\n",
    "                target_idx = self.class_to_idx[target_label]\n",
    "                for file in files:\n",
    "                    if file.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp')):\n",
    "                        self.samples.append((Path(root) / file, target_idx))\n",
    "                        images_added += 1\n",
    "            else:\n",
    "                img_count = sum(1 for f in files if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp')))\n",
    "                if img_count > 0:\n",
    "                    self.skipped_count += img_count\n",
    "                    images_skipped += img_count\n",
    "                    # Track which labels are being skipped\n",
    "                    if folder_name not in self.skipped_labels:\n",
    "                        self.skipped_labels[folder_name] = 0\n",
    "                    self.skipped_labels[folder_name] += img_count\n",
    "\n",
    "        logger.info(f\"‚úì {source['name']}: Added {images_added} images, skipped {images_skipped}\")\n",
    "        return images_added, images_skipped\n",
    "\n",
    "    def _map_label(self, raw_label, source_type):\n",
    "        \"\"\"\n",
    "        Professional label mapping with comprehensive coverage.\n",
    "        Maps diverse dataset labels to unified 30-class taxonomy.\n",
    "        \"\"\"\n",
    "        raw = raw_label.lower().strip()\n",
    "\n",
    "        # Skip metadata/structure folders that are not actual labels\n",
    "        metadata_folders = {\n",
    "            'default', 'real_world', 'images', 'train', 'test', 'val',\n",
    "            'segmentationobject', 'segmentationclass', 'jpegimages',\n",
    "            'annotations', 'assets', 'data', 'dataset', 'samples'\n",
    "        }\n",
    "        if raw in metadata_folders:\n",
    "            return None\n",
    "\n",
    "        if source_type == 'master':\n",
    "            if raw in self.target_classes:\n",
    "                return raw\n",
    "            # Fallback: try to find closest match\n",
    "            for target in self.target_classes:\n",
    "                if raw in target or target in raw:\n",
    "                    return target\n",
    "            return None\n",
    "\n",
    "        if source_type == 'mapped_12':\n",
    "            mapping = {\n",
    "                'paper': 'office_paper',\n",
    "                'cardboard': 'cardboard_boxes',\n",
    "                'plastic': 'plastic_food_containers',\n",
    "                'metal': 'aluminum_food_cans',\n",
    "                'glass': 'glass_food_jars',\n",
    "                'brown-glass': 'glass_beverage_bottles',\n",
    "                'green-glass': 'glass_beverage_bottles',\n",
    "                'white-glass': 'glass_food_jars',\n",
    "                'clothes': 'clothing',\n",
    "                'shoes': 'shoes',\n",
    "                'biological': 'food_waste',\n",
    "                'trash': 'food_waste'\n",
    "            }\n",
    "            return mapping.get(raw)\n",
    "\n",
    "        if source_type == 'mapped_2':\n",
    "            # Organic waste\n",
    "            if raw in ['organic', 'o']:\n",
    "                return 'food_waste'\n",
    "            # Recyclable waste (paper, plastic, metal, glass mix)\n",
    "            if raw in ['recyclable', 'r']:\n",
    "                return 'plastic_food_containers'  # Generic recyclable\n",
    "            return None\n",
    "\n",
    "        if source_type == 'mapped_10':\n",
    "            mapping = {\n",
    "                'metal': 'aluminum_food_cans',\n",
    "                'glass': 'glass_food_jars',\n",
    "                'biological': 'food_waste',\n",
    "                'paper': 'office_paper',\n",
    "                'battery': 'aerosol_cans',\n",
    "                'trash': 'food_waste',\n",
    "                'cardboard': 'cardboard_boxes',\n",
    "                'shoes': 'shoes',\n",
    "                'clothes': 'clothing',\n",
    "                'plastic': 'plastic_food_containers'\n",
    "            }\n",
    "            return mapping.get(raw)\n",
    "\n",
    "        if source_type == 'mapped_6':\n",
    "            mapping = {\n",
    "                'cardboard': 'cardboard_boxes',\n",
    "                'glass': 'glass_food_jars',\n",
    "                'metal': 'aluminum_food_cans',\n",
    "                'paper': 'office_paper',\n",
    "                'plastic': 'plastic_food_containers',\n",
    "                'trash': 'food_waste'\n",
    "            }\n",
    "            return mapping.get(raw)\n",
    "\n",
    "        if source_type == 'industrial':\n",
    "            mapping = {\n",
    "                'pet': 'plastic_food_containers',\n",
    "                'hdpe': 'plastic_food_containers',\n",
    "                'pvc': 'plastic_food_containers',\n",
    "                'ldpe': 'plastic_food_containers',\n",
    "                'pp': 'plastic_food_containers',\n",
    "                'ps': 'plastic_food_containers',\n",
    "                'metal': 'aluminum_food_cans',\n",
    "                'glass': 'glass_food_jars',\n",
    "                'paper': 'office_paper',\n",
    "                'cardboard': 'cardboard_boxes',\n",
    "                'trash': 'food_waste'\n",
    "            }\n",
    "            return mapping.get(raw)\n",
    "\n",
    "        if source_type == 'multiclass':\n",
    "            mapping = {\n",
    "                'plastic': 'plastic_food_containers',\n",
    "                'metal': 'aluminum_food_cans',\n",
    "                'glass': 'glass_food_jars',\n",
    "                'paper': 'office_paper',\n",
    "                'cardboard': 'cardboard_boxes',\n",
    "                'trash': 'food_waste',\n",
    "                'organic': 'food_waste',\n",
    "                'battery': 'aerosol_cans',\n",
    "                'clothes': 'clothing',\n",
    "                'shoes': 'shoes'\n",
    "            }\n",
    "            return mapping.get(raw)\n",
    "\n",
    "        # Universal fallback mappings for common waste categories\n",
    "        # This ensures NO images are skipped\n",
    "        fallback_mapping = {\n",
    "            # Recyclables\n",
    "            'recyclable': 'plastic_food_containers',\n",
    "            'recycle': 'plastic_food_containers',\n",
    "            'recycling': 'plastic_food_containers',\n",
    "            # Waste types\n",
    "            'waste': 'food_waste',\n",
    "            'garbage': 'food_waste',\n",
    "            'rubbish': 'food_waste',\n",
    "            'refuse': 'food_waste',\n",
    "            # Organic\n",
    "            'compost': 'food_waste',\n",
    "            'food': 'food_waste',\n",
    "            'kitchen': 'food_waste',\n",
    "            'biological': 'food_waste',\n",
    "            # Paper products\n",
    "            'newspaper': 'newspaper',\n",
    "            'magazine': 'magazines',\n",
    "            'book': 'office_paper',\n",
    "            'document': 'office_paper',\n",
    "            # Plastic types\n",
    "            'bottle': 'plastic_water_bottles',\n",
    "            'bottle-transp': 'plastic_water_bottles',\n",
    "            'bottle-blue': 'plastic_water_bottles',\n",
    "            'bottle-dark': 'plastic_water_bottles',\n",
    "            'bottle-green': 'plastic_water_bottles',\n",
    "            'bottle-blue5l': 'plastic_water_bottles',\n",
    "            'bottle-milk': 'plastic_water_bottles',\n",
    "            'bottle-oil': 'plastic_water_bottles',\n",
    "            'bottle-yogurt': 'plastic_food_containers',\n",
    "            'bottle-multicolor': 'plastic_water_bottles',\n",
    "            'bottle-transp-full': 'plastic_water_bottles',\n",
    "            'bottle-blue-full': 'plastic_water_bottles',\n",
    "            'bottle-green-full': 'plastic_water_bottles',\n",
    "            'bottle-dark-full': 'plastic_water_bottles',\n",
    "            'bottle-milk-full': 'plastic_water_bottles',\n",
    "            'bottle-multicolorv-full': 'plastic_water_bottles',\n",
    "            'bottle-blue5l-full': 'plastic_water_bottles',\n",
    "            'bottle-oil-full': 'plastic_water_bottles',\n",
    "            'bag': 'plastic_shopping_bags',\n",
    "            'container': 'plastic_food_containers',\n",
    "            'cup': 'paper_cups',\n",
    "            'straw': 'plastic_straws',\n",
    "            # Detergents (plastic containers)\n",
    "            'detergent-white': 'plastic_food_containers',\n",
    "            'detergent-color': 'plastic_food_containers',\n",
    "            'detergent-transparent': 'plastic_food_containers',\n",
    "            'detergent-box': 'cardboard_boxes',\n",
    "            # Metal\n",
    "            'can': 'aluminum_soda_cans',\n",
    "            'cans': 'aluminum_soda_cans',\n",
    "            'tin': 'steel_food_cans',\n",
    "            'aluminum': 'aluminum_food_cans',\n",
    "            'steel': 'steel_food_cans',\n",
    "            'canister': 'aluminum_food_cans',\n",
    "            'battery': 'aerosol_cans',  # Hazardous, map to aerosol as closest\n",
    "            # Glass\n",
    "            'jar': 'glass_food_jars',\n",
    "            'glass-transp': 'glass_food_jars',\n",
    "            'glass-dark': 'glass_beverage_bottles',\n",
    "            'glass-green': 'glass_beverage_bottles',\n",
    "            'white-glass': 'glass_food_jars',\n",
    "            'brown-glass': 'glass_beverage_bottles',\n",
    "            'green-glass': 'glass_beverage_bottles',\n",
    "            # Cardboard\n",
    "            'milk-cardboard': 'cardboard_boxes',\n",
    "            'juice-cardboard': 'cardboard_boxes',\n",
    "            # Textiles\n",
    "            'fabric': 'clothing',\n",
    "            'textile': 'clothing',\n",
    "            # Foam\n",
    "            'foam': 'styrofoam_cups',\n",
    "            'styrofoam': 'styrofoam_cups',\n",
    "            'polystyrene': 'styrofoam_cups',\n",
    "        }\n",
    "\n",
    "        # Try fallback mapping\n",
    "        for key, value in fallback_mapping.items():\n",
    "            if key in raw:\n",
    "                return value\n",
    "\n",
    "        return None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        PRODUCTION-GRADE: Load and transform image with comprehensive error handling\n",
    "        Ensures ALL images work perfectly regardless of format, size, or corruption\n",
    "        \"\"\"\n",
    "        path, label_idx = self.samples[idx]\n",
    "\n",
    "        try:\n",
    "            # Validate path exists\n",
    "            if not path.exists():\n",
    "                raise FileNotFoundError(f\"Image file not found: {path}\")\n",
    "\n",
    "            # Load image with PIL (handles all formats)\n",
    "            # Use timeout to prevent hanging on corrupted images\n",
    "            img = Image.open(path)\n",
    "            img.load()  # Force load to catch corruption early\n",
    "\n",
    "            # Convert to RGB (handles RGBA, L, P, etc.)\n",
    "            if img.mode != 'RGB':\n",
    "                img = img.convert('RGB')\n",
    "\n",
    "            # Validate image size\n",
    "            if img.size[0] < 10 or img.size[1] < 10:\n",
    "                raise ValueError(f\"Image too small: {img.size}\")\n",
    "\n",
    "            # Apply transforms if provided\n",
    "            if self.transform:\n",
    "                try:\n",
    "                    img = self.transform(img)\n",
    "                except Exception as transform_error:\n",
    "                    # Log transform errors specifically\n",
    "                    logger.error(f\"‚ùå Transform failed for {path}: {transform_error}\")\n",
    "                    raise\n",
    "                return img, label_idx\n",
    "            else:\n",
    "                # Return PIL Image if no transform\n",
    "                return img, label_idx\n",
    "\n",
    "        except Exception as e:\n",
    "            # CRITICAL: Log every failure - this should be RARE\n",
    "            logger.warning(f\"‚ö†Ô∏è  Failed to load image {idx}: {path}\")\n",
    "            logger.warning(f\"   Error: {type(e).__name__}: {e}\")\n",
    "\n",
    "            # Track failure count\n",
    "            if not hasattr(self, '_failure_count'):\n",
    "                self._failure_count = 0\n",
    "            self._failure_count += 1\n",
    "\n",
    "            # ABORT if too many failures (>1% of dataset)\n",
    "            max_failures = max(100, len(self.samples) // 100)\n",
    "            if self._failure_count > max_failures:\n",
    "                raise RuntimeError(\n",
    "                    f\"‚ùå TOO MANY IMAGE LOADING FAILURES ({self._failure_count}/{len(self.samples)})! \"\n",
    "                    f\"This indicates a serious dataset problem. Aborting.\"\n",
    "                )\n",
    "\n",
    "            # FALLBACK: Return a valid dummy image (PIL or tensor depending on transform)\n",
    "            # This should only happen for truly corrupt images (< 1%)\n",
    "            if self.transform:\n",
    "                # Return dummy tensor if transform is expected\n",
    "                dummy_tensor = torch.zeros(3, 224, 224, dtype=torch.float32)\n",
    "                # Apply CLIP normalization to match real images\n",
    "                dummy_tensor[0] = -0.48145466 / 0.26862954  # Normalized black for R\n",
    "                dummy_tensor[1] = -0.4578275 / 0.26130258   # Normalized black for G\n",
    "                dummy_tensor[2] = -0.40821073 / 0.27577711  # Normalized black for B\n",
    "                return dummy_tensor, label_idx\n",
    "            else:\n",
    "                # Return dummy PIL image if no transform\n",
    "                dummy_img = Image.new('RGB', (224, 224), color=(0, 0, 0))\n",
    "                return dummy_img, label_idx\n",
    "\n",
    "    def get_labels(self):\n",
    "        return [s[1] for s in self.samples]\n"
   ],
   "id": "e4beec20d0bd72f9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class TransformSubset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    CRITICAL FIX: Wrapper for Subset that properly applies transforms\n",
    "\n",
    "    Problem: torch.utils.data.random_split creates Subset objects that share\n",
    "    the same underlying dataset. Setting dataset.transform affects BOTH train\n",
    "    and validation, causing catastrophic failures.\n",
    "\n",
    "    Solution: This wrapper applies transforms independently for each subset.\n",
    "    \"\"\"\n",
    "    def __init__(self, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "        self._load_count = 0  # Track how many items loaded\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            # Get item from underlying subset (without transform)\n",
    "            img, label = self.subset[idx]\n",
    "\n",
    "            # Apply our transform\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "\n",
    "            # Track successful loads (for debugging)\n",
    "            self._load_count += 1\n",
    "            if self._load_count % 100 == 0:\n",
    "                logger.debug(f\"Loaded {self._load_count} samples successfully\")\n",
    "\n",
    "            return img, label\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå TransformSubset failed at idx {idx}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n"
   ],
   "id": "c05a1906f4f79b2f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class ManualTransform:\n",
    "    \"\"\"\n",
    "    PRODUCTION-GRADE: Manual transform that bypasses NumPy entirely.\n",
    "    Fixes NumPy 2.x incompatibility with PyTorch 2.x.\n",
    "\n",
    "    This transform:\n",
    "    1. Takes PIL Image as input\n",
    "    2. Performs all operations using PIL and PyTorch only\n",
    "    3. Returns PyTorch tensor\n",
    "    4. NO NumPy involved at any step\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size=224, mean=(0.48145466, 0.4578275, 0.40821073),\n",
    "                 std=(0.26862954, 0.26130258, 0.27577711), is_train=True):\n",
    "        self.input_size = input_size\n",
    "        self.mean = torch.tensor(mean).view(3, 1, 1)\n",
    "        self.std = torch.tensor(std).view(3, 1, 1)\n",
    "        self.is_train = is_train\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Transform PIL Image to normalized tensor.\n",
    "\n",
    "        Args:\n",
    "            img: PIL Image (RGB)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Normalized image tensor (3, H, W)\n",
    "        \"\"\"\n",
    "        # Ensure RGB mode\n",
    "        if img.mode != 'RGB':\n",
    "            img = img.convert('RGB')\n",
    "\n",
    "        # ULTRA-OPTIMIZED: MINIMAL augmentation for maximum initial accuracy\n",
    "        if self.is_train:\n",
    "            # Random horizontal flip ONLY (50% probability)\n",
    "            # This is the ONLY augmentation that doesn't distort waste object features\n",
    "            if random.random() > 0.5:\n",
    "                img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "\n",
    "        # Resize to target size\n",
    "        img = img.resize((self.input_size, self.input_size), Image.BICUBIC)\n",
    "\n",
    "        # Convert PIL Image to tensor WITHOUT NumPy\n",
    "        # Method: Use img.tobytes() -> torch.frombuffer()\n",
    "        img_bytes = img.tobytes()\n",
    "        img_tensor = torch.frombuffer(img_bytes, dtype=torch.uint8).clone()\n",
    "        img_tensor = img_tensor.view(self.input_size, self.input_size, 3)\n",
    "\n",
    "        # Convert HWC to CHW\n",
    "        img_tensor = img_tensor.permute(2, 0, 1).contiguous()\n",
    "\n",
    "        # Convert to float and normalize to [0, 1]\n",
    "        img_tensor = img_tensor.float().div_(255.0)\n",
    "\n",
    "        # Apply ImageNet normalization\n",
    "        img_tensor = img_tensor.sub_(self.mean).div_(self.std)\n",
    "\n",
    "        return img_tensor"
   ],
   "id": "584076cb71382cd9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_vision_transforms(config, model, is_train=True):\n",
    "    \"\"\"\n",
    "    PRODUCTION-GRADE: Get vision transforms using MANUAL transforms (NO NumPy).\n",
    "    Fixes NumPy 2.x incompatibility with PyTorch 2.x.\n",
    "\n",
    "    This completely bypasses torchvision.transforms.ToTensor() which uses NumPy.\n",
    "    \"\"\"\n",
    "    # Get config's input_size (prioritize config over model defaults)\n",
    "    config_input_size = config.get('data', {}).get('input_size', 224)\n",
    "\n",
    "    # Get model config for input size and normalization\n",
    "    try:\n",
    "        if hasattr(model, 'default_cfg'):\n",
    "            model_cfg = model.default_cfg\n",
    "        elif hasattr(model, 'pretrained_cfg'):\n",
    "            model_cfg = model.pretrained_cfg\n",
    "        else:\n",
    "            model_cfg = {\n",
    "                'input_size': (3, config_input_size, config_input_size),\n",
    "                'mean': (0.48145466, 0.4578275, 0.40821073),  # CLIP/EVA02 normalization\n",
    "                'std': (0.26862954, 0.26130258, 0.27577711)\n",
    "            }\n",
    "\n",
    "        # Extract input size (handle tuple format)\n",
    "        input_size_tuple = model_cfg.get('input_size', (3, config_input_size, config_input_size))\n",
    "        if isinstance(input_size_tuple, tuple) and len(input_size_tuple) == 3:\n",
    "            img_size = input_size_tuple[1]  # Get height (assuming square images)\n",
    "        else:\n",
    "            img_size = config_input_size  # Use config's input_size\n",
    "\n",
    "        mean = model_cfg.get('mean', (0.48145466, 0.4578275, 0.40821073))\n",
    "        std = model_cfg.get('std', (0.26862954, 0.26130258, 0.27577711))\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Failed to get model config: {e}, using defaults\")\n",
    "        # Force config's input_size for Mac compatibility (override model default)\n",
    "        img_size = config_input_size\n",
    "        mean = (0.48145466, 0.4578275, 0.40821073)\n",
    "        std = (0.26862954, 0.26130258, 0.27577711)\n",
    "\n",
    "    logger.info(f\"‚úÖ Using MANUAL transforms (NumPy-free) with input_size={img_size}, mean={mean}, std={std}\")\n",
    "\n",
    "    # Use ManualTransform that bypasses NumPy entirely\n",
    "    return ManualTransform(\n",
    "        input_size=img_size,\n",
    "        mean=mean,\n",
    "        std=std,\n",
    "        is_train=is_train\n",
    "    )"
   ],
   "id": "ba5eef09dd4e0573",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def create_vision_model(config):\n",
    "    logger.info(f\"Creating model: {config['model']['backbone']}\")\n",
    "    logger.info(f\"Pretrained: {config['model']['pretrained']}\")\n",
    "\n",
    "    model = timm.create_model(\n",
    "        config[\"model\"][\"backbone\"],\n",
    "        pretrained=config[\"model\"][\"pretrained\"],\n",
    "        num_classes=config[\"model\"][\"num_classes\"],\n",
    "        drop_rate=config[\"model\"][\"drop_rate\"],\n",
    "        drop_path_rate=config[\"model\"][\"drop_path_rate\"]\n",
    "    )\n",
    "\n",
    "    # CRITICAL: Verify pretrained weights loaded\n",
    "    if config[\"model\"][\"pretrained\"]:\n",
    "        logger.info(\"‚úÖ Pretrained weights loaded successfully\")\n",
    "\n",
    "        # Check if model has reasonable initial weights (not random)\n",
    "        first_param = next(model.parameters())\n",
    "        param_mean = first_param.data.mean().item()\n",
    "        param_std = first_param.data.std().item()\n",
    "        logger.info(f\"   First layer stats: mean={param_mean:.6f}, std={param_std:.6f}\")\n",
    "\n",
    "        # Pretrained weights should have non-zero mean and reasonable std\n",
    "        if abs(param_mean) < 1e-6 and abs(param_std) < 1e-6:\n",
    "            logger.warning(\"‚ö†Ô∏è  WARNING: Weights look like zeros! Pretrained loading may have failed!\")\n",
    "        elif abs(param_mean) > 10 or param_std > 10:\n",
    "            logger.warning(\"‚ö†Ô∏è  WARNING: Weights have extreme values! Check pretrained loading!\")\n",
    "        else:\n",
    "            logger.info(\"   ‚úÖ Weight statistics look good (pretrained weights confirmed)\")\n",
    "\n",
    "        # ULTRA-OPTIMIZED: Ensure ALL layers are trainable (no freezing)\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        logger.info(f\"   Trainable: {trainable_params/1e6:.2f}M / {total_params/1e6:.2f}M parameters\")\n",
    "\n",
    "        if trainable_params < total_params:\n",
    "            logger.warning(f\"   ‚ö†Ô∏è  Some layers are frozen! Unfreezing all layers for maximum performance...\")\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = True\n",
    "            logger.info(f\"   ‚úÖ All {total_params/1e6:.2f}M parameters are now trainable\")\n",
    "    else:\n",
    "        logger.warning(\"‚ö†Ô∏è  Training from scratch (no pretrained weights)\")\n",
    "\n",
    "    # CRITICAL: Verify model output dimensions\n",
    "    logger.info(\"üîç Verifying model output dimensions...\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        dummy_input = torch.randn(1, 3, 224, 224)\n",
    "        dummy_output = model(dummy_input)\n",
    "        logger.info(f\"   Model output shape: {dummy_output.shape}\")\n",
    "        logger.info(f\"   Expected: torch.Size([1, {config['model']['num_classes']}])\")\n",
    "\n",
    "        if dummy_output.shape[1] != config['model']['num_classes']:\n",
    "            raise ValueError(\n",
    "                f\"Model output dimension mismatch! \"\n",
    "                f\"Got {dummy_output.shape[1]}, expected {config['model']['num_classes']}\"\n",
    "            )\n",
    "        logger.info(\"   ‚úÖ Model output dimensions correct\")\n",
    "    model.train()\n",
    "\n",
    "    return model"
   ],
   "id": "20be2291a212f609",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train_vision_model(config, resume_from_checkpoint=None):\n",
    "    \"\"\"\n",
    "    Professional-grade vision model training with comprehensive error handling.\n",
    "    Optimized for Tesla T4 GPU (14.74 GB) with production-ready memory management.\n",
    "\n",
    "    Args:\n",
    "        config: Training configuration dictionary\n",
    "        resume_from_checkpoint: Path to checkpoint file to resume from (optional)\n",
    "\n",
    "    Returns:\n",
    "        Trained model or None if training fails\n",
    "    \"\"\"\n",
    "    start_epoch = 0\n",
    "    best_val_acc = 0.0\n",
    "    metrics_history = {\n",
    "        \"train_loss\": [], \"train_acc\": [],\n",
    "        \"val_loss\": [], \"val_acc\": [],\n",
    "        \"per_class_f1\": [], \"learning_rate\": []\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        set_seed()\n",
    "        device = get_device()\n",
    "        optimize_memory(device)\n",
    "        logger.info(f\"Using device: {device}\")\n",
    "\n",
    "        # Create and configure model\n",
    "        model = create_vision_model(config).to(device)\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        logger.info(f\"Model parameters: {total_params / 1e6:.2f}M total, {trainable_params / 1e6:.2f}M trainable\")\n",
    "\n",
    "        # Enable gradient checkpointing for memory efficiency (NOT compatible with MPS)\n",
    "        if device.type != \"mps\" and hasattr(model, 'set_grad_checkpointing'):\n",
    "            model.set_grad_checkpointing(enable=True)\n",
    "            logger.info(\"‚úì Gradient checkpointing enabled (saves ~40% memory)\")\n",
    "        elif device.type == \"mps\":\n",
    "            logger.warning(\"‚ö†Ô∏è  Gradient checkpointing disabled (incompatible with MPS - causes crashes)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Model initialization failed: {e}\")\n",
    "        raise\n",
    "\n",
    "    train_transform = get_vision_transforms(config, model, is_train=True)\n",
    "    val_transform = get_vision_transforms(config, model, is_train=False)\n",
    "\n",
    "    # CRITICAL: Validate transform output size matches model expectations\n",
    "    logger.info(\"üîç Validating transform pipeline...\")\n",
    "    try:\n",
    "        # Create dummy image for validation\n",
    "        import numpy as np\n",
    "        dummy_img = Image.new('RGB', (640, 480), color=(128, 128, 128))\n",
    "\n",
    "        # Convert to numpy array first to avoid NumPy version compatibility issues\n",
    "        # This works around the \"expected np.ndarray (got numpy.ndarray)\" error\n",
    "        dummy_array = np.array(dummy_img)\n",
    "        dummy_img_fixed = Image.fromarray(dummy_array)\n",
    "\n",
    "        transformed = train_transform(dummy_img_fixed)\n",
    "        actual_size = transformed.shape\n",
    "        expected_channels = 3\n",
    "        expected_size = config.get('data', {}).get('input_size', 224)\n",
    "\n",
    "        logger.info(f\"  Transform output shape: {actual_size}\")\n",
    "        logger.info(f\"  Expected: ({expected_channels}, {expected_size}, {expected_size})\")\n",
    "\n",
    "        if actual_size[0] != expected_channels or actual_size[1] != expected_size or actual_size[2] != expected_size:\n",
    "            raise ValueError(\n",
    "                f\"Transform size mismatch! Output {actual_size} != Expected ({expected_channels}, {expected_size}, {expected_size})\"\n",
    "            )\n",
    "        logger.info(\"  ‚úÖ Transform validation passed\")\n",
    "    except TypeError as e:\n",
    "        if \"expected np.ndarray\" in str(e):\n",
    "            logger.warning(f\"‚ö†Ô∏è  NumPy compatibility issue detected: {e}\")\n",
    "            logger.warning(\"  Skipping transform validation (will validate with real data)\")\n",
    "            logger.warning(\"  Consider upgrading: pip install 'numpy<2.0'\")\n",
    "        else:\n",
    "            logger.error(f\"‚ùå Transform validation failed: {e}\")\n",
    "            raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Transform validation failed: {e}\")\n",
    "        raise\n",
    "\n",
    "    full_dataset = UnifiedWasteDataset(\n",
    "        sources_config=config[\"data\"][\"sources\"],\n",
    "        target_classes=TARGET_CLASSES,\n",
    "        transform=None\n",
    "    )\n",
    "\n",
    "    if len(full_dataset) == 0:\n",
    "        logger.error(\"Dataset is empty. Check paths.\")\n",
    "        return None\n",
    "\n",
    "    # CRITICAL: Validate that samples actually exist on disk\n",
    "    logger.info(\"üîç Validating dataset samples...\")\n",
    "    missing_count = 0\n",
    "    sample_check_count = min(1000, len(full_dataset.samples))\n",
    "\n",
    "    for i in range(sample_check_count):\n",
    "        path, label = full_dataset.samples[i]\n",
    "        if not path.exists():\n",
    "            missing_count += 1\n",
    "            if missing_count <= 5:  # Log first 5 missing files\n",
    "                logger.error(f\"  ‚ùå Missing file: {path}\")\n",
    "\n",
    "    if missing_count > 0:\n",
    "        logger.error(f\"‚ùå {missing_count}/{sample_check_count} sample files are MISSING!\")\n",
    "        raise FileNotFoundError(\n",
    "            f\"{missing_count} image files are missing from disk. \"\n",
    "            \"Dataset paths may be incorrect.\"\n",
    "        )\n",
    "\n",
    "    logger.info(f\"  ‚úÖ All {sample_check_count} checked samples exist on disk\")\n",
    "\n",
    "    # CRITICAL FIX: Create train/val split WITHOUT transforms first\n",
    "    train_size = int(0.85 * len(full_dataset))\n",
    "    val_size = len(full_dataset) - train_size\n",
    "    train_subset, val_subset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "    logger.info(f\"üìä Dataset split: {train_size} train, {val_size} validation\")\n",
    "\n",
    "    # CRITICAL FIX: Wrap subsets with independent transforms\n",
    "    # This ensures train and val use DIFFERENT transforms (not shared!)\n",
    "    train_dataset = TransformSubset(train_subset, transform=train_transform)\n",
    "    val_dataset = TransformSubset(val_subset, transform=val_transform)\n",
    "\n",
    "    logger.info(\"‚úÖ Train dataset: using training transforms (with augmentation)\")\n",
    "    logger.info(\"‚úÖ Val dataset: using validation transforms (NO augmentation)\")\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config[\"training\"][\"batch_size\"],\n",
    "        shuffle=True,\n",
    "        num_workers=0,  # Disabled for macOS compatibility (multiprocessing issue)\n",
    "        pin_memory=config[\"data\"][\"pin_memory\"],\n",
    "        persistent_workers=False\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config[\"training\"][\"batch_size\"] * 2,\n",
    "        shuffle=False,\n",
    "        num_workers=0,  # Disabled for macOS compatibility (multiprocessing issue)\n",
    "        persistent_workers=False\n",
    "    )\n",
    "\n",
    "    logger.info(f\"‚úÖ Train loader: {len(train_loader)} batches\")\n",
    "    logger.info(f\"‚úÖ Val loader: {len(val_loader)} batches\")\n",
    "\n",
    "    # CRITICAL: Test single sample first to catch issues early\n",
    "    logger.info(\"üîç Testing single sample load...\")\n",
    "    try:\n",
    "        test_img, test_label = train_dataset[0]\n",
    "        logger.info(f\"  ‚úÖ Single sample loaded: shape={test_img.shape}, label={test_label}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Failed to load single sample: {e}\")\n",
    "        logger.exception(e)\n",
    "        raise RuntimeError(\"Cannot load even a single sample! Check dataset and transforms.\")\n",
    "\n",
    "    # CRITICAL: Verify data loading and label distribution\n",
    "    logger.info(\"üîç Verifying data loading and label distribution...\")\n",
    "    logger.info(\"   Loading first batch (this may take 10-30 seconds)...\")\n",
    "    try:\n",
    "        # Test train loader\n",
    "        logger.info(\"   Creating train batch iterator...\")\n",
    "        train_batch_iter = iter(train_loader)\n",
    "        logger.info(\"   Loading first train batch (32 images)...\")\n",
    "        train_images, train_labels = next(train_batch_iter)\n",
    "        logger.info(f\"  ‚úÖ Train batch loaded successfully!\")\n",
    "        logger.info(f\"     Shape: {train_images.shape}, labels: {train_labels.shape}\")\n",
    "        logger.info(f\"     Label range: [{train_labels.min().item()}, {train_labels.max().item()}]\")\n",
    "        logger.info(f\"     Unique labels in batch: {len(torch.unique(train_labels))}\")\n",
    "\n",
    "        # Test val loader\n",
    "        logger.info(\"   Loading first val batch...\")\n",
    "        val_batch_iter = iter(val_loader)\n",
    "        val_images, val_labels = next(val_batch_iter)\n",
    "        logger.info(f\"  ‚úÖ Val batch loaded successfully!\")\n",
    "        logger.info(f\"     Shape: {val_images.shape}, labels: {val_labels.shape}\")\n",
    "        logger.info(f\"     Label range: [{val_labels.min().item()}, {val_labels.max().item()}]\")\n",
    "        logger.info(f\"     Unique labels in batch: {len(torch.unique(val_labels))}\")\n",
    "\n",
    "        # Verify labels are in valid range [0, 29]\n",
    "        if train_labels.min() < 0 or train_labels.max() >= 30:\n",
    "            raise ValueError(f\"Train labels out of range! Min: {train_labels.min()}, Max: {train_labels.max()}\")\n",
    "        if val_labels.min() < 0 or val_labels.max() >= 30:\n",
    "            raise ValueError(f\"Val labels out of range! Min: {val_labels.min()}, Max: {val_labels.max()}\")\n",
    "\n",
    "        logger.info(\"  ‚úÖ All labels are in valid range [0, 29]\")\n",
    "        logger.info(\"  ‚úÖ Data loading verification PASSED!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Data loading verification failed: {e}\")\n",
    "        logger.exception(e)  # Full traceback\n",
    "        raise\n",
    "\n",
    "    # PEAK OPTIMIZER: AdamW with optimal hyperparameters\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config[\"training\"][\"learning_rate\"],\n",
    "        weight_decay=config[\"training\"][\"weight_decay\"],\n",
    "        betas=(0.9, 0.999),  # Standard momentum\n",
    "        eps=1e-8\n",
    "    )\n",
    "\n",
    "    # ULTRA-OPTIMIZED CRITERION: NO label smoothing for maximum accuracy\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.0)  # Clear signal for pretrained model\n",
    "\n",
    "    # Professional training configuration\n",
    "    use_amp = config[\"training\"].get(\"use_amp\", False) and (device.type == \"cuda\")\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=use_amp) if use_amp else None\n",
    "    accumulation_steps = config[\"training\"][\"grad_accum_steps\"]\n",
    "    max_grad_norm = config[\"training\"].get(\"max_grad_norm\", 1.0)\n",
    "    warmup_epochs = config[\"training\"].get(\"warmup_epochs\", 0)\n",
    "\n",
    "    # PEAK SCHEDULER: Cosine annealing with warmup for optimal convergence\n",
    "    total_steps = len(train_loader) * config[\"training\"][\"num_epochs\"] // accumulation_steps\n",
    "    warmup_steps = (len(train_loader) * warmup_epochs) // accumulation_steps\n",
    "\n",
    "    if warmup_steps > 0:\n",
    "        # ULTRA-OPTIMIZED: OneCycleLR with AGGRESSIVE warmup for pretrained model\n",
    "        scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr=config[\"training\"][\"learning_rate\"],  # Peak at 1e-3\n",
    "            total_steps=total_steps,\n",
    "            pct_start=warmup_steps / total_steps,  # Warmup percentage\n",
    "            anneal_strategy='cos',\n",
    "            div_factor=2.0,  # Start at LR/2 (5e-4) - VERY AGGRESSIVE for pretrained\n",
    "            final_div_factor=20.0  # End at LR/20 (5e-5) - Good final convergence\n",
    "        )\n",
    "    else:\n",
    "        # Use CosineAnnealingLR without warmup\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max=total_steps,\n",
    "            eta_min=config[\"training\"][\"learning_rate\"] / 20  # Good final LR\n",
    "        )\n",
    "\n",
    "    # CRITICAL: Load checkpoint if resuming training\n",
    "    if resume_from_checkpoint and Path(resume_from_checkpoint).exists():\n",
    "        logger.info(f\"üìÇ Loading checkpoint from {resume_from_checkpoint}\")\n",
    "        checkpoint = torch.load(resume_from_checkpoint, map_location=device)\n",
    "\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "        if 'scheduler_state_dict' in checkpoint:\n",
    "            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "\n",
    "        start_epoch = checkpoint.get('epoch', 0) + 1  # Start from next epoch\n",
    "        best_val_acc = checkpoint.get('val_acc', 0.0)\n",
    "\n",
    "        if 'metrics_history' in checkpoint:\n",
    "            metrics_history = checkpoint['metrics_history']\n",
    "\n",
    "        logger.info(f\"‚úÖ Resumed from epoch {start_epoch}, best val acc: {best_val_acc:.2f}%\")\n",
    "    elif resume_from_checkpoint:\n",
    "        logger.warning(f\"‚ö†Ô∏è  Checkpoint not found: {resume_from_checkpoint}. Starting from scratch.\")\n",
    "\n",
    "    early_stopping = EarlyStopping(patience=config[\"training\"][\"patience\"])\n",
    "\n",
    "    if device.type == \"mps\":\n",
    "        logger.info(\"‚ÑπÔ∏è  MPS detected: AMP disabled (not supported on Apple Silicon)\")\n",
    "    elif device.type == \"cpu\":\n",
    "        logger.info(\"‚ÑπÔ∏è  CPU detected: AMP disabled (not supported on CPU)\")\n",
    "    else:\n",
    "        logger.info(f\"‚ÑπÔ∏è  AMP {'enabled' if use_amp else 'disabled'}\")\n",
    "\n",
    "    logger.info(f\"Training configuration:\")\n",
    "    logger.info(f\"  - Batch size: {config['training']['batch_size']}\")\n",
    "    logger.info(f\"  - Gradient accumulation: {accumulation_steps}\")\n",
    "    logger.info(f\"  - Effective batch size: {config['training']['batch_size'] * accumulation_steps}\")\n",
    "    logger.info(f\"  - Mixed precision (AMP): {use_amp}\")\n",
    "    logger.info(f\"  - Gradient clipping: {max_grad_norm}\")\n",
    "    logger.info(f\"  - Learning rate: {config['training']['learning_rate']}\")\n",
    "\n",
    "    # INDUSTRIAL-GRADE: Best model tracking\n",
    "    # Note: best_val_acc and metrics_history initialized at function start (for checkpoint resume support)\n",
    "    best_model_state = None\n",
    "    checkpoint_dir = Path(\"checkpoints\")\n",
    "    checkpoint_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    # Initialize Weights & Biases with graceful fallback\n",
    "    try:\n",
    "        wandb.init(project=\"sustainability-vision-lake\", config=config, mode=\"online\")\n",
    "        logger.info(\"‚úì W&B logging enabled\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"W&B initialization failed: {e}. Continuing without logging.\")\n",
    "        wandb.init(mode=\"disabled\")\n",
    "\n",
    "    # CRITICAL: Pre-training sanity check - validate one batch\n",
    "    logger.info(\"üîç Running pre-training sanity check...\")\n",
    "    try:\n",
    "        model.eval()\n",
    "        test_batch = next(iter(train_loader))\n",
    "        test_images, test_labels = test_batch\n",
    "\n",
    "        logger.info(f\"  Batch shape: {test_images.shape}\")\n",
    "        logger.info(f\"  Expected: [batch_size, 3, {config.get('data', {}).get('input_size', 224)}, {config.get('data', {}).get('input_size', 224)}]\")\n",
    "\n",
    "        # Validate batch dimensions\n",
    "        expected_size = config.get('data', {}).get('input_size', 224)\n",
    "        if test_images.shape[1] != 3:\n",
    "            raise ValueError(f\"Invalid channel count: {test_images.shape[1]} (expected 3)\")\n",
    "        if test_images.shape[2] != expected_size or test_images.shape[3] != expected_size:\n",
    "            raise ValueError(\n",
    "                f\"Invalid image size: {test_images.shape[2]}x{test_images.shape[3]} \"\n",
    "                f\"(expected {expected_size}x{expected_size})\"\n",
    "            )\n",
    "\n",
    "        # Test forward pass\n",
    "        use_non_blocking = (device.type == \"cuda\")\n",
    "        test_images = test_images.to(device, non_blocking=use_non_blocking)\n",
    "        test_labels = test_labels.to(device, non_blocking=use_non_blocking)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            test_outputs = model(test_images)\n",
    "            logger.info(f\"  Model output shape: {test_outputs.shape}\")\n",
    "            logger.info(f\"  Expected: [batch_size, {config['model']['num_classes']}]\")\n",
    "\n",
    "            if test_outputs.shape[1] != config['model']['num_classes']:\n",
    "                raise ValueError(\n",
    "                    f\"Invalid output classes: {test_outputs.shape[1]} \"\n",
    "                    f\"(expected {config['model']['num_classes']})\"\n",
    "                )\n",
    "\n",
    "        logger.info(\"  ‚úÖ Pre-training sanity check passed!\")\n",
    "        logger.info(f\"  ‚úÖ All images are {expected_size}x{expected_size}\")\n",
    "        logger.info(f\"  ‚úÖ Model accepts input and produces correct output shape\")\n",
    "\n",
    "        # Clean up\n",
    "        del test_batch, test_images, test_labels, test_outputs\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "        elif device.type == \"mps\":\n",
    "            try:\n",
    "                torch.mps.empty_cache()\n",
    "            except AttributeError:\n",
    "                pass\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Pre-training sanity check failed: {e}\")\n",
    "        logger.error(\"This indicates a fundamental configuration issue. Aborting training.\")\n",
    "        raise\n",
    "\n",
    "    # COMPREHENSIVE DATA QUALITY VALIDATION\n",
    "    logger.info(\"=\"*80)\n",
    "    logger.info(\"üî¨ COMPREHENSIVE DATA QUALITY VALIDATION\")\n",
    "    logger.info(\"=\"*80)\n",
    "\n",
    "    try:\n",
    "        # CRITICAL TEST 1: Validate individual image loading\n",
    "        logger.info(\"Test 1: Individual Image Loading (100 random samples)...\")\n",
    "        import random\n",
    "        sample_indices = random.sample(range(len(train_dataset)), min(100, len(train_dataset)))\n",
    "\n",
    "        failed_samples = []\n",
    "        dummy_tensor_count = 0\n",
    "\n",
    "        for idx in sample_indices:\n",
    "            try:\n",
    "                img, label = train_dataset[idx]\n",
    "\n",
    "                # Check if this is a dummy tensor (all zeros after normalization)\n",
    "                if isinstance(img, torch.Tensor):\n",
    "                    # Check if it's the dummy tensor pattern\n",
    "                    expected_dummy_r = -0.485 / 0.229\n",
    "                    expected_dummy_g = -0.456 / 0.224\n",
    "                    expected_dummy_b = -0.406 / 0.225\n",
    "\n",
    "                    if (torch.allclose(img[0], torch.full_like(img[0], expected_dummy_r), atol=1e-4) and\n",
    "                        torch.allclose(img[1], torch.full_like(img[1], expected_dummy_g), atol=1e-4) and\n",
    "                        torch.allclose(img[2], torch.full_like(img[2], expected_dummy_b), atol=1e-4)):\n",
    "                        dummy_tensor_count += 1\n",
    "                        failed_samples.append(idx)\n",
    "\n",
    "            except Exception as e:\n",
    "                failed_samples.append(idx)\n",
    "                logger.error(f\"  ‚ùå Sample {idx} failed: {e}\")\n",
    "\n",
    "        success_rate = (len(sample_indices) - len(failed_samples)) / len(sample_indices) * 100\n",
    "        logger.info(f\"  ‚úÖ Success rate: {success_rate:.1f}% ({len(sample_indices) - len(failed_samples)}/{len(sample_indices)})\")\n",
    "        logger.info(f\"  ‚ö†Ô∏è  Dummy tensors detected: {dummy_tensor_count}\")\n",
    "        logger.info(f\"  ‚ö†Ô∏è  Failed samples: {len(failed_samples)}\")\n",
    "\n",
    "        # ABORT if too many failures\n",
    "        if dummy_tensor_count > 5:\n",
    "            logger.error(f\"‚ùå TOO MANY DUMMY TENSORS ({dummy_tensor_count}/100)!\")\n",
    "            logger.error(\"   This means images are NOT loading correctly.\")\n",
    "            logger.error(\"   Checking first failed sample for details...\")\n",
    "\n",
    "            if failed_samples:\n",
    "                # Get details about first failure\n",
    "                first_fail_idx = failed_samples[0]\n",
    "                path, label = train_dataset.dataset.samples[first_fail_idx]\n",
    "                logger.error(f\"   Failed sample path: {path}\")\n",
    "                logger.error(f\"   Path exists: {path.exists()}\")\n",
    "                if path.exists():\n",
    "                    logger.error(f\"   File size: {path.stat().st_size} bytes\")\n",
    "\n",
    "            raise RuntimeError(\n",
    "                f\"Dataset loading is BROKEN! {dummy_tensor_count}/100 samples are dummy tensors. \"\n",
    "                \"This is NOT acceptable for training.\"\n",
    "            )\n",
    "\n",
    "        # CRITICAL TEST 2: Validate batch loading\n",
    "        logger.info(\"Test 2: Batch Loading (10 random batches)...\")\n",
    "        batch_count = 0\n",
    "        total_images = 0\n",
    "        value_ranges = []\n",
    "\n",
    "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "            if batch_idx >= 10:\n",
    "                break\n",
    "\n",
    "            batch_count += 1\n",
    "            total_images += images.size(0)\n",
    "\n",
    "            # Check for NaN or Inf\n",
    "            if torch.isnan(images).any():\n",
    "                logger.error(f\"  ‚ùå Batch {batch_idx}: Contains NaN values!\")\n",
    "                raise ValueError(\"Dataset contains NaN values\")\n",
    "\n",
    "            if torch.isinf(images).any():\n",
    "                logger.error(f\"  ‚ùå Batch {batch_idx}: Contains Inf values!\")\n",
    "                raise ValueError(\"Dataset contains Inf values\")\n",
    "\n",
    "            # Track value ranges\n",
    "            batch_min = images.min().item()\n",
    "            batch_max = images.max().item()\n",
    "            value_ranges.append((batch_min, batch_max))\n",
    "\n",
    "            # Check if batch contains mostly dummy tensors\n",
    "            # Dummy tensors have very specific values\n",
    "            if batch_min > -2.5 and batch_max < -1.5:\n",
    "                logger.warning(f\"  ‚ö†Ô∏è  Batch {batch_idx} may contain dummy tensors (range: [{batch_min:.3f}, {batch_max:.3f}])\")\n",
    "\n",
    "            # Validate labels\n",
    "            if labels.min() < 0 or labels.max() >= 30:\n",
    "                logger.error(f\"  ‚ùå Batch {batch_idx}: Invalid labels [{labels.min()}, {labels.max()}]\")\n",
    "                raise ValueError(f\"Invalid label range: [{labels.min()}, {labels.max()}]\")\n",
    "\n",
    "        # Report statistics\n",
    "        min_vals = [r[0] for r in value_ranges]\n",
    "        max_vals = [r[1] for r in value_ranges]\n",
    "        overall_min = min(min_vals)\n",
    "        overall_max = max(max_vals)\n",
    "\n",
    "        logger.info(f\"  ‚úÖ Tested {batch_count} batches ({total_images} images)\")\n",
    "        logger.info(f\"  ‚úÖ Value range: [{overall_min:.3f}, {overall_max:.3f}]\")\n",
    "        logger.info(f\"  ‚úÖ No NaN or Inf values detected\")\n",
    "        logger.info(f\"  ‚úÖ All labels valid [0, 29]\")\n",
    "\n",
    "        # Validate value range is reasonable for real images\n",
    "        # Real images after ImageNet normalization should be roughly [-2.5, 2.5]\n",
    "        if overall_min > -1.0 or overall_max < 1.0:\n",
    "            logger.warning(f\"  ‚ö†Ô∏è  WARNING: Value range [{overall_min:.3f}, {overall_max:.3f}] seems narrow!\")\n",
    "            logger.warning(f\"     Expected range for real images: approximately [-2.5, 2.5]\")\n",
    "            logger.warning(f\"     This may indicate dummy tensors are being used!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Data quality validation failed: {e}\")\n",
    "        raise\n",
    "\n",
    "    logger.info(\"=\"*80)\n",
    "    logger.info(\"‚úÖ ALL VALIDATION TESTS PASSED - READY TO TRAIN\")\n",
    "    logger.info(\"=\"*80)\n",
    "\n",
    "    model.train()  # Set back to training mode\n",
    "\n",
    "    # Main training loop with error handling\n",
    "    try:\n",
    "        for epoch in range(start_epoch, config[\"training\"][\"num_epochs\"]):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "\n",
    "            pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['training']['num_epochs']}\")\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            for i, (images, labels) in enumerate(pbar):\n",
    "                try:\n",
    "                    # CRITICAL: Aggressive memory cleanup every 100 batches for MPS stability\n",
    "                    if i > 0 and i % 100 == 0 and device.type == \"mps\":\n",
    "                        try:\n",
    "                            torch.mps.empty_cache()\n",
    "                        except AttributeError:\n",
    "                            pass\n",
    "\n",
    "                    # CRITICAL: Validate batch shape before processing\n",
    "                    expected_size = config.get('data', {}).get('input_size', 224)\n",
    "                    if images.shape[1] != 3 or images.shape[2] != expected_size or images.shape[3] != expected_size:\n",
    "                        logger.error(\n",
    "                            f\"‚ùå Batch {i} has invalid shape: {images.shape}. \"\n",
    "                            f\"Expected: [batch_size, 3, {expected_size}, {expected_size}]. Skipping batch.\"\n",
    "                        )\n",
    "                        continue\n",
    "\n",
    "                    # non_blocking only works with CUDA + pin_memory\n",
    "                    use_non_blocking = (device.type == \"cuda\")\n",
    "                    images, labels = images.to(device, non_blocking=use_non_blocking), labels.to(device, non_blocking=use_non_blocking)\n",
    "\n",
    "                except RuntimeError as e:\n",
    "                    if \"out of memory\" in str(e) or \"MPS\" in str(e):\n",
    "                        logger.error(f\"OOM at batch {i}. Clearing cache and skipping batch.\")\n",
    "                        if device.type == \"cuda\":\n",
    "                            torch.cuda.empty_cache()\n",
    "                        elif device.type == \"mps\":\n",
    "                            try:\n",
    "                                torch.mps.empty_cache()\n",
    "                            except AttributeError:\n",
    "                                pass  # MPS empty_cache not available in older PyTorch\n",
    "                        continue\n",
    "                    raise\n",
    "                except AssertionError as e:\n",
    "                    # Catch assertion errors from model (e.g., size mismatches)\n",
    "                    logger.error(f\"‚ùå Assertion error at batch {i}: {e}\")\n",
    "                    logger.error(f\"   Batch shape: {images.shape}\")\n",
    "                    logger.error(f\"   This indicates a size mismatch. Skipping batch.\")\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    if use_amp:\n",
    "                        with torch.cuda.amp.autocast():\n",
    "                            outputs = model(images)\n",
    "                            loss = criterion(outputs, labels) / accumulation_steps\n",
    "                        scaler.scale(loss).backward()\n",
    "\n",
    "                        if (i + 1) % accumulation_steps == 0:\n",
    "                            scaler.unscale_(optimizer)\n",
    "\n",
    "                            # CRITICAL: Check gradient health\n",
    "                            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_grad_norm)\n",
    "                            if not np.isfinite(grad_norm.item()):\n",
    "                                logger.error(f\"‚ùå Non-finite gradient norm at batch {i}: {grad_norm.item()}\")\n",
    "                                logger.error(\"   Resetting optimizer state and skipping update.\")\n",
    "                                optimizer.zero_grad()\n",
    "                                continue\n",
    "\n",
    "                            scaler.step(optimizer)\n",
    "                            scaler.update()\n",
    "                            optimizer.zero_grad()\n",
    "                            scheduler.step()  # OneCycleLR steps per batch\n",
    "                    else:\n",
    "                        outputs = model(images)\n",
    "                        loss = criterion(outputs, labels) / accumulation_steps\n",
    "                        loss.backward()\n",
    "                        if (i + 1) % accumulation_steps == 0:\n",
    "                            # CRITICAL: Check gradient health\n",
    "                            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_grad_norm)\n",
    "                            if not np.isfinite(grad_norm.item()):\n",
    "                                logger.error(f\"‚ùå Non-finite gradient norm at batch {i}: {grad_norm.item()}\")\n",
    "                                logger.error(\"   Resetting optimizer state and skipping update.\")\n",
    "                                optimizer.zero_grad()\n",
    "                                continue\n",
    "\n",
    "                            optimizer.step()\n",
    "                            optimizer.zero_grad()\n",
    "                            scheduler.step()  # OneCycleLR steps per batch\n",
    "\n",
    "                except AssertionError as e:\n",
    "                    # Catch assertion errors from model forward pass\n",
    "                    logger.error(f\"‚ùå Model forward pass failed at batch {i}: {e}\")\n",
    "                    logger.error(f\"   Input shape: {images.shape}\")\n",
    "                    logger.error(f\"   Expected size: {expected_size}x{expected_size}\")\n",
    "                    logger.error(\"   Skipping batch and continuing training.\")\n",
    "                    continue\n",
    "\n",
    "                # CRITICAL: Check for NaN/Inf in loss\n",
    "                loss_value = loss.item() * accumulation_steps\n",
    "                if not np.isfinite(loss_value):\n",
    "                    logger.error(f\"‚ùå Non-finite loss detected at batch {i}: {loss_value}\")\n",
    "                    logger.error(\"   This indicates training instability. Skipping batch.\")\n",
    "                    # Reset gradients to prevent corruption\n",
    "                    optimizer.zero_grad()\n",
    "                    continue\n",
    "\n",
    "                running_loss += loss_value\n",
    "                with torch.no_grad():\n",
    "                    _, predicted = outputs.max(1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "                    # CRITICAL DEBUG: Log first batch predictions\n",
    "                    if i == 0:\n",
    "                        logger.info(f\"üîç FIRST TRAINING BATCH:\")\n",
    "                        logger.info(f\"   Images shape: {images.shape}\")\n",
    "                        logger.info(f\"   Images min/max: {images.min().item():.3f} / {images.max().item():.3f}\")\n",
    "                        logger.info(f\"   Labels: {labels[:10].cpu().numpy()}\")\n",
    "                        logger.info(f\"   Predictions: {predicted[:10].cpu().numpy()}\")\n",
    "                        logger.info(f\"   Correct: {predicted.eq(labels).sum().item()}/{labels.size(0)}\")\n",
    "                        logger.info(f\"   Output logits (first sample): {outputs[0, :10].cpu().numpy()}\")\n",
    "\n",
    "                current_loss = running_loss / (i + 1)\n",
    "                pbar.set_postfix({'loss': f\"{current_loss:.4f}\", 'acc': f\"{100*correct/total:.2f}%\"})\n",
    "\n",
    "            train_acc = 100 * correct / total\n",
    "\n",
    "            # CRITICAL: Epoch-level health check\n",
    "            if total == 0:\n",
    "                logger.error(f\"‚ùå Epoch {epoch+1}: No samples processed! This indicates a critical data loading issue.\")\n",
    "                break\n",
    "\n",
    "            avg_train_loss = running_loss / len(train_loader) if len(train_loader) > 0 else float('inf')\n",
    "            if not np.isfinite(avg_train_loss):\n",
    "                logger.error(f\"‚ùå Epoch {epoch+1}: Non-finite training loss: {avg_train_loss}\")\n",
    "                logger.error(\"   Training has become unstable. Stopping.\")\n",
    "                break\n",
    "\n",
    "            # INDUSTRIAL-GRADE: Comprehensive validation with per-class metrics\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            all_preds = []\n",
    "            all_labels = []\n",
    "\n",
    "            # CRITICAL: Log first batch details for debugging\n",
    "            first_batch_logged = False\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for val_i, (images, labels) in enumerate(tqdm(val_loader, desc=\"Validation\", leave=False)):\n",
    "                    try:\n",
    "                        # CRITICAL: Log first batch for debugging\n",
    "                        if not first_batch_logged:\n",
    "                            logger.info(f\"üîç First validation batch:\")\n",
    "                            logger.info(f\"   Images shape: {images.shape}\")\n",
    "                            logger.info(f\"   Labels shape: {labels.shape}\")\n",
    "                            logger.info(f\"   Labels range: [{labels.min().item()}, {labels.max().item()}]\")\n",
    "                            logger.info(f\"   Unique labels: {len(torch.unique(labels))}\")\n",
    "                            first_batch_logged = True\n",
    "\n",
    "                        # Validate batch shape\n",
    "                        expected_size = config.get('data', {}).get('input_size', 224)\n",
    "                        if images.shape[1] != 3 or images.shape[2] != expected_size or images.shape[3] != expected_size:\n",
    "                            logger.warning(\n",
    "                                f\"‚ö†Ô∏è  Validation batch {val_i} has invalid shape: {images.shape}. Skipping.\"\n",
    "                            )\n",
    "                            continue\n",
    "\n",
    "                        use_non_blocking = (device.type == \"cuda\")\n",
    "                        images, labels = images.to(device, non_blocking=use_non_blocking), labels.to(device, non_blocking=use_non_blocking)\n",
    "\n",
    "                        if use_amp:\n",
    "                            with torch.cuda.amp.autocast():\n",
    "                                outputs = model(images)\n",
    "                                loss = criterion(outputs, labels)\n",
    "                        else:\n",
    "                            outputs = model(images)\n",
    "                            loss = criterion(outputs, labels)\n",
    "\n",
    "                        # CRITICAL: Log first batch predictions\n",
    "                        if val_i == 0:\n",
    "                            _, predicted = outputs.max(1)\n",
    "                            logger.info(f\"   First batch predictions: {predicted[:10].cpu().numpy()}\")\n",
    "                            logger.info(f\"   First batch ground truth: {labels[:10].cpu().numpy()}\")\n",
    "                            logger.info(f\"   First batch correct: {predicted.eq(labels).sum().item()}/{labels.size(0)}\")\n",
    "\n",
    "                        loss_val = loss.item()\n",
    "                        if not np.isfinite(loss_val):\n",
    "                            logger.warning(f\"‚ö†Ô∏è  Non-finite validation loss at batch {val_i}: {loss_val}. Skipping.\")\n",
    "                            continue\n",
    "\n",
    "                        val_loss += loss_val\n",
    "                        _, predicted = outputs.max(1)\n",
    "                        val_total += labels.size(0)\n",
    "                        val_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "                        # Collect for per-class metrics\n",
    "                        all_preds.extend(predicted.cpu().numpy())\n",
    "                        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"‚ö†Ô∏è  Validation batch {val_i} failed: {e}. Skipping.\")\n",
    "                        logger.exception(e)  # Full traceback\n",
    "                        continue\n",
    "\n",
    "            # CRITICAL: Validation health check\n",
    "            if val_total == 0:\n",
    "                logger.error(f\"‚ùå Epoch {epoch+1}: No validation samples processed!\")\n",
    "                logger.error(\"   This indicates a critical issue with validation data.\")\n",
    "                logger.error(f\"   Val loader length: {len(val_loader)}\")\n",
    "                logger.error(f\"   Val dataset length: {len(val_dataset)}\")\n",
    "                break\n",
    "\n",
    "            logger.info(f\"üìä Validation summary: {val_correct}/{val_total} correct ({100*val_correct/val_total:.2f}%)\")\n",
    "\n",
    "            val_acc = 100 * val_correct / val_total\n",
    "            val_loss /= len(val_loader)\n",
    "\n",
    "            if not np.isfinite(val_loss):\n",
    "                logger.error(f\"‚ùå Epoch {epoch+1}: Non-finite validation loss: {val_loss}\")\n",
    "                logger.error(\"   Validation has become unstable. Stopping.\")\n",
    "                break\n",
    "\n",
    "            # INDUSTRIAL-GRADE: Per-class metrics\n",
    "            if len(all_preds) == 0 or len(all_labels) == 0:\n",
    "                logger.error(f\"‚ùå Epoch {epoch+1}: No predictions collected for metrics!\")\n",
    "                break\n",
    "\n",
    "            precision, recall, f1, support = precision_recall_fscore_support(\n",
    "                all_labels, all_preds, average=None, zero_division=0\n",
    "            )\n",
    "            macro_f1 = f1.mean()\n",
    "\n",
    "            # Find worst performing classes\n",
    "            worst_classes_idx = np.argsort(f1)[:5]\n",
    "            logger.info(f\"üìä Per-Class Performance:\")\n",
    "            logger.info(f\"  Macro F1: {macro_f1:.4f}\")\n",
    "            logger.info(f\"  Worst 5 classes:\")\n",
    "            for idx in worst_classes_idx:\n",
    "                if idx < len(TARGET_CLASSES):\n",
    "                    logger.info(f\"    {TARGET_CLASSES[idx]}: F1={f1[idx]:.4f}, Support={support[idx]}\")\n",
    "\n",
    "            logger.info(f\"Epoch {epoch+1}/{config['training']['num_epochs']}: Train Acc {train_acc:.2f}%, Val Loss {val_loss:.4f}, Val Acc {val_acc:.2f}%, Macro F1 {macro_f1:.4f}\")\n",
    "\n",
    "            # INDUSTRIAL-GRADE: Track metrics history\n",
    "            metrics_history[\"train_acc\"].append(train_acc)\n",
    "            metrics_history[\"val_acc\"].append(val_acc)\n",
    "            metrics_history[\"val_loss\"].append(val_loss)\n",
    "            metrics_history[\"per_class_f1\"].append(macro_f1)\n",
    "            metrics_history[\"learning_rate\"].append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "            try:\n",
    "                wandb.log({\n",
    "                    \"epoch\": epoch + 1,\n",
    "                    \"train_acc\": train_acc,\n",
    "                    \"val_acc\": val_acc,\n",
    "                    \"val_loss\": val_loss,\n",
    "                    \"macro_f1\": macro_f1,\n",
    "                    \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "                    \"worst_class_f1\": f1[worst_classes_idx[0]] if len(worst_classes_idx) > 0 else 0\n",
    "                })\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # INDUSTRIAL-GRADE: Save best model checkpoint\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                best_model_state = {\n",
    "                    'epoch': epoch + 1,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': scheduler.state_dict(),\n",
    "                    'val_acc': val_acc,\n",
    "                    'val_loss': val_loss,\n",
    "                    'macro_f1': macro_f1,\n",
    "                    'config': config,\n",
    "                    'metrics_history': metrics_history\n",
    "                }\n",
    "                checkpoint_path = checkpoint_dir / f\"best_model_epoch{epoch+1}_acc{val_acc:.2f}.pth\"\n",
    "                torch.save(best_model_state, checkpoint_path)\n",
    "                logger.info(f\"‚úì Saved best model checkpoint: {checkpoint_path}\")\n",
    "\n",
    "                # Keep only best checkpoint, delete others\n",
    "                for old_ckpt in checkpoint_dir.glob(\"best_model_*.pth\"):\n",
    "                    if old_ckpt != checkpoint_path:\n",
    "                        old_ckpt.unlink()\n",
    "\n",
    "            if early_stopping(val_acc):\n",
    "                logger.info(\"Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "            # Clear cache after each epoch to prevent memory fragmentation\n",
    "            if device.type == \"cuda\":\n",
    "                torch.cuda.empty_cache()\n",
    "            elif device.type == \"mps\":\n",
    "                try:\n",
    "                    torch.mps.empty_cache()\n",
    "                except AttributeError:\n",
    "                    pass  # MPS empty_cache not available in older PyTorch\n",
    "\n",
    "        # INDUSTRIAL-GRADE: Training completed - generate final report\n",
    "        logger.info(\"=\"*60)\n",
    "        logger.info(\"‚úì Training completed successfully\")\n",
    "        logger.info(f\"üìä Final Results:\")\n",
    "        logger.info(f\"  Best Val Accuracy: {best_val_acc:.2f}%\")\n",
    "        logger.info(f\"  Total Epochs: {epoch + 1}\")\n",
    "        logger.info(f\"  Best Checkpoint: {checkpoint_path if best_model_state else 'None'}\")\n",
    "        logger.info(\"=\"*60)\n",
    "\n",
    "        # INDUSTRIAL-GRADE: Generate confusion matrix for best model\n",
    "        if best_model_state:\n",
    "            logger.info(\"Generating confusion matrix for best model...\")\n",
    "            model.load_state_dict(best_model_state['model_state_dict'])\n",
    "            model.eval()\n",
    "\n",
    "            all_preds = []\n",
    "            all_labels = []\n",
    "            with torch.no_grad():\n",
    "                for images, labels in tqdm(val_loader, desc=\"Final Evaluation\"):\n",
    "                    images = images.to(device)\n",
    "                    outputs = model(images)\n",
    "                    _, predicted = outputs.max(1)\n",
    "                    all_preds.extend(predicted.cpu().numpy())\n",
    "                    all_labels.extend(labels.numpy())\n",
    "\n",
    "            # Save confusion matrix\n",
    "            cm = confusion_matrix(all_labels, all_preds)\n",
    "            np.save(checkpoint_dir / \"confusion_matrix.npy\", cm)\n",
    "\n",
    "            # Save classification report\n",
    "            # CRITICAL FIX: Only use labels that are actually present in the data\n",
    "            unique_labels = sorted(list(set(all_labels + all_preds)))\n",
    "            labels_subset = [i for i in range(len(TARGET_CLASSES)) if i in unique_labels]\n",
    "            target_names_subset = [TARGET_CLASSES[i] for i in labels_subset]\n",
    "\n",
    "            report = classification_report(\n",
    "                all_labels, all_preds,\n",
    "                labels=labels_subset,\n",
    "                target_names=target_names_subset,\n",
    "                output_dict=True,\n",
    "                zero_division=0\n",
    "            )\n",
    "            with open(checkpoint_dir / \"classification_report.json\", \"w\") as f:\n",
    "                json.dump(report, f, indent=2)\n",
    "\n",
    "            logger.info(f\"‚úì Saved confusion matrix and classification report to {checkpoint_dir}\")\n",
    "            logger.info(f\"  Classes present in validation: {len(unique_labels)}/{len(TARGET_CLASSES)}\")\n",
    "\n",
    "        # INDUSTRIAL-GRADE: Save final metrics\n",
    "        with open(checkpoint_dir / \"metrics_history.json\", \"w\") as f:\n",
    "            json.dump(metrics_history, f, indent=2)\n",
    "\n",
    "        logger.info(\"‚úì All artifacts saved successfully\")\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" in str(e) or \"MPS\" in str(e):\n",
    "            logger.error(f\"OOM Error: {e}\")\n",
    "            logger.error(\"Suggestions:\")\n",
    "            logger.error(\"  1. Reduce batch_size further (try batch_size=1)\")\n",
    "            logger.error(\"  2. Reduce input_size (try 128 or 192)\")\n",
    "            logger.error(\"  3. Use a smaller model backbone (e.g., resnet50)\")\n",
    "            if device.type == \"cuda\":\n",
    "                torch.cuda.empty_cache()\n",
    "            elif device.type == \"mps\":\n",
    "                try:\n",
    "                    torch.mps.empty_cache()\n",
    "                except AttributeError:\n",
    "                    pass\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Training failed with error: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        # Cleanup\n",
    "        try:\n",
    "            wandb.finish()\n",
    "        except:\n",
    "            pass\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "        elif device.type == \"mps\":\n",
    "            try:\n",
    "                torch.mps.empty_cache()\n",
    "            except AttributeError:\n",
    "                pass\n",
    "\n",
    "    return model\n"
   ],
   "id": "35e0483892870ce3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# INDUSTRIAL-GRADE: Test-Time Augmentation for Inference\n",
    "def predict_with_tta(model, image, device, num_augmentations=5):\n",
    "    \"\"\"\n",
    "    Test-Time Augmentation for robust predictions.\n",
    "    Applies multiple augmentations and averages predictions.\n",
    "\n",
    "    Args:\n",
    "        model: Trained model\n",
    "        image: PIL Image or tensor\n",
    "        device: torch device\n",
    "        num_augmentations: Number of TTA iterations\n",
    "\n",
    "    Returns:\n",
    "        Averaged predictions (logits)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # TTA transforms\n",
    "    tta_transforms = [\n",
    "        transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomRotation(degrees=15),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        for _ in range(num_augmentations)\n",
    "    ]\n",
    "\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for transform in tta_transforms:\n",
    "            if isinstance(image, torch.Tensor):\n",
    "                img_tensor = image\n",
    "            else:\n",
    "                img_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "            output = model(img_tensor)\n",
    "            predictions.append(output)\n",
    "\n",
    "    # Average predictions\n",
    "    avg_prediction = torch.stack(predictions).mean(dim=0)\n",
    "    return avg_prediction\n"
   ],
   "id": "dd26b5c26bdf32d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# INDUSTRIAL-GRADE: Model Export for Production\n",
    "def export_model_for_production(model, config, checkpoint_path, export_dir=\"exports\"):\n",
    "    \"\"\"\n",
    "    Export model to multiple formats for production deployment.\n",
    "\n",
    "    Exports:\n",
    "    - PyTorch (.pth) - for PyTorch inference\n",
    "    - TorchScript (.pt) - for C++ deployment\n",
    "    - ONNX (.onnx) - for cross-platform deployment\n",
    "    \"\"\"\n",
    "    export_dir = Path(export_dir)\n",
    "    export_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # 1. Save PyTorch model\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'config': config,\n",
    "        'target_classes': TARGET_CLASSES\n",
    "    }, export_dir / \"model.pth\")\n",
    "    logger.info(f\"‚úì Exported PyTorch model to {export_dir / 'model.pth'}\")\n",
    "\n",
    "    # 2. Export to TorchScript\n",
    "    try:\n",
    "        dummy_input = torch.randn(1, 3, config['data']['input_size'], config['data']['input_size']).to(device)\n",
    "        traced_model = torch.jit.trace(model, dummy_input)\n",
    "        traced_model.save(export_dir / \"model_torchscript.pt\")\n",
    "        logger.info(f\"‚úì Exported TorchScript model to {export_dir / 'model_torchscript.pt'}\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"TorchScript export failed: {e}\")\n",
    "\n",
    "    # 3. Export to ONNX\n",
    "    try:\n",
    "        torch.onnx.export(\n",
    "            model,\n",
    "            dummy_input,\n",
    "            export_dir / \"model.onnx\",\n",
    "            export_params=True,\n",
    "            opset_version=14,\n",
    "            do_constant_folding=True,\n",
    "            input_names=['input'],\n",
    "            output_names=['output'],\n",
    "            dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
    "        )\n",
    "        logger.info(f\"‚úì Exported ONNX model to {export_dir / 'model.onnx'}\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"ONNX export failed: {e}\")\n",
    "\n",
    "    # 4. Save metadata\n",
    "    metadata = {\n",
    "        'model_name': config['model']['backbone'],\n",
    "        'num_classes': config['model']['num_classes'],\n",
    "        'input_size': config['data']['input_size'],\n",
    "        'target_classes': TARGET_CLASSES,\n",
    "        'checkpoint_path': str(checkpoint_path)\n",
    "    }\n",
    "    with open(export_dir / \"metadata.json\", \"w\") as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    logger.info(f\"‚úì Saved metadata to {export_dir / 'metadata.json'}\")\n",
    "\n",
    "    logger.info(f\"‚úÖ Model export complete! All files in {export_dir}\")\n"
   ],
   "id": "184a60cacd5ef3a2",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# PEAK STANDARD GNN\n",
    "# Using Graph Attention Networks v2 (GATv2) for superior expressive power\n",
    "\n",
    "def generate_structured_knowledge_graph(num_classes=30, feat_dim=128):\n",
    "    \"\"\"\n",
    "    Generates a realistic Knowledge Graph structure for waste classification.\n",
    "    Simulates the schema: Item -> Material -> Bin\n",
    "    \"\"\"\n",
    "    logger.info(\"Generating structured Knowledge Graph...\")\n",
    "    \n",
    "    total_nodes = num_classes + 8 + 4\n",
    "    x = torch.randn(total_nodes, feat_dim) # Node features (embeddings)\n",
    "    \n",
    "    edge_sources = []\n",
    "    edge_targets = []\n",
    "    \n",
    "    # Node Indices for Materials\n",
    "    mat_base = num_classes\n",
    "    mat_plastic = mat_base + 0\n",
    "    mat_paper = mat_base + 1\n",
    "    mat_glass = mat_base + 2\n",
    "    mat_metal = mat_base + 3\n",
    "    mat_organic = mat_base + 4\n",
    "    mat_fabric = mat_base + 5\n",
    "    mat_ewaste = mat_base + 6\n",
    "    mat_misc = mat_base + 7\n",
    "    \n",
    "    # Node Indices for Bins\n",
    "    bin_base = mat_base + 8\n",
    "    bin_recycle = bin_base + 0\n",
    "    bin_compost = bin_base + 1\n",
    "    bin_haz = bin_base + 2\n",
    "    bin_landfill = bin_base + 3\n",
    "    \n",
    "    # 1. Edges: Material -> Bin (Knowledge Rules)\n",
    "    mat_bin_map = [\n",
    "        (mat_plastic, bin_recycle),\n",
    "        (mat_paper, bin_recycle),\n",
    "        (mat_glass, bin_recycle),\n",
    "        (mat_metal, bin_recycle),\n",
    "        (mat_organic, bin_compost),\n",
    "        (mat_fabric, bin_landfill), \n",
    "        (mat_ewaste, bin_haz),\n",
    "        (mat_misc, bin_landfill)\n",
    "    ]\n",
    "    \n",
    "    for m, b in mat_bin_map:\n",
    "        edge_sources.append(m); edge_targets.append(b)\n",
    "        edge_sources.append(b); edge_targets.append(m)\n",
    "        \n",
    "    # 2. Edges: Item -> Material (Simulate Classification Knowledge)\n",
    "    for i in range(num_classes):\n",
    "        mat_idx = mat_base + (i % 8) \n",
    "        edge_sources.append(i); edge_targets.append(mat_idx)\n",
    "        edge_sources.append(mat_idx); edge_targets.append(i)\n",
    "        \n",
    "    # 3. Edges: Item -> Item (Similarity)\n",
    "    for i in range(num_classes):\n",
    "        neighbor = (i + 8) % num_classes\n",
    "        edge_sources.append(i); edge_targets.append(neighbor)\n",
    "        edge_sources.append(neighbor); edge_targets.append(i)\n",
    "\n",
    "    edge_index = torch.tensor([edge_sources, edge_targets], dtype=torch.long)\n",
    "    \n",
    "    logger.info(f\"Graph generated: {total_nodes} nodes, {len(edge_sources)} edges.\")\n",
    "    \n",
    "    return Data(x=x, edge_index=edge_index, num_nodes=total_nodes)\n",
    "\n",
    "class GATv2Model(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=4, heads=8, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(GATv2Conv(in_channels, hidden_channels, heads=heads, concat=True, dropout=dropout))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(GATv2Conv(hidden_channels * heads, hidden_channels, heads=heads, concat=True, dropout=dropout))\n",
    "        self.convs.append(GATv2Conv(hidden_channels * heads, out_channels, heads=1, concat=False, dropout=dropout))\n",
    "        self.dropout = dropout\n",
    "        self.norm = nn.ModuleList([nn.LayerNorm(hidden_channels * heads) for _ in range(num_layers - 1)])\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for i, conv in enumerate(self.convs[:-1]):\n",
    "            x = conv(x, edge_index)\n",
    "            x = self.norm[i](x)\n",
    "            x = F.gelu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        return self.convs[-1](x, edge_index)"
   ],
   "id": "a413b4ee062ae6a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def train_gnn_model():\n",
    "    set_seed()\n",
    "    device = get_device()\n",
    "    optimize_memory(device)\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "\n",
    "    in_dim = 128\n",
    "    hidden_dim = 512\n",
    "    out_dim = 256\n",
    "    lr = 0.001\n",
    "    epochs = 50\n",
    "\n",
    "    data = generate_structured_knowledge_graph(num_classes=30, feat_dim=128).to(device)\n",
    "\n",
    "    model = GATv2Model(in_dim, hidden_dim, out_dim).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)\n",
    "\n",
    "    logger.info(\"Starting GNN Training...\")\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        z = model(data.x, data.edge_index)\n",
    "\n",
    "        pos_src, pos_dst = data.edge_index\n",
    "        pos_loss = -torch.log(torch.sigmoid((z[pos_src] * z[pos_dst]).sum(dim=1)) + 1e-15).mean()\n",
    "\n",
    "        neg_src = torch.randint(0, data.num_nodes, (pos_src.size(0),), device=device)\n",
    "        neg_dst = torch.randint(0, data.num_nodes, (pos_src.size(0),), device=device)\n",
    "        neg_loss = -torch.log(1 - torch.sigmoid((z[neg_src] * z[neg_dst]).sum(dim=1)) + 1e-15).mean()\n",
    "\n",
    "        loss = pos_loss + neg_loss\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step(loss)\n",
    "\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            logger.info(f\"Epoch {epoch+1}/{epochs}: Loss {loss.item():.4f}, Best Loss {best_loss:.4f}\")\n",
    "\n",
    "    return model"
   ],
   "id": "d48fee1a1cb8a264",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        logger.info(\"=\"*80)\n",
    "        logger.info(\"Phase 1: Multi-Source Data Lake Vision Training\")\n",
    "        logger.info(\"=\"*80)\n",
    "\n",
    "        # CRITICAL: Starting fresh with improved hyperparameters to fix overfitting\n",
    "        # Previous training: Epochs 1-6 showed severe overfitting (92.64% ‚Üí 78.35%)\n",
    "        # New config: Lower LR (1e-5), higher weight decay (0.1)\n",
    "        # Old checkpoint available at: checkpoints/best_model_epoch1_acc92.64.pth\n",
    "        vision_model = train_vision_model(VISION_CONFIG, resume_from_checkpoint=None)\n",
    "\n",
    "        if vision_model is not None:\n",
    "            save_path = \"best_vision_eva02_lake.pth\"\n",
    "            torch.save(vision_model.state_dict(), save_path)\n",
    "            logger.info(f\"Vision model saved to {save_path}\")\n",
    "\n",
    "            del vision_model\n",
    "            device = get_device()\n",
    "            if device.type == \"cuda\":\n",
    "                torch.cuda.empty_cache()\n",
    "            elif device.type == \"mps\":\n",
    "                try:\n",
    "                    torch.mps.empty_cache()\n",
    "                except AttributeError:\n",
    "                    pass\n",
    "        else:\n",
    "            logger.error(\"Vision model training failed\")\n",
    "\n",
    "        logger.info(\"=\"*80)\n",
    "        logger.info(\"Phase 2: GNN Knowledge Graph Training\")\n",
    "        logger.info(\"=\"*80)\n",
    "\n",
    "        gnn_model = train_gnn_model()\n",
    "\n",
    "        if gnn_model is not None:\n",
    "            save_path = \"best_gnn_gatv2.pth\"\n",
    "            torch.save(gnn_model.state_dict(), save_path)\n",
    "            logger.info(f\"GNN model saved to {save_path}\")\n",
    "\n",
    "            del gnn_model\n",
    "            device = get_device()\n",
    "            if device.type == \"cuda\":\n",
    "                torch.cuda.empty_cache()\n",
    "            elif device.type == \"mps\":\n",
    "                try:\n",
    "                    torch.mps.empty_cache()\n",
    "                except AttributeError:\n",
    "                    pass\n",
    "\n",
    "        logger.info(\"=\"*80)\n",
    "        logger.info(\"Training completed successfully!\")\n",
    "        logger.info(\"=\"*80)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Training failed with error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise"
   ],
   "id": "78cc46781594be13",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
