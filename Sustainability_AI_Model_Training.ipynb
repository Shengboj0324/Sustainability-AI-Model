{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "print(\"Installing dependencies for Sustainability AI Model...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--upgrade\", \"pip\"])\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numpy<2.0\"])\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"scipy<1.15.0\"])\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--no-deps\", \"timm==1.0.12\"])\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"albumentations==1.4.22\"])\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"einops==0.8.0\"])\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"wandb==0.19.1\"])\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--no-deps\", \"torch-geometric==2.6.1\"])\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"torch-scatter\", \"torch-sparse\", \"-f\", \"https://data.pyg.org/whl/torch-2.5.0+cu121.html\"])\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"âœ… Dependencies installed successfully!\")\n",
    "print(\"=\"*60)\n",
    "\n"
   ],
   "id": "cd88f3b1d3c6931"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "import timm\n",
    "from timm.data import create_transform, resolve_data_config\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "from tqdm.notebook import tqdm\n",
    "import wandb"
   ],
   "id": "af610a383c091269"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    logger.info(f\"Random seed set to {seed}\")\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        logger.info(f\"CUDA available: {torch.cuda.get_device_name(0)}\")\n",
    "        logger.info(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "        return device\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "def optimize_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "        # Enable expandable segments to reduce fragmentation\n",
    "        import os\n",
    "        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "        logger.info(\"Memory optimization enabled: expandable_segments=True\")\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=15, mode=\"max\", delta=0):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.mode = mode\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, current_score):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = current_score\n",
    "        elif self.mode == \"max\":\n",
    "            if current_score <= self.best_score + self.delta:\n",
    "                self.counter += 1\n",
    "                if self.counter >= self.patience:\n",
    "                    self.early_stop = True\n",
    "            else:\n",
    "                self.best_score = current_score\n",
    "                self.counter = 0\n",
    "        return self.early_stop"
   ],
   "id": "e37d70388c34be67"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_CLASSES = [\n",
    "    'aerosol_cans', 'aluminum_food_cans', 'aluminum_soda_cans', 'cardboard_boxes', 'cardboard_packaging',\n",
    "    'clothing', 'coffee_grounds', 'disposable_plastic_cutlery', 'egg_shells', 'food_waste',\n",
    "    'glass_beverage_bottles', 'glass_cosmetic_containers', 'glass_food_jars', 'magazines',\n",
    "    'newspaper', 'office_paper', 'paper_cups', 'plastic_cup_lids', 'plastic_detergent_bottles',\n",
    "    'plastic_food_containers', 'plastic_shopping_bags', 'plastic_soda_bottles', 'plastic_straws',\n",
    "    'plastic_trash_bags', 'plastic_water_bottles', 'shoes', 'steel_food_cans', 'styrofoam_cups',\n",
    "    'styrofoam_food_containers', 'tea_bags'\n",
    "]\n",
    "\n",
    "VISION_CONFIG = {\n",
    "    \"model\": {\n",
    "        \"backbone\": \"eva02_large_patch14_448.mim_m38m_ft_in22k_in1k\",\n",
    "        \"pretrained\": True,\n",
    "        \"num_classes\": 30,\n",
    "        \"drop_rate\": 0.3,\n",
    "        \"drop_path_rate\": 0.2\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"input_size\": 448,\n",
    "        \"num_workers\": 2,\n",
    "        \"pin_memory\": False,  # Reduce memory pressure\n",
    "        \"sources\": [\n",
    "            {\n",
    "                \"name\": \"master_30\",\n",
    "                \"path\": \"/kaggle/input/recyclable-and-household-waste-classification/images\",\n",
    "                \"type\": \"master\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"garbage_12\",\n",
    "                \"path\": \"/kaggle/input/garbage-classification/garbage_classification\",\n",
    "                \"type\": \"mapped_12\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"waste_22k\",\n",
    "                \"path\": \"/kaggle/input/waste-classification-data/DATASET\",\n",
    "                \"type\": \"mapped_2\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"garbage_v2_10\",\n",
    "                \"path\": \"/kaggle/input/garbage-classification-v2\",\n",
    "                \"type\": \"mapped_10\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"garbage_6\",\n",
    "                \"path\": \"/kaggle/input/garbage-classification\",\n",
    "                \"type\": \"mapped_6\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"garbage_balanced\",\n",
    "                \"path\": \"/kaggle/input/garbage-dataset-classification\",\n",
    "                \"type\": \"mapped_6\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"warp_industrial\",\n",
    "                \"path\": \"/kaggle/input/warp-waste-recycling-plant-dataset\",\n",
    "                \"type\": \"industrial\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"multiclass_garbage\",\n",
    "                \"path\": \"/kaggle/input/multi-class-garbage-classification-dataset\",\n",
    "                \"type\": \"multiclass\"\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"batch_size\": 4,  # Reduced from 8 to fit in 14.74 GB GPU\n",
    "        \"grad_accum_steps\": 16,  # Increased to maintain effective batch size of 64\n",
    "        \"learning_rate\": 5e-5,\n",
    "        \"weight_decay\": 0.05,\n",
    "        \"num_epochs\": 20,\n",
    "        \"patience\": 5\n",
    "    }\n",
    "}"
   ],
   "id": "e85800f6504ff2e9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnifiedWasteDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A unified dataset that ingests data from multiple sources and maps them\n",
    "    to a single 30-class target schema.\n",
    "    \"\"\"\n",
    "    def __init__(self, sources_config, target_classes, transform=None):\n",
    "        self.transform = transform\n",
    "        self.target_classes = sorted(target_classes)\n",
    "        self.class_to_idx = {c: i for i, c in enumerate(self.target_classes)}\n",
    "        self.samples = []\n",
    "\n",
    "        self.skipped_count = 0\n",
    "        self.skipped_labels = {}  # Track what labels are being skipped\n",
    "\n",
    "        for source in sources_config:\n",
    "            self._ingest_source(source)\n",
    "\n",
    "        logger.info(f\"Unified Dataset Created: {len(self.samples)} images. Skipped {self.skipped_count} unmappable images.\")\n",
    "\n",
    "        # Log skipped labels for debugging\n",
    "        if self.skipped_labels:\n",
    "            logger.warning(f\"Skipped labels breakdown:\")\n",
    "            for label, count in sorted(self.skipped_labels.items(), key=lambda x: x[1], reverse=True):\n",
    "                logger.warning(f\"  '{label}': {count} images\")\n",
    "\n",
    "    def _ingest_source(self, source):\n",
    "        path = Path(source[\"path\"])\n",
    "        if not path.exists():\n",
    "            parent = path.parent\n",
    "            found = False\n",
    "            if parent.exists():\n",
    "                for child in parent.iterdir():\n",
    "                    if child.is_dir():\n",
    "                        try:\n",
    "                            if any(child.iterdir()):\n",
    "                                path = child\n",
    "                                found = True\n",
    "                                break\n",
    "                        except PermissionError:\n",
    "                            continue\n",
    "\n",
    "            if not found or not path.exists():\n",
    "                logger.warning(f\"Source {source['name']} not found at {source['path']}. Skipping.\")\n",
    "                return\n",
    "\n",
    "        logger.info(f\"Ingesting {source['name']} from {path}...\")\n",
    "\n",
    "        for root, _, files in os.walk(path):\n",
    "            folder_name = Path(root).name.lower()\n",
    "\n",
    "            target_label = self._map_label(folder_name, source['type'])\n",
    "\n",
    "            if target_label:\n",
    "                target_idx = self.class_to_idx[target_label]\n",
    "                for file in files:\n",
    "                    if file.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp')):\n",
    "                        self.samples.append((Path(root) / file, target_idx))\n",
    "            else:\n",
    "                img_count = sum(1 for f in files if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp')))\n",
    "                if img_count > 0:\n",
    "                    self.skipped_count += img_count\n",
    "                    # Track which labels are being skipped\n",
    "                    if folder_name not in self.skipped_labels:\n",
    "                        self.skipped_labels[folder_name] = 0\n",
    "                    self.skipped_labels[folder_name] += img_count\n",
    "\n",
    "    def _map_label(self, raw_label, source_type):\n",
    "        raw = raw_label.lower().strip()\n",
    "\n",
    "        if source_type == 'master':\n",
    "            if raw in self.target_classes:\n",
    "                return raw\n",
    "            # Fallback: try to find closest match\n",
    "            for target in self.target_classes:\n",
    "                if raw in target or target in raw:\n",
    "                    return target\n",
    "            return None\n",
    "\n",
    "        if source_type == 'mapped_12':\n",
    "            mapping = {\n",
    "                'paper': 'office_paper',\n",
    "                'cardboard': 'cardboard_boxes',\n",
    "                'plastic': 'plastic_food_containers',\n",
    "                'metal': 'aluminum_food_cans',\n",
    "                'glass': 'glass_food_jars',\n",
    "                'brown-glass': 'glass_beverage_bottles',\n",
    "                'green-glass': 'glass_beverage_bottles',\n",
    "                'white-glass': 'glass_food_jars',\n",
    "                'clothes': 'clothing',\n",
    "                'shoes': 'shoes',\n",
    "                'biological': 'food_waste',\n",
    "                'trash': 'food_waste'\n",
    "            }\n",
    "            return mapping.get(raw)\n",
    "\n",
    "        if source_type == 'mapped_2':\n",
    "            # Organic waste\n",
    "            if raw in ['organic', 'o']:\n",
    "                return 'food_waste'\n",
    "            # Recyclable waste (paper, plastic, metal, glass mix)\n",
    "            if raw in ['recyclable', 'r']:\n",
    "                return 'plastic_food_containers'  # Generic recyclable\n",
    "            return None\n",
    "\n",
    "        if source_type == 'mapped_10':\n",
    "            mapping = {\n",
    "                'metal': 'aluminum_food_cans',\n",
    "                'glass': 'glass_food_jars',\n",
    "                'biological': 'food_waste',\n",
    "                'paper': 'office_paper',\n",
    "                'battery': 'aerosol_cans',\n",
    "                'trash': 'food_waste',\n",
    "                'cardboard': 'cardboard_boxes',\n",
    "                'shoes': 'shoes',\n",
    "                'clothes': 'clothing',\n",
    "                'plastic': 'plastic_food_containers'\n",
    "            }\n",
    "            return mapping.get(raw)\n",
    "\n",
    "        if source_type == 'mapped_6':\n",
    "            mapping = {\n",
    "                'cardboard': 'cardboard_boxes',\n",
    "                'glass': 'glass_food_jars',\n",
    "                'metal': 'aluminum_food_cans',\n",
    "                'paper': 'office_paper',\n",
    "                'plastic': 'plastic_food_containers',\n",
    "                'trash': 'food_waste'\n",
    "            }\n",
    "            return mapping.get(raw)\n",
    "\n",
    "        if source_type == 'industrial':\n",
    "            mapping = {\n",
    "                'pet': 'plastic_food_containers',\n",
    "                'hdpe': 'plastic_food_containers',\n",
    "                'pvc': 'plastic_food_containers',\n",
    "                'ldpe': 'plastic_food_containers',\n",
    "                'pp': 'plastic_food_containers',\n",
    "                'ps': 'plastic_food_containers',\n",
    "                'metal': 'aluminum_food_cans',\n",
    "                'glass': 'glass_food_jars',\n",
    "                'paper': 'office_paper',\n",
    "                'cardboard': 'cardboard_boxes',\n",
    "                'trash': 'food_waste'\n",
    "            }\n",
    "            return mapping.get(raw)\n",
    "\n",
    "        if source_type == 'multiclass':\n",
    "            mapping = {\n",
    "                'plastic': 'plastic_food_containers',\n",
    "                'metal': 'aluminum_food_cans',\n",
    "                'glass': 'glass_food_jars',\n",
    "                'paper': 'office_paper',\n",
    "                'cardboard': 'cardboard_boxes',\n",
    "                'trash': 'food_waste',\n",
    "                'organic': 'food_waste',\n",
    "                'battery': 'aerosol_cans',\n",
    "                'clothes': 'clothing',\n",
    "                'shoes': 'shoes'\n",
    "            }\n",
    "            return mapping.get(raw)\n",
    "\n",
    "        # Universal fallback mappings for common waste categories\n",
    "        # This ensures NO images are skipped\n",
    "        fallback_mapping = {\n",
    "            # Recyclables\n",
    "            'recyclable': 'plastic_food_containers',\n",
    "            'recycle': 'plastic_food_containers',\n",
    "            'recycling': 'plastic_food_containers',\n",
    "            # Waste types\n",
    "            'waste': 'food_waste',\n",
    "            'garbage': 'food_waste',\n",
    "            'rubbish': 'food_waste',\n",
    "            'refuse': 'food_waste',\n",
    "            # Organic\n",
    "            'compost': 'food_waste',\n",
    "            'food': 'food_waste',\n",
    "            'kitchen': 'food_waste',\n",
    "            'biological': 'food_waste',\n",
    "            # Paper products\n",
    "            'newspaper': 'newspaper',\n",
    "            'magazine': 'magazines',\n",
    "            'book': 'office_paper',\n",
    "            'document': 'office_paper',\n",
    "            # Plastic types\n",
    "            'bottle': 'plastic_water_bottles',\n",
    "            'bottle-transp': 'plastic_water_bottles',\n",
    "            'bottle-blue': 'plastic_water_bottles',\n",
    "            'bottle-dark': 'plastic_water_bottles',\n",
    "            'bottle-green': 'plastic_water_bottles',\n",
    "            'bottle-blue5l': 'plastic_water_bottles',\n",
    "            'bottle-milk': 'plastic_water_bottles',\n",
    "            'bottle-oil': 'plastic_water_bottles',\n",
    "            'bottle-yogurt': 'plastic_food_containers',\n",
    "            'bottle-multicolor': 'plastic_water_bottles',\n",
    "            'bottle-transp-full': 'plastic_water_bottles',\n",
    "            'bottle-blue-full': 'plastic_water_bottles',\n",
    "            'bottle-green-full': 'plastic_water_bottles',\n",
    "            'bottle-dark-full': 'plastic_water_bottles',\n",
    "            'bottle-milk-full': 'plastic_water_bottles',\n",
    "            'bottle-multicolorv-full': 'plastic_water_bottles',\n",
    "            'bottle-blue5l-full': 'plastic_water_bottles',\n",
    "            'bottle-oil-full': 'plastic_water_bottles',\n",
    "            'bag': 'plastic_shopping_bags',\n",
    "            'container': 'plastic_food_containers',\n",
    "            'cup': 'paper_cups',\n",
    "            'straw': 'plastic_straws',\n",
    "            # Detergents (plastic containers)\n",
    "            'detergent-white': 'plastic_food_containers',\n",
    "            'detergent-color': 'plastic_food_containers',\n",
    "            'detergent-transparent': 'plastic_food_containers',\n",
    "            'detergent-box': 'cardboard_boxes',\n",
    "            # Metal\n",
    "            'can': 'aluminum_soda_cans',\n",
    "            'cans': 'aluminum_soda_cans',\n",
    "            'tin': 'steel_food_cans',\n",
    "            'aluminum': 'aluminum_food_cans',\n",
    "            'steel': 'steel_food_cans',\n",
    "            'canister': 'aluminum_food_cans',\n",
    "            'battery': 'aerosol_cans',  # Hazardous, map to aerosol as closest\n",
    "            # Glass\n",
    "            'jar': 'glass_food_jars',\n",
    "            'glass-transp': 'glass_food_jars',\n",
    "            'glass-dark': 'glass_beverage_bottles',\n",
    "            'glass-green': 'glass_beverage_bottles',\n",
    "            'white-glass': 'glass_food_jars',\n",
    "            'brown-glass': 'glass_beverage_bottles',\n",
    "            'green-glass': 'glass_beverage_bottles',\n",
    "            # Cardboard\n",
    "            'milk-cardboard': 'cardboard_boxes',\n",
    "            'juice-cardboard': 'cardboard_boxes',\n",
    "            # Textiles\n",
    "            'fabric': 'clothing',\n",
    "            'textile': 'clothing',\n",
    "            # Foam\n",
    "            'foam': 'styrofoam_cups',\n",
    "            'styrofoam': 'styrofoam_cups',\n",
    "            'polystyrene': 'styrofoam_cups',\n",
    "        }\n",
    "\n",
    "        # Try fallback mapping\n",
    "        for key, value in fallback_mapping.items():\n",
    "            if key in raw:\n",
    "                return value\n",
    "\n",
    "        return None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label_idx = self.samples[idx]\n",
    "        try:\n",
    "            img = Image.open(path).convert('RGB')\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            return img, label_idx\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Corrupt image {path}: {e}\")\n",
    "            return torch.zeros((3, 448, 448)), label_idx\n",
    "\n",
    "    def get_labels(self):\n",
    "        return [s[1] for s in self.samples]"
   ],
   "id": "4a83856bf02ace6d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vision_transforms(config, model, is_train=True):\n",
    "    try:\n",
    "        data_config = resolve_data_config(model.default_cfg, model=model)\n",
    "        if is_train:\n",
    "            return create_transform(\n",
    "                input_size=data_config['input_size'],\n",
    "                is_training=True,\n",
    "                use_prefetcher=False,\n",
    "                no_aug=False,\n",
    "                scale=(0.08, 1.0),\n",
    "                ratio=(0.75, 1.33),\n",
    "                hflip=0.5,\n",
    "                vflip=0.0,\n",
    "                color_jitter=0.4,\n",
    "                auto_augment='rand-m9-mstd0.5-inc1',\n",
    "                interpolation=data_config['interpolation'],\n",
    "                mean=data_config['mean'],\n",
    "                std=data_config['std'],\n",
    "                re_prob=0.25,\n",
    "                re_mode='pixel',\n",
    "                re_count=1,\n",
    "            )\n",
    "        else:\n",
    "            return create_transform(\n",
    "                input_size=data_config['input_size'],\n",
    "                is_training=False,\n",
    "                use_prefetcher=False,\n",
    "                interpolation=data_config['interpolation'],\n",
    "                mean=data_config['mean'],\n",
    "                std=data_config['std'],\n",
    "            )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to create transforms: {e}\")\n",
    "        raise"
   ],
   "id": "8cac2161a057a33e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vision_model(config):\n",
    "    logger.info(f\"Creating model: {config['model']['backbone']}\")\n",
    "    model = timm.create_model(\n",
    "        config[\"model\"][\"backbone\"],\n",
    "        pretrained=config[\"model\"][\"pretrained\"],\n",
    "        num_classes=config[\"model\"][\"num_classes\"],\n",
    "        drop_rate=config[\"model\"][\"drop_rate\"],\n",
    "        drop_path_rate=config[\"model\"][\"drop_path_rate\"]\n",
    "    )\n",
    "    return model"
   ],
   "id": "20be2291a212f609"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vision_model(config):\n",
    "    set_seed()\n",
    "    optimize_memory()\n",
    "    device = get_device()\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "\n",
    "    model = create_vision_model(config).to(device)\n",
    "    logger.info(f\"Model parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M\")\n",
    "\n",
    "    # Enable gradient checkpointing to save memory\n",
    "    if hasattr(model, 'set_grad_checkpointing'):\n",
    "        model.set_grad_checkpointing(enable=True)\n",
    "        logger.info(\"Gradient checkpointing enabled\")\n",
    "\n",
    "    train_transform = get_vision_transforms(config, model, is_train=True)\n",
    "    val_transform = get_vision_transforms(config, model, is_train=False)\n",
    "\n",
    "    full_dataset = UnifiedWasteDataset(\n",
    "        sources_config=config[\"data\"][\"sources\"],\n",
    "        target_classes=TARGET_CLASSES,\n",
    "        transform=None\n",
    "    )\n",
    "\n",
    "    if len(full_dataset) == 0:\n",
    "        logger.error(\"Dataset is empty. Check paths.\")\n",
    "        return None\n",
    "\n",
    "    train_size = int(0.85 * len(full_dataset))\n",
    "    val_size = len(full_dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "    train_dataset.dataset.transform = train_transform\n",
    "    val_dataset.dataset.transform = val_transform\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config[\"training\"][\"batch_size\"],\n",
    "        shuffle=True,\n",
    "        num_workers=config[\"data\"][\"num_workers\"],\n",
    "        pin_memory=config[\"data\"][\"pin_memory\"],\n",
    "        persistent_workers=True if config[\"data\"][\"num_workers\"] > 0 else False\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config[\"training\"][\"batch_size\"] * 2,\n",
    "        shuffle=False,\n",
    "        num_workers=config[\"data\"][\"num_workers\"],\n",
    "        persistent_workers=True if config[\"data\"][\"num_workers\"] > 0 else False\n",
    "    )\n",
    "\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config[\"training\"][\"learning_rate\"],\n",
    "        weight_decay=config[\"training\"][\"weight_decay\"]\n",
    "    )\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config[\"training\"][\"num_epochs\"])\n",
    "    early_stopping = EarlyStopping(patience=config[\"training\"][\"patience\"])\n",
    "\n",
    "    use_amp = (device.type == \"cuda\")\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=use_amp) if use_amp else None\n",
    "\n",
    "    accumulation_steps = config[\"training\"][\"grad_accum_steps\"]\n",
    "\n",
    "    try:\n",
    "        wandb.init(project=\"sustainability-vision-lake\", config=config, mode=\"online\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"W&B initialization failed: {e}. Continuing without logging.\")\n",
    "        wandb.init(mode=\"disabled\")\n",
    "\n",
    "    for epoch in range(config[\"training\"][\"num_epochs\"]):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['training']['num_epochs']}\")\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for i, (images, labels) in enumerate(pbar):\n",
    "            images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "\n",
    "            if use_amp:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels) / accumulation_steps\n",
    "                scaler.scale(loss).backward()\n",
    "\n",
    "                if (i + 1) % accumulation_steps == 0:\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                    optimizer.zero_grad()\n",
    "            else:\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels) / accumulation_steps\n",
    "                loss.backward()\n",
    "                if (i + 1) % accumulation_steps == 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "            running_loss += loss.item() * accumulation_steps\n",
    "            with torch.no_grad():\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            current_loss = running_loss / (i + 1)\n",
    "            pbar.set_postfix({'loss': f\"{current_loss:.4f}\", 'acc': f\"{100*correct/total:.2f}%\"})\n",
    "\n",
    "        scheduler.step()\n",
    "        train_acc = 100 * correct / total\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(val_loader, desc=\"Validation\", leave=False):\n",
    "                images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "\n",
    "                if use_amp:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        outputs = model(images)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                else:\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        logger.info(f\"Epoch {epoch+1}/{config['training']['num_epochs']}: Train Acc {train_acc:.2f}%, Val Loss {val_loss:.4f}, Val Acc {val_acc:.2f}%\")\n",
    "\n",
    "        try:\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"train_acc\": train_acc,\n",
    "                \"val_acc\": val_acc,\n",
    "                \"val_loss\": val_loss,\n",
    "                \"learning_rate\": optimizer.param_groups[0]['lr']\n",
    "            })\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        if early_stopping(val_acc):\n",
    "            logger.info(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "        # Clear GPU cache after each epoch to prevent memory fragmentation\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    try:\n",
    "        wandb.finish()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return model"
   ],
   "id": "56b93d44ea191d12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PEAK STANDARD GNN\n",
    "# Using Graph Attention Networks v2 (GATv2) for superior expressive power\n",
    "\n",
    "def generate_structured_knowledge_graph(num_classes=30, feat_dim=128):\n",
    "    \"\"\"\n",
    "    Generates a realistic Knowledge Graph structure for waste classification.\n",
    "    Simulates the schema: Item -> Material -> Bin\n",
    "    \"\"\"\n",
    "    logger.info(\"Generating structured Knowledge Graph...\")\n",
    "    \n",
    "    total_nodes = num_classes + 8 + 4\n",
    "    x = torch.randn(total_nodes, feat_dim) # Node features (embeddings)\n",
    "    \n",
    "    edge_sources = []\n",
    "    edge_targets = []\n",
    "    \n",
    "    # Node Indices for Materials\n",
    "    mat_base = num_classes\n",
    "    mat_plastic = mat_base + 0\n",
    "    mat_paper = mat_base + 1\n",
    "    mat_glass = mat_base + 2\n",
    "    mat_metal = mat_base + 3\n",
    "    mat_organic = mat_base + 4\n",
    "    mat_fabric = mat_base + 5\n",
    "    mat_ewaste = mat_base + 6\n",
    "    mat_misc = mat_base + 7\n",
    "    \n",
    "    # Node Indices for Bins\n",
    "    bin_base = mat_base + 8\n",
    "    bin_recycle = bin_base + 0\n",
    "    bin_compost = bin_base + 1\n",
    "    bin_haz = bin_base + 2\n",
    "    bin_landfill = bin_base + 3\n",
    "    \n",
    "    # 1. Edges: Material -> Bin (Knowledge Rules)\n",
    "    mat_bin_map = [\n",
    "        (mat_plastic, bin_recycle),\n",
    "        (mat_paper, bin_recycle),\n",
    "        (mat_glass, bin_recycle),\n",
    "        (mat_metal, bin_recycle),\n",
    "        (mat_organic, bin_compost),\n",
    "        (mat_fabric, bin_landfill), \n",
    "        (mat_ewaste, bin_haz),\n",
    "        (mat_misc, bin_landfill)\n",
    "    ]\n",
    "    \n",
    "    for m, b in mat_bin_map:\n",
    "        edge_sources.append(m); edge_targets.append(b)\n",
    "        edge_sources.append(b); edge_targets.append(m)\n",
    "        \n",
    "    # 2. Edges: Item -> Material (Simulate Classification Knowledge)\n",
    "    for i in range(num_classes):\n",
    "        mat_idx = mat_base + (i % 8) \n",
    "        edge_sources.append(i); edge_targets.append(mat_idx)\n",
    "        edge_sources.append(mat_idx); edge_targets.append(i)\n",
    "        \n",
    "    # 3. Edges: Item -> Item (Similarity)\n",
    "    for i in range(num_classes):\n",
    "        neighbor = (i + 8) % num_classes\n",
    "        edge_sources.append(i); edge_targets.append(neighbor)\n",
    "        edge_sources.append(neighbor); edge_targets.append(i)\n",
    "\n",
    "    edge_index = torch.tensor([edge_sources, edge_targets], dtype=torch.long)\n",
    "    \n",
    "    logger.info(f\"Graph generated: {total_nodes} nodes, {len(edge_sources)} edges.\")\n",
    "    \n",
    "    return Data(x=x, edge_index=edge_index, num_nodes=total_nodes)\n",
    "\n",
    "class GATv2Model(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=4, heads=8, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(GATv2Conv(in_channels, hidden_channels, heads=heads, concat=True, dropout=dropout))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(GATv2Conv(hidden_channels * heads, hidden_channels, heads=heads, concat=True, dropout=dropout))\n",
    "        self.convs.append(GATv2Conv(hidden_channels * heads, out_channels, heads=1, concat=False, dropout=dropout))\n",
    "        self.dropout = dropout\n",
    "        self.norm = nn.ModuleList([nn.LayerNorm(hidden_channels * heads) for _ in range(num_layers - 1)])\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for i, conv in enumerate(self.convs[:-1]):\n",
    "            x = conv(x, edge_index)\n",
    "            x = self.norm[i](x)\n",
    "            x = F.gelu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        return self.convs[-1](x, edge_index)"
   ],
   "id": "a413b4ee062ae6a8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gnn_model():\n",
    "    set_seed()\n",
    "    optimize_memory()\n",
    "    device = get_device()\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "\n",
    "    in_dim = 128\n",
    "    hidden_dim = 512\n",
    "    out_dim = 256\n",
    "    lr = 0.001\n",
    "    epochs = 50\n",
    "\n",
    "    data = generate_structured_knowledge_graph(num_classes=30, feat_dim=128).to(device)\n",
    "\n",
    "    model = GATv2Model(in_dim, hidden_dim, out_dim).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)\n",
    "\n",
    "    logger.info(\"Starting GNN Training...\")\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        z = model(data.x, data.edge_index)\n",
    "\n",
    "        pos_src, pos_dst = data.edge_index\n",
    "        pos_loss = -torch.log(torch.sigmoid((z[pos_src] * z[pos_dst]).sum(dim=1)) + 1e-15).mean()\n",
    "\n",
    "        neg_src = torch.randint(0, data.num_nodes, (pos_src.size(0),), device=device)\n",
    "        neg_dst = torch.randint(0, data.num_nodes, (pos_src.size(0),), device=device)\n",
    "        neg_loss = -torch.log(1 - torch.sigmoid((z[neg_src] * z[neg_dst]).sum(dim=1)) + 1e-15).mean()\n",
    "\n",
    "        loss = pos_loss + neg_loss\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step(loss)\n",
    "\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            logger.info(f\"Epoch {epoch+1}/{epochs}: Loss {loss.item():.4f}, Best Loss {best_loss:.4f}\")\n",
    "\n",
    "    return model"
   ],
   "id": "d48fee1a1cb8a264"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        logger.info(\"=\"*80)\n",
    "        logger.info(\"Phase 1: Multi-Source Data Lake Vision Training\")\n",
    "        logger.info(\"=\"*80)\n",
    "\n",
    "        vision_model = train_vision_model(VISION_CONFIG)\n",
    "\n",
    "        if vision_model is not None:\n",
    "            save_path = \"best_vision_eva02_lake.pth\"\n",
    "            torch.save(vision_model.state_dict(), save_path)\n",
    "            logger.info(f\"Vision model saved to {save_path}\")\n",
    "\n",
    "            del vision_model\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "        else:\n",
    "            logger.error(\"Vision model training failed\")\n",
    "\n",
    "        logger.info(\"=\"*80)\n",
    "        logger.info(\"Phase 2: GNN Knowledge Graph Training\")\n",
    "        logger.info(\"=\"*80)\n",
    "\n",
    "        gnn_model = train_gnn_model()\n",
    "\n",
    "        if gnn_model is not None:\n",
    "            save_path = \"best_gnn_gatv2.pth\"\n",
    "            torch.save(gnn_model.state_dict(), save_path)\n",
    "            logger.info(f\"GNN model saved to {save_path}\")\n",
    "\n",
    "            del gnn_model\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        logger.info(\"=\"*80)\n",
    "        logger.info(\"Training completed successfully!\")\n",
    "        logger.info(\"=\"*80)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Training failed with error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise"
   ],
   "id": "78cc46781594be13"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
